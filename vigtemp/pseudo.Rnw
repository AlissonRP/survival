\documentclass{article}[11pt]
\usepackage{Sweave}
\usepackage{amsmath}
\addtolength{\textwidth}{1in}
\addtolength{\oddsidemargin}{-.5in}
\setlength{\evensidemargin}{\oddsidemargin}
%\VignetteIndexEntry{Multi-state models and competing risks}

\SweaveOpts{keep.source=TRUE, fig=FALSE}
% Ross Ihaka suggestions
\DefineVerbatimEnvironment{Sinput}{Verbatim} {xleftmargin=2em}
\DefineVerbatimEnvironment{Soutput}{Verbatim}{xleftmargin=2em}
\DefineVerbatimEnvironment{Scode}{Verbatim}{xleftmargin=2em}
\fvset{listparameters={\setlength{\topsep}{0pt}}}
\renewenvironment{Schunk}{\vspace{\topsep}}{\vspace{\topsep}}

% I had been putting figures in the figures/ directory, but the standard
%  R build script does not copy it and then R CMD check fails
\SweaveOpts{prefix.string=compete,width=6,height=4}
\newcommand{\myfig}[1]{\includegraphics[height=!, width=\textwidth]
                        {pseudo-#1.pdf}}
\setkeys{Gin}{width=\textwidth}
<<echo=FALSE>>=
options(continue="  ", width=70)
options(SweaveHooks=list(fig=function() par(mar=c(4.1, 4.1, .3, 1.1))))
pdf.options(pointsize=10) #text in graph about the same as regular text
options(contrasts=c("contr.treatment", "contr.poly")) #ensure default
options(show.signif.stars = FALSE)  # show statistical intelligence
library("survival")
library("geepack")
@

\title{Psuedo-values}
\author{Terry Therneau}
\newcommand{\code}[1]{\texttt{#1}}

\begin{document}
\maketitle

\section{Motivation}
Survival analysis models sometime focus directly on the time to event, e.g.,
\code{survreg} but most often on the hazard rate, which underlies both the
Cox proportional hazard model and the nonparametric Kaplan-Meier and 
Nelson-Aalen estimates.
A third approach is to look at the simple binomial response of alive/dead.
The most naive way to do so is to simply ignore subjects' follow-up time,
but the results of this approach are often badly biased.
Slightly more sophisticated is to use landmark approach, by choosing a
predetermined cutoff time $\tau$, along with a binary indicator of whether
each subject did or did not survival longer than $\tau$.
Problems with this approach were pointed out 4 decades ago by Berkson and
Gage \cite{Berkson52}:
subjects censored before $\tau$ have to be discarded,
making inefficient use of the data, and any difference between treatments in
terms of the censoring distribution will again lead to biased estimates.

If the data were not censored, of course, then no points would be discarded.  
This is one
motivation for the use of \emph{pseudo values}, which ``fill in'' the
missing observations.
The key insight is to note that if the data were uncensored, one could recover
$N_i(t)$ from the survival curve of the entire data $S(t)$ along with curve
$S_{-i}(t)$ which omits observation $i$, as
\begin{align*}
   N_i(t) &=  nS(t) - (n-1)S_{-i}(t) \\
         &=   S(t) + (n-1)[S(t) -S_{-i}(t)]
\end{align*}
To adapt this to censored data one can use the difference in the 
two Kaplan-Meier estimators.
A computationally simpler approach is to use the infinitesimal 
jackknife (IJ) estimate:
\begin{equation}
  S(t) -S_{-i}(t) \approx \frac{\partial S(t)}{\partial w_i} \label{IJ}
\end{equation}
The right hand side of \eqref{IJ} is known as the infinitesimal jackknife.
The left hand side corresponds to a change of subject $i$'s weight
from $w_i=1$ to $w_i=0$;
the infinitesimal jackknife approximates this as a simple Taylor series.
The estimate is simple from the user's point of view
because the IJ estimates are returned directly by the
\code{survfit} function; computationally the internal calculation
is more efficient than the $n$ recomputations of the Kaplan-Meier
required for the formal pseudo values.  

\section{Restricted mean survival time}
The restricted mean survival time (RMST) can be calculated as the
area under the Kaplan-Meier curve up to some cutpoint $\tau$.
The endpoint $\tau$ will often be prespecified; other common choices are
the last observed time in the data or the last observed event time.

Here is a very simple example using overall survival from
a clincal trial in colon cancer.
The resulting KM influence matrix has a row for each observation and a column 
for each unique time.
We then calculate the restricted mean survival time (RMST), which is the
area under the survival curve, i.e., the summed areas of a series of rectangles.
The influence of each observation on the RMST is computed in the same way,
since sum(changes in rectangles) = change in RMST.

<<aml1, echo=TRUE>>=
cdata <- subset(colon, etype==2)  # 1 = death or progression
cdata$years <- cdata$time/365.25  # Simpler display in years
sfit <- survfit(Surv(years, status) ~1, cdata, influence=1)
dim(sfit$influence.surv)
nrow(cdata)
length(sfit$time)

rmst <- function(fit, tmax) {
    # extract the RMST, and the leverage of each subject on the RMST
    if (missing(tmax)) tmax <- max(sfit0$time)
    if (length(tmax) !=1) stop("tmax must be a single value")

    sfit0 <- survfit0(fit)      # add the time=0 point to the result
    rsum <- function(y, x= sfit0$time) {  # sum of rectangles
        keep <- which(x < tmax)
        width <- diff(c(x[keep], tmax))  
        sum(width * y[keep])
    }
    rmst <- rsum(sfit0$surv, sfit0$time)
    ijack <- apply(sfit0$influence.surv, 1, rsum)
    list(rmst=rmst, sd.rmst = sqrt(sum(ijack^2)), 
         pseudo = rmst + (sfit$n -1)* ijack)
}

cmean <- rmst(sfit, 9)
unlist(cmean[1:2])
print(sfit, rmean= 9)
@ 

The RMST and robust standard error computed by \code{rmst} agree with
the estimate and asymptotic standard error from the print command.
Now approach the estimate using pseudo values:

<<rmstfit>>=
cdata$pseudo <- cmean$pseudo   # add it to the data frame
pfit1 <- lm(pseudo ~ rx -1, data=cdata)
round(summary(pfit1)$coef, 3)

print(survfit(Surv(years, status) ~ rx, cdata), rmean=9)
@ 

A simple linear regression has created values that are almost identical
to the per-group non=parametric RMST, as well as replicating the standard
errors.  
More interesting is to look at multivariate fits.
A Cox model with treatment, the presence of 4 or more lymph nodes, and
extent of disease shows that all 3 variables are important.
Compare 3 different RMST estimates for the 12 groups that these
define: non-parametric, predictions from the fitted Cox model, and predictions
from a linear model based on the pseudo-values.

<<aml2, fig=TRUE>>=
cdata$extent2 <- ifelse(cdata$extent > 2, 0, 1)
# KM for the 12 groups
sfit3 <- survfit(Surv(years, status) ~ extent2 + node4 + rx, cdata)

# Cox fit + 12 predicted curves
cox3  <- coxph(Surv(years, status) ~ rx + node4 + extent2, cdata)
print(cox3)

tempdata <- expand.grid(rx = levels(cdata$rx), 
                         node4=0:1,  extent2=0:1 )
csurv <- survfit(cox3, newdata= tempdata)  

# pseudo-value
pfit3 <- lm(pseudo ~ rx + node4 + extent2, cdata)

# glue results together into one object and print
psum <- cbind(direct= summary(sfit3, rmean=9)$table[,5],
              cox   = summary(csurv, rmean=9)$table[,5],
              pseudo= predict(pfit3, newdata=tempdata))
round(psum, 2)

# plot the results
matplot(psum[,1], psum[,-1], pch='cp',
        xlab="Direct RSTM estimate", ylab= "Model estimate")
abline(0,1, lty=2)
legend(5.5, 4.5, c("Cox model estimate", "Psuedo-value estimate"),
       pch='cp', col=1:2, bty='n')
@ 

A linear regression on the psuedo values has given almost idential RMST
values to the Cox model estimates.
The plot shows that for 3 of the 12 subgroups the Cox and pseudo-value
approaches predict larger RMST values than the direct
estimates from individual KM curves.  
These three groups, it turns out, are the patients with 4+ nodes and extensive
disease, for each of the three different treatments.
It appears that the negative survival impact of the combination of these two 
factors is larger than an additive risk score predicts;
both the Cox and linear models may need an \code{extent * node} 
interaction term.

\section{Extended landmark method and Cox models}
We stated above that a landmark approach is often used, i.e., pick some
cutpoint time $\tau$ and compare those who died before $\tau$ to those
known to still be alive at $\tau$.
This is inefficient since observations censored before $\tau$ are not used at
all.
One way to extend this approach and remove the dependence on $\tau$ is to use
a variant of the proportional odds model, namely
\begin{align}
  E(N(t)) &= Pr(N(t) ==1) \nonumber \\
          &= g( \beta_0(t) + \beta_1 x_1 + \beta_2 x_2 + \ldots) \label{propodds}
\end{align}
where $g$ is an appropriate transformation function,
often using the usual logisitc regression link of  $g(x) = \exp(x)/[1+ \exp(x)]$,
and $\beta_0$ is a time dependent intercept.
Interestingly, if instead we use the complimentary log-log link  
$g(x) = \exp(-\exp(x))$, then
\begin{align*}
  S(t) &= Pr(N(t) ==1) \\
 \Lambda(t) &= -\log(S(t)) \\
            &= \exp(\beta_0(t) + \beta_1 x_1 + \beta_2 x_2 + \ldots) \\
            &= \Lambda_0(t) \exp(\beta_1 x_1 + \beta_2 x_2 + \ldots)
\end{align*}
which is exactly the Cox proportional hazards model.
A ``stacked'' data set with all $n_1$ observations and status values at
time $t_1$, then $n_2$ values at time $t_2$, \ldots is then another (tedious)
way to fit the proportional hazards model, $n_1$ being the number of subjects
at risk and under observation at time $t_1$, etc.
This route is one way to derive the formal equivalence of conditional logistic
regression and the partial likelihood, which is used to computational 
advantage in the \code{clogit} function.

A further development along these lines is to use psuedo values, which make use
of all $n$ subjects at all of the event times.  
The resulting data set is large: all 929 subjects appear at each of the 409
unique death times.
Andersen and Pohar \cite{Anxx} showed that the results of a such a fit are 
essentially
unchanged if only a half dozen time points are used, provided that they are 
spread across the time scale.  T
his speeds up the fit considerably.  
Below we will use 1/2 year intervals
for 8 years; 16 time points.
In this case we do need to use a robust variance;
the geese program requires subjects that are clustered to be contiguous
in the dataset.

<<cdata2>>=
pseudo <- function(fit, times) {
    keep <- findInterval(times, fit$time)
    n <- sum(fit$n)
    inf <- fit$influence[,keep, drop=FALSE]  #only keep these times
    surv <- fit$surv[keep]
    pseudoy <- inf*(n-1) + surv[col(inf)]
    drop(pseudoy)
}
times <- 1:16/2
temp  <- pseudo(sfit, times)
cdata2 <- data.frame(cdata[row(temp),],
                     yp  = c(temp),   # c() turn the matrix into a single col
                     dtime = c(col(temp)))
dim(cdata2)
cdata2 <- cdata2[order(cdata2$id), ]  # sort by id
clfit <- geese(yp ~ rx + node4 + extent2 + factor(dtime), data=cdata2,
               mean.link="cloglog", scale.fix=TRUE, id = cdata2$id,
               corstr= "independence")
round(summary(clfit)$mean[2:5,] , 3)

# Compare this to the naive glm variance, and to the Cox fit
gfit2 <- glm(yp ~ rx + node4 + extent2 + factor(dtime), data=cdata2,
             family=quasi(link='cloglog', variance='constant'),
             mustart = pmax(.05, pmin(.95, yp)))

temp <- cbind(coef.cox = coef(cox3),
              coef.gee = clfit$beta[2:5],
              coef.glm = coef(gfit2)[2:5],
              std.cox  = sqrt(diag(vcov(cox3))),
              std.gee  = summary(clfit)$mean[2:5, 2],
              std.glm  = summary(gfit2)$coefficients[2:5, 2])
round(temp,2)
@ 

The glm and gee fits have identical coefficients, but the glm standard
errors are systematically too small; they do not take account of the
fact that a single subject generates 16 data points.
For this particular data set the standard errors of the Cox and GEE
fits are similar, but other's simulations show that the Cox model
standard errors are somewhat smaller than GEE on average.
The coefficients for the glm and gee models are reversed from the Cox model 
coefficients, since the former are predicting the probablility of
survival and the latter a death rate. 
A high survival implies a low death rate and vice versa.

\section{Multi-state models}

When the underlying survival curve is from a multi-state model, the
\code{survfit} function will estimate the \emph{probability in
state} matrix (\code{pstate}) which contains a row for each
unique event or censoring time and a column for each state.
Each row of the matrix sums to 1 since everyone has to be somewhere.

The area under each curve, up to a cutoff time $\tau$, is an estimate of the 
total time spent in the state and is the analog to the RMST.
(The sum of the value across all the states will equal $\tau$.)
These can be explored with pseudo values, as was done for the single state
models.
As an example consider the simple multi-state model for the NAFLD data
based on the number of metabolic comorbidities for each subject of
diabetes, hyperlipidemia, and hypertension. 
Subjects traverse from 0 to 3 of the these over time, with death as a 
competing risk.

<<state3, fig=TRUE>>=
states <- c("0 MC", "1 MC", "2 MC", "3 MC", "death")
tmat <- matrix(0, 5,5, dimnames = list(states, states))
tmat[1,2] <- tmat[2,3] <- tmat[3,4] <- 1
tmat[-5, 5] <- 1
statefig(cbind(c(4,1)), tmat)
@ 

As usual, building the data set is the most work.  The natural time scale
for analysis is age.
<<nbuild>>=
temp <- subset(nafld3, event %in% c("diabetes", "dyslipidemia", "htn"))
ndata <- tmerge(nafld1[,1:7], nafld1, id = id, death= event(futime, status))
ndata <- tmerge(ndata, temp, id=id, mc= cumevent(days),
                istate = cumtdc(days))
ndata$age1 <- ndata$age + ndata$tstart/365
ndata$age2 <- ndata$age + ndata$tstop/365
ndata$nafld <- ifelse(ndata$id == ndata$case.id, 1, 0)
ndata$event <- factor(ifelse(ndata$death==1, 4, ndata$mc), 0:4,
                      c("censor", "1 MC", "2 MC", "3 MC", "death"))
ndata$istate <- factor(ndata$istate, 0:3, c("0 MC", "1 MC", "2 MC", "3 MC"))
attr(ndata, "tcount")
@ 

The \code{tcount} attribute gives a history of the additions.
\begin{itemize}
  \item The deaths all occur at the end of a participant's follow-up time
(trailing), which is as it should be.
  \item Many of the metabolic comorbidities are pre-existing, they occur 
    before participants were enrolled in the NAFLD study (early). 
  \item There are 575 tied values for the MC, for example a patient who had 0
    comorbidies at the prior visit and 2 at the current one: they will have 2
    comorbidities coded on the same day.
  \item There are 4 comorbidities that occur on the day of enrollment.  
    (What happens with these?  I think they do the right thing, but check.)
\end{itemize}

In this study 318 of the 17590 subject who entered the study as controls for
a NAFLD case were later diagnosed themselves with the disease.
Survival curves do not extend to time-dependent covariates, nor does the
concept of mean time in state, so for this analysis we use NAFLD at study entry
as the covariate of interest.
Start the analysis with a check of the data set.

<<nafld0, size='small'>>=
temp <- survcheck(Surv(age1, age2, event) ~1, data=ndata, 
                    id=id, istate=istate)
temp
@ 

The second table above shows that no participant had more than 1 transition to
a given state (no repeats), which is the intention. 
The majority of the subjects (12733/17549 = 73\%) had no transitions during
their follow-up, while 22 of them visited all 5 states.
The first table shows that there are subjects who jump states, e.g., 
directly from 0MC to 2MC;
on one visit they had no MC and the next visit, perhaps several years later,
2 MC were observd. 
This highlights that we are building a model for time to diagnosis of a condition
rather than time to occurrence. 
More importantly, the check did not reveal any reversions from a greater
state to a lesser, or other serious errors.

<<nafld1, fig=TRUE>>=
nfit1 <- survfit(Surv(age1, age2, event) ~ 1, data=ndata,
                 id = id, istate=istate, start.time=40)
nfit1$start.time <- 40  # oversight in survfit, fixed in 3.1-8
plot(nfit1, col=1:5, lwd=2, xlab="Age", ylab="P(state)")
legend(50, .95, nfit1$states, lty=1, lwd=2, col=1:5, bty='n')
@ 

With (time1, time2) data very early time points can have unstable 
P(state) estimates due
to a small number of subjects at risk; we decided to start the curves at age 40
to avoid this.
The set of subjects at risk at age 40 (at risk for further events) 
defines the initial
probabilities, thus by definition 0\% begin in the death state.
The curves then give future estimates for this cohort of 40 year olds.
The probability in state curves show that at age 40 about 60\% will be in the
0MC state, while the 1MC state peaks at 33\% in the early 50s and the 3MC
state peaks at 26\% in the 80s.
To obtain the estimated prevalence of states among those still alive (the
common epidemiologic definition of prevalence), one needs to divide each
curve by the fraction still alive, and omit the death curve.

It is also interesting to look at the estimated future for a 
hypothetical cohort of
subjects who were all in the 0MC state at age 40, as is shown below.
Looking forward, the NAFLD subject migrates out of the 0MC state more
quickly, 1MC and 2MC peak earlier, and there is a higher prevalence of
3MC.  Basically, NAFLD subjects have an earlier onset of MC.

<<nafld2, fig=TRUE>>=
nfit2 <- survfit(Surv(age1, age2, event) ~ nafld, data=ndata,
                 id = id, istate=istate, start.time=40,
                 p0=c(1,0,0,0,0), influence=1)

plot(nfit2[,-5], col=rep(1:4,each=2), lty=1:2, lwd=2, 
     xlab="Age", ylab="P(state)")
legend(80, .95, c(nfit1$states[-5], "NAFLD"), 
       lty=c(1,1,1,1,2), lwd=2, col=c(1,2,3,4,1), bty='n')
@ 

We can summarize this in terms of the mean time in state or RMTS.
(RMST isn't as nice an acronym for the general case.)
The NAFLD subjects spend less total time in the 0MC and 1MC states,
and more in the 3MC and death states.

<<natime>>=
mtable <- summary(nfit2, rmean=100)$table
# print(mtable, digits=2)   # simple printout
# format it more nicely
mtemp <- matrix(mtable[,3], ncol=2, byrow=2)
mse   <- matrix(mtable[,4], ncol=2, byrow=2) 
tfun <- function(x, se, digits=2) 
    paste0(format(round(x, digits)), " (",
           format(round(se,digits)), ")")

temp  <- cbind("Control" = tfun(mtemp[,1], mse[,1]),
               "NAFLD"   = tfun(mtemp[,2], mse[,2]),
               "Difference" = tfun(mtemp[,2] - mtemp[,1],
                                sqrt(mse[,1]^2 + mse[,2]^2)))
rownames(temp) <- nfit2$states
print(temp, quote=FALSE)
@ 

Now look at the same comparison using pseudo values.  
The \code{survfit} call to create them will not include covariates.
With this approach the total size of the influence
matrix grows as $n^2$, since both the number of subjects and the number
of unique event times is larger, which can cause issues with very large
data sets.

<<nfit3>>=
nfit3 <- survfit(Surv(age1, age2, event) ~ 1, data=ndata,
                 id = id, istate=istate, start.time=40,
                 p0=c(1,0,0,0,0), influence = 1)
nfit3$start.time <- 40  # oversight, fixed in survival_3.1-8
dim(nfit3$influence.pstate)

# updated function for multi-state
rmst <- function(fit, tmax) {
    # extract the RMST, and the leverage of each subject on the RMST
    if (missing(tmax)) tmax <- max(sfit0$time)
    if (length(tmax) !=1) stop("tmax must be a single value")

    sfit0 <- survfit0(fit)      # add the time=0 point to the result
    rsum <- function(y, x= sfit0$time) {  # sum of rectangles
        keep <- which(x < tmax)
        width <- diff(c(x[keep], tmax))  
        sum(width * y[keep])
    }
    
    if (is.null(sfit0$states)) { # ordinary survival
        rmst <- rsum(sfit0$surv, sfit0$time)
        ijack <- apply(sfit0$influence.surv, 1, rsum)
        list(rmst=rmst, sd.rmst = sqrt(sum(ijack^2)), 
             pseudo = rmst + (sfit$n -1)* ijack)
    }
    else {
        nstate <- length(sfit0$states)
        rmst <- apply(sfit0$pstate, 2, rsum)
        ijack <- apply(sfit0$influence.pstate, c(1,3), rsum)
        list(rmst=rmst, sd.rmst = sqrt(colSums(ijack^2)),
             pseudo = rep(rmst, each= nrow(ijack)) + (nrow(ijack)-1)*ijack)
    }
}
# go up to age 100
nmean <- rmst(nfit3, 100)
sum(nmean$rmst)   # will sum to 60

# Subjects censored before age 40 do not appear in the survival curve
#  and thus not in the pseudovalues
tdata <- subset(nafld1, (age + futime/365) > 40)
tdata$nafld <- ifelse(tdata$case.id == tdata$id, 1, 0)

nfit4 <- lm(nmean$pseudo ~ nafld, data= tdata)
nfit4
# summmary(nfit4)   longer, simple printout
# make a compact one
temp2 <- summary(nfit4)
temp3 <- cbind(estimate = sapply(temp2, function(x) x$coefficients[2,1]),
               se       = sapply(temp2, function(x) x$coefficients[2,2]))
rownames(temp3) <- nfit3$states
round(temp3, 2)
@ 

The estimated RMTS for the large control group is nearly identical for the
direct and pseudo-value approachs (intercept coefficients).
The estimated difference in RMTS between NAFLD and control is somewhat smaller
using pseudo values and the standard errors are comparable.
The big advantage for the pseudo value approach is the ease of fitting
additional multivariate models for RMTS values, e.g.,

<<nafld4>>=
lm(nmean$pseudo ~ nafld + male + bmi, tdata)
@ 

This estimates that a 40 year old male with 0MC will spend 4.3 more years,
out of the next 60, in the death state, after adjustment for BMI and NAFLD;
higher death rates for males are what we would have expected.  
Each 1 unit increase in body mass index (BMI) is estimated to result
in .3 years less in the 0MC state, .3 years more in the 3MC state, but only
a modest change in overall survival; again after adjustment.
The NAFLD effects on metabolic state are attenuated by the adjustment but
the effect on overall survival (time in the death state) does not change
substantially from the unadjusted model.
Together these raise tantalizing questions about possible underlying
biological mechanisms. 

\end{document}
