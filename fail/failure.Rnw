\documentclass{article}
\title{Overflow example}
\newcommand{\code}[1]{\texttt{#1}}
\begin{document}

\section{Overflow example}
<<readin>>=
adata <- read.csv('overflow.csv')
dim(adata)
etime <- table(adata$end[adata$status==1])
sum(etime)
length(etime)
etime
@ 
This data set has 240 thousand lines and 145 thousand events, but only
48 unique event times.  This makes it possible to debug issues with the
code in a simple fashion.  The time variable has been changed to 1, 2,
\ldots to make things even simpler.  There are a \emph{lot} of tied events.

<<skew>>=
with(adata, quantile(v2, 0:10/10))
with(adata, quantile(v2, 990:1000/1000))

b <- seq(0, .007, length=15)
logb <- double(15)
for (i in 1:15) 
    logb[i] <- coxph(Surv(start, end, status) ~ v2, data=adata, iter=0,
                     init=b[i])$loglik[1]
plot(b, logb/1e5)
@ 
The first problem with the data set is variable \code{v2} which is terribly
skewed.  
The second is that the log-likelihood is asymmetric around the maximum of
about .004.  

<<initial>>=
fit0 <- coxph(Surv(start, end, status) ~ v2, data=adata, iter=0)
dt0 <- coxph.detail(fit0)
beta1 <- sum(dt0$score)/ sum(dt0$imat)
beta1
@ 

The very first iteration of the program takes far too large
a step; the initial value for $\hat beta$ is approximately 25 times
too large.
The effect of this on our calculations is catastrophic: here are the
quantiles of the risk weights $\exp(X \beta)$.

<<risk>>=
rwt <- exp(.1 * (adata$v2 - mean(adata$v2)))
wt2 <- sort(rwt/sum(rwt))
cumsum(rev(wt2))[1:5]
@ 

The largest observation has 55\% of the total weight, the first 2 99.5\% and
the first 5 99.99\%. 
Look at what happens to our calculation of the variance, which uses the centered
values. Compute the mean and variance at each of the 48 death times.

<<var>>=
vmean <- vvar <- double(48)
for (i in 1:48) {
    atrisk <- with(adata, which(start < i & end >= i))
    denom <- sum(rwt[atrisk])
    vmean[i] <- sum((rwt*adata$v2)[atrisk])/ denom
    vvar[i]  <- sum(rwt[atrisk]*(adata$v2[atrisk] - vmean[i])^2)  / denom
}
signif(vmean,4)
signif(vvar, 3)
@ 

The standard formula used by the routine is below, where $m$ is
the current working definition of the mean and the weights are 
scaled so as to add to 1.
\begin{equation}
  \sum w_i (x_i - \bar x)^2 = \sum w_i(x-m)^2 - (m- \bar x)^2 \label{vvar}
\end{equation}
The routine uses the grand mean for $m$ (\Sexpr{round(mean(adata$v2), 3)}), %$
while $\bar x$ is the actual mean over the current risk set.
With 16 digits of accuracy, the formula is doomed to fail for any term where
the the true variance is proprotional to the 16th or smaller digit of the 
second term in equation eqref{vvar}.  That is, for almost all of the 48 death
times.

The log-likelihood ends up as NaN for this estimate as well, for essentially
the same reason.  The contribution to the partial likelihood is the probability
for the subject who perished = $w_i/ \sum w_j$ where $i$ indexes the death
and $j$ the risk set.  This fraction becomes 0 due to round off error and
the log is undefined.  

The update formulas do hold in the neighborhood of the MLE, which is between
.003 and .004.  
<<try4>>=
fit4 <- coxph(Surv(start, end, status) ~ v2, adata, iter=0, init=.004)
rwt <- exp(.04 * (adata$v2 - mean(adata$v2)))
for (i in 1:48) {
    atrisk <- with(adata, which(start < i & end >= i))
    denom <- sum(rwt[atrisk])
    vmean[i] <- sum((rwt*adata$v2)[atrisk])/ denom
    vvar[i]  <- sum(rwt[atrisk]*(adata$v2[atrisk] - vmean[i])^2)  / denom
}
signif(vmean,4)
signif(vvar, 3)
@ 

The iteration succeeds if step halving is invoked whenever the 
loglik is infinite or NaN, the information matrix drops in rank, or the 
step has increased the loglikelihood.
<<fit>>= 
fit <- coxph(Surv(start, end, status) ~ v2, adata)
quantile(exp(fit$linear.predictor), c(0, .5, .9, .99, .999, 1))
@
The utility of the fit remains dubious, however; the final model is driven
by a miniscule fraction of the subjects (16 out of 240 thousand) who have risks
of 100 fold.

\section{Corrleation example}
This example was pointed out by Brian Borstrom and uses the mort data
set from the eha package.
The data set contains mortality data from a parish in northern Sweden, for
all males born in the years 1800--1820 and who survived to age 40, 
followed until death or their 60th birthday.  
The start-stop aspect is due to subjects who change social strata.

<<mort1>>=
load('mort.rda')
mort[1:5,]
fita <- coxph(Surv(enter, exit, event) ~ ses*birthdate, mort, x=T)
cor(fita$x)
svd(fita$var)$d
@ 
A singular value decomposition shows that the information
matrix has a condition number of about $10^{11}$, which is large.
(The ratio of the largest and smallest singular values.)
The cholesky decomposition is still stable. 
The very high correlation, however, makes this susceptible to round off
errors.  Centering the data after forming the interactions does not
help with this.

When the data is pre-centered the correlation number is much more
sensible and the condtion number is on the order of 1500, making it
a well behaved problem.
However, the number of iterations is exactly the same.
This fact was a surprise to me when I first encountered it. 
When using Newton-Raphson iteration the iteration path is completely
invariant to any affine transformation of the covariates,
as long as accuracy is not lost.  
Centering does change the ses coefficient, but not the predictions.
<<center>>=
birth2 <- mort$birthdate -1800
fitb <-  coxph(Surv(enter, exit, event) ~ ses*birth2, mort, x=TRUE)
cor(fitb$x)
svd(fitb$var)$d
all.equal(predict(fita), predict(fitb))
@ 

In the 2015 version of the library, iteration for this simple model got stuck,
while the eha package succeeded (pointed out by Goran Borstrom).
A small mistake in the information matrix sufficed.
We can force failure in the current code making year less centered, but it
requires a strong shift.
<<center2>>=
birth3 <- mort$birthdate + 1e7
fitc <-  coxph(Surv(enter, exit, event) ~ ses*birth3, mort, x=TRUE)
@ 

\section{Sliding mean}
This is synthetic data that follows a particular example.  The covariate was
the number of nursing shift changes, with a hypothesis that each hand off
increased the chance of an adverse event. 
The number of changes moves almost in lockstep for subjects: the normal shift
is 8 hours during the week and 12 on the weekend.  
For this reason the covariate turned out to be uninteresting, but it did cause
the routine to fail.

<<sliding>>=
fit0 <- coxph(Surv(time, status) ~ ph.ecog + age, lung) 
sdata <- survSplit(lung, seq(5, 1000, by=50), end="time", event="status")
sfit0 <- coxph(Surv(tstart, time, status) ~ ph.ecog + age, sdata)
all.equal(fit0$coef, sfit0$coef)

sdata$fakeph <- sdata$ph.ecog + sdata$tstart
sfit <- coxph(Surv(tstart, time, status) ~ fakeph + age, sdata)


temp <- survSplit(lung[1:10, 2:6], seq(100, 1000, 100), end="time", event="status")
@ 

<<dummy>>=
# start simple
test2 <- data.frame(start=c(1, 2, 5, 2, 1, 7, 3, 4, 8, 8),
                    stop =c(2, 3, 6, 7, 8, 9, 9, 9,14,17),
                    event=c(1, 1, 1, 1, 1, 1, 1, 0, 0, 0),
                    x    =c(1, 0, 0, 1, 0, 1, 1, 1, 0, 0) )
test2b <- survSplit(test2, c(6.,9.), end='stop', event='event', start='start')
fit1 <- coxph(Surv(start, stop, event) ~ x, test2)
fit2 <- coxph(Surv(start, stop, event) ~ x, test2b)

@ 
\end{document}
