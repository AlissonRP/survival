\documentclass{article}
\title{Overflow example}
\newcommand{\code}[1]{\texttt{#1}}
\begin{document}

\section{Overflow examples}
\subsection{Skewed covariate}
<<readin>>=
adata <- read.csv('overflow.csv')
dim(adata)
etime <- table(adata$end[adata$status==1])
sum(etime)
length(etime)
etime
@ 
This data set has 240 thousand lines and 145 thousand events, but only
48 unique event times.  This makes it possible to debug issues with the
code in a simple fashion.  The time variable has been changed to 1, 2,
\ldots to make things even simpler.  There are a \emph{lot} of tied events.
The plot shows the partial likelihood as points, and the Newton-Raphson
quadratic approximation at $\beta=0$ as a line.

<<skew>>=
with(adata, quantile(v2, 0:10/10))
with(adata, quantile(v2, 990:1000/1000))
fit0 <- coxph(Surv(start, end, status) ~ v2, data=adata, iter=0)
dt0 <- coxph.detail(fit0)

b <- seq(0, .007, length=15)
logb <- matrix(0, 15, 2)
for (i in 1:15) {
    logb[i,1] <- coxph(Surv(start, end, status) ~ v2, data=adata, iter=0,
                     init=b[i])$loglik[1]
    logb[i,2] <- fit0$loglik[1] + b[i]*sum(dt0$score) - b[i]^2*sum(dt0$imat)/2
}
matplot(b, logb/1e5, type='n', xlab='beta', ylab="Loglik/1e5")
lines(b, logb[,2]/1e5, lty=2)
points(b, logb[,1]/1e5)
@ 

The first problem with the data set is variable \code{v2} which is terribly
skewed.  
The second is that the log-likelihood is asymmetric around the maximum of
about .004.  

<<initial>>=
beta1 <- sum(dt0$score)/ sum(dt0$imat)
beta1
@ 

The very first iteration of the program takes far too large
a step; the initial value for $\hat beta$ is approximately 30 times
too large.  
The effect of this on our calculations is catastrophic: here are the
quantiles of the risk weights $\exp(X \beta)$.

<<risk>>=
rwt <- exp(.1 * (adata$v2 - mean(adata$v2)))
wt2 <- sort(rwt/sum(rwt))
cumsum(rev(wt2))[1:5]
@ 

The largest observation has 55\% of the total weight, the first 2 99.5\% and
the first 5 99.99\%. 
Look at what happens to our calculation of the variance, which uses the centered
values. Compute the mean and variance at each of the 48 death times.

<<var>>=
vmean <- vvar <- double(48)
for (i in 1:48) {
    atrisk <- with(adata, which(start < i & end >= i))
    denom <- sum(rwt[atrisk])
    vmean[i] <- sum((rwt*adata$v2)[atrisk])/ denom
    vvar[i]  <- sum(rwt[atrisk]*(adata$v2[atrisk] - vmean[i])^2)  / denom
}
signif(vmean,4)
signif(vvar, 3)
@ 

The standard formula used by the routine is below, where $m$ is
the current working definition of the mean and the weights are 
scaled so as to add to 1.
\begin{equation}
  \sum w_i (x_i - \bar x)^2 = \sum w_i(x-m)^2 - (m- \bar x)^2 \label{vvar}
\end{equation}
The routine uses the grand mean for $m$ (\Sexpr{round(mean(adata$v2), 3)}), %$
while $\bar x$ is the actual mean over the current risk set.
With 16 digits of accuracy, the formula is doomed to fail for any term where
the the true variance is proprotional to the 16th or smaller digit of the 
second term in equation eqref{vvar}.  That is, for almost all of the 48 death
times.

The log-likelihood ends up as NaN for this estimate as well, for essentially
the same reason.  The contribution to the partial likelihood is the probability
for the subject who perished = $w_i/ \sum w_j$ where $i$ indexes the death
and $j$ the risk set.  This fraction becomes 0 due to round off error and
the log is undefined.  

The update formulas do hold in the neighborhood of the MLE, which is between
.003 and .004.  
<<try4>>=
fit4 <- coxph(Surv(start, end, status) ~ v2, adata, iter=0, init=.004)
rwt <- exp(.04 * (adata$v2 - mean(adata$v2)))
for (i in 1:48) {
    atrisk <- with(adata, which(start < i & end >= i))
    denom <- sum(rwt[atrisk])
    vmean[i] <- sum((rwt*adata$v2)[atrisk])/ denom
    vvar[i]  <- sum(rwt[atrisk]*(adata$v2[atrisk] - vmean[i])^2)  / denom
}
signif(vmean,4)
signif(vvar, 3)
@ 

The iteration succeeds if step halving is invoked whenever the 
loglik is infinite or NaN, the information matrix drops in rank, or the 
step has increased the loglikelihood.
<<fit>>= 
fit <- coxph(Surv(start, end, status) ~ v2, adata)
quantile(exp(fit$linear.predictor), c(0, .5, .9, .99, .999, 1))
@
The utility of the fit remains dubious, however; the final model is driven
by a miniscule fraction of the subjects (16 out of 240 thousand) who have risks
of 100 fold.

\subsection{Offset}
A nastier problem is found as a test case in the model4you package.  It is
an artificial data set shown below.
<<m4>>=
set.seed(1212)
n <- 90
d1 <- data.frame(y = abs(rnorm(n) +5) + .5, x= 1:n -10,
                    trt= rep(1:3, each=n/3))
mfit0 <- coxph(Surv(y) ~ trt + offset(x), data=d1, iter=0)
dtm <- coxph.detail(mfit0)
mcoef <- sum(dtm$score)/sum(dtm$imat)
mcoef

beta <- seq(-40, 5, by=1)
loglik <- double(length(beta))
for (i in 1:length(beta)) {
    tfit <- coxph(Surv(y) ~ trt + offset(x), iter=0, init=beta[i], d1)
    loglik[i] <- tfit$loglik[1]
}
plot(beta, loglik)

mfit30 <- coxph(Surv(y) ~ trt + offset(x), data=d1, init=-30)
mfit30
@ 

In this case the starting point of 0 is over 170 standard errors from the
solution, due to the bizarre offset.
The partial likelihood has so little curvature at 0 that the first first
estimate of $\beta$ is so large that the risk scores $\exp(X\beta)$ contain
infinite values, and the routine fails at the first iteration.
Step halving would eventially get to a decent answer, in about 40 steps, but
the routine packs it in after 20.  

\subsection{Trust region}
A trust region approach asks the question: over what region is the Newton-Raphson
step reliable?  It then takes the maximum step within the reliable region.
Reliable is normally defined as $r$ =(actual improvement)/(predicted improvement)
$> .25$.  
If $r < .25$ then the trust region is made smaller, if $r > .5$ it is made
bigger.
Because of the equivalence of penalties and limits, a Levenberg-Marquardt
step of $d = -(H + \lambda I)^{-1} U$ is the maximum of the quadratic NR
update under a
constraint that $\sum d^2 \le c$ for some constant $c$, $\lambda$ playing the
role of a Lagrange multiplier.
When there is a single covariate this is no different than step halving, with
$\lambda =  kH$ and $k$ of $1,3, 5, 7, \ldots$.  
But it does give a rationale for tuning the amount of shrinkage.

So one simple way to calibrate a trust region approach is to calibrate $\lambda$.


\section{Corrleation example}
This example was pointed out by Brian Borstrom and uses the mort data
set from the eha package.
The data set contains mortality data from a parish in northern Sweden, for
all males born in the years 1800--1820 and who survived to age 40, 
followed until death or their 60th birthday.  
The start-stop aspect is due to subjects who change social strata.

<<mort1>>=
load('mort.rda')
mort[1:5,]
fita <- coxph(Surv(enter, exit, event) ~ ses*birthdate, mort, x=T)
cor(fita$x)
svd(fita$var)$d
@ 
A singular value decomposition shows that the information
matrix has a condition number of about $10^{11}$, which is large.
(The ratio of the largest and smallest singular values.)
The cholesky decomposition is still stable. 
The very high correlation, however, makes this susceptible to round off
errors.  Centering the data after forming the interactions does not
help with this.

When the data is pre-centered the correlation number is much more
sensible and the condtion number is on the order of 1500, making it
a well behaved problem.
However, the number of iterations is exactly the same.
This fact was a surprise to me when I first encountered it. 
When using Newton-Raphson iteration the iteration path is completely
invariant to any affine transformation of the covariates,
as long as accuracy is not lost.  
Centering does change the ses coefficient, but not the predictions.
<<center>>=
birth2 <- mort$birthdate -1800
fitb <-  coxph(Surv(enter, exit, event) ~ ses*birth2, mort, x=TRUE)
cor(fitb$x)
svd(fitb$var)$d
all.equal(predict(fita), predict(fitb))
@ 

In the 2015 version of the library, iteration for this simple model got stuck,
while the eha package succeeded (pointed out by Goran Borstrom).
A small mistake in the information matrix sufficed.
We can force failure in the current code making year less centered, but it
requires a strong shift.
<<center2>>=
birth3 <- mort$birthdate + 1e7
fitc <-  coxph(Surv(enter, exit, event) ~ ses*birth3, mort, x=TRUE)
@ 

\section{Sliding mean}
This is synthetic data that follows a particular example.  The covariate was
the number of nursing shift changes, with a hypothesis that each hand off
increased the chance of an adverse event. 
The number of changes moves almost in lockstep for subjects: the normal shift
is 8 hours during the week and 12 on the weekend.  
For this reason the covariate turned out to be uninteresting, but it did cause
the routine to fail.

<<sliding>>=
fit0 <- coxph(Surv(time, status) ~ ph.ecog + age, lung) 
sdata <- survSplit(Surv(time, status) ~., lung, cut=seq(5, 1000, by=50))
sfit0 <- coxph(Surv(tstart, time, status) ~ ph.ecog + age, sdata)
all.equal(fit0$coef, sfit0$coef)

sdata$fakeph <- sdata$ph.ecog + sdata$tstart
sfit <- coxph(Surv(tstart, time, status) ~ fakeph + age, sdata)
@ 


\section{Infinite coefficients}
For an infinite coefficient, after the first few iterations the coefficient
grows by an approximate constant at each iteration, while the loglik 
asymptotes to a constant.
There is a race condition:
\begin{enumerate}
  \item Convergence of the loglik
  \item Overflow of the exp function
  \item Singularity in the computed information matrix
\end{enumerate}

Data sets where one group has no events usually end up at condition 1.
Ones that do the other are more trouble.

This example comes from the moonBook package.  It's a sneaky
way to get a variable on both sides, and fails with number 2.
<<sneaky>>=
ysurv <- Surv(colon$time, colon$status)
stest <- coxph(ysurv ~ time, colon)
@ 

The Rehberg data set manages 2 and 3 at the same iteration, which forced a
different failure in the code: an NaN in the infinite coefficient check.
<<rehberg>>=
rdata <- read.csv('rdata.csv')
fit <- coxph(Surv(time, status) ~x, rdata)  

options(warn=-1)
tcoef <- tlog <- timat <- tscore <- double(17)
for (i in 1:17) {
    tfit <- coxph(Surv(time, status) ~x, rdata, iter=i)
    tcoef[i] <- tfit$coef
    tlog[i] <-  tfit$loglik[2]
    dt <- coxph.detail(tfit)
    tscore[i] <- sum(dt$score)
    timat[i]  <- sum(dt$imat)
}
options(warn=0)

#plot(tcoef, tlog, xlab="Coefficient", ylab="Loglik")
plot(tcoef, -tlog, log='y', xlab="Coefficient", ylab= "-loglik")
matplot(1:17, cbind(timat, tscore), pch='is', log='y',
        xlab="Iteration", ylab="Score and Information")
@ 

At iteration 17 the computations for both the first and second derivative
fail due to exp() overflow.

\end{document}
