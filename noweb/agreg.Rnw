\subsection{Anderson-Gill fits}
When the survival data set has (start, stop] data a couple of computational
issues are added.  
A primary one is how to do this compuation efficiently.
At each event time we need to compute 3 quantities, each of them added up 
over the current risk set.
\begin{itemize}
  \item The weighted sum of the risk scores $\sum w_i r_i$ where
    $r_i = \exp(\eta_i)$ and $\eta_i = x_{i1}\beta_1 + x_{i2}\beta_2 +\ldots$
    is the current linear predictor.
  \item The weighted mean of the covariates $x$, with weight $w_i r_i$.
  \item The weighted variance-covariance matrix of $x$.
\end{itemize}
The current risk set at some event time $t$ is the set of all (start, stop]
intervals that overlap $t$, and are part of the same strata. 
The round/square brackets in the prior sentence are important: for an event time
$t=20$ the interval $(5,20]$ is considered to overlap $t$ and the interval
$(20,55]$ does not overlap $t$.
    
Our routine for the simple right censored Cox model computes these efficiently
by keeping a cumulative sum.  Starting with the longest survival move
backwards through time, adding subjects to the sums as we go.
For this routine we also move backwards through time, and subjects both
enter and leave the sums.  
The code below creates two sort indices, one orders the data by reverse stop
time and the other by reverse start time.

The fit routine is called by the coxph function with arguments
\begin{description}
  \item[x] matrix of covariates
  \item[y] three column matrix containing the start time, stop time, and event
    for each observation
  \item[strata] for stratified fits, the strata of each subject
  \item[offset] the offset, usually a vector of zeros
  \item[init] initial estimate for the coefficients
  \item[control] results of the coxph.control function
  \item[weights] case weights, often a vector of ones.
  \item[method] how ties are handled: 1=Breslow, 2=Efron
  \item[rownames] used to label the residuals
\end{description}

<<agreg.fit>>=
agreg.fit <- function(x, y, strata, offset, init, control,
			weights, method, rownames)
    {
    n <- nrow(y)
    nvar <- ncol(x)
    start <- y[,1]
    stopp <- y[,2]
    event <- y[,3]
    if (all(event==0)) stop("Can't fit a Cox model with 0 failures")

    # Sort the data (or rather, get a list of sorted indices)
    #  For both stop and start times, the indices go from last to first
    if (length(strata)==0) {
	sort.end  <- order(-stopp, event) -1L #indices start at 0 for C code
	sort.start<- order(-start) -1L
	newstrat  <- n
	}
    else {
	sort.end  <- order(strata, -stopp, event) -1L
	sort.start<- order(strata, -start) -1L
	newstrat  <- cumsum(table(strata))
	}
    if (missing(offset) || is.null(offset)) offset <- rep(0.0, n)
    if (missing(weights)|| is.null(weights))weights<- rep(1.0, n)
    else if (any(weights<=0)) stop("Invalid weights, must be >0")
    else weights <- as.vector(weights)

    if (is.null(nvar) || nvar==0) {
	# A special case: Null model.  Just return obvious stuff
        #  To keep the C code to a small set, we call the usual routines, but
	#  with a dummy X matrix and 0 iterations
	nvar <- 1
	x <- matrix(as.double(1:n), ncol=1)  #keep the .C call happy
	maxiter <- 0
	nullmodel <- TRUE
        if (length(init) !=0) stop("Wrong length for inital values")
        init <- 0.0  #dummy value to keep a .C call happy (doesn't like 0 length)
	}
    else {
	nullmodel <- FALSE
	maxiter <- control$iter.max
        
        if (is.null(init)) init <- rep(0., nvar)
	if (length(init) != nvar) stop("Wrong length for inital values")
	}

    # the returned value of agfit$coef starts as a copy of init, so make sure
    #  is is a vector and not a matrix; as.double does so.
    # Solidify the storage mode of other arguments
    storage.mode(y) <- storage.mode(x) <- "double"
    storage.mode(offset) <- storage.mode(weights) <- "double"
    storage.mode(newstrat) <- "integer"
    agfit <- .Call(Cagfit4, 
                   y, x, newstrat, weights, 
                   offset,
                   as.double(init), 
                   sort.end, sort.start, 
                   as.integer(method=="efron"),
                   as.integer(maxiter), 
                   as.double(control$eps),
                   as.double(control$toler.chol),
                   as.integer(1)) # internally rescale
    <<agreg-fixup>>
    <<agreg-finish>>
}  
@

Upon return we need to clean up two simple things.
The first is that if any of the covariates were redudant then this
will be marked by zeros on the diagonal of the variance matrix.
Replace these coefficients and their variances with NA.
The second is to post a warning message about possible infinite coefficients.
The algorithm for determining this is unreliable, unfortunately.  
Sometimes coefficients are marked as infinite when the solution is not tending
to infinity (usually associated with a very skewed covariate), and sometimes
one that is tending to infinity is not marked.  Que sera sera.
<<agreg-fixup>>=
var <- matrix(agfit$imat,nvar,nvar)
coef <- agfit$coef
if (agfit$flag < nvar) which.sing <- diag(var)==0
else which.sing <- rep(FALSE,nvar)

infs <- abs(agfit$u %*% var)
if (maxiter >1) {
    if (agfit$iter > maxiter)
        warning("Ran out of iterations and did not converge")
    else {
        infs <- ((infs > control$eps) & 
                 infs > control$toler.inf*abs(coef))
        if (any(infs))
            warning(paste("Loglik converged before variable ",
                          paste((1:nvar)[infs],collapse=","),
				      "; beta may be infinite. "))
    }
}
@ 

The last of the code is very standard.  Compute residuals and package
up the results.
<<agreg-finish>>=
lp  <- as.vector(x %*% coef + offset - sum(coef * colMeans(X)))
score <- as.double(exp(lp))
resid <- .Call(Cagmart3,
               y, score, weights,
               newstrat,
               cbind(sort.end, sort.start),
               as.integer(method=='efron'))
names(resid) <- rownames
if (nullmodel) {
    list(loglik=agfit$loglik[2],
         linear.predictors = offset,
         residuals = resid,
         method= c("coxph.null", 'coxph') )
}
else {
    names(coef) <- dimnames(x)[[2]]
    coef[which.sing] <- NA

    concordance <- survConcordance.fit(y, lp, strata, weights) 
    list(coefficients  = coef,
         var    = var,
         loglik = agfit$loglik,
         score  = agfit$sctest,
         iter   = agfit$iter,
         linear.predictors = as.vector(lp),
         residuals = resid,
         means = colMeans(X),
         concordance = concordance,
         method= 'coxph')
}
@

The details of the C code contain the more challenging part of the
computations.
It starts with the usual dull stuff.
My standard coding style for a variable zed to to use
[[zed2]] as the variable name for the R object, and [[zed]] for
the pointer to the contents of the object, i.e., what the
C code will manipulate.
For the matrix objects I make use of ragged arrays, this
allows for reference to the i,j element as \code{cmat[i][j]}
and makes for more readable code.

<<agfit4>>=
#include <math.h>
#include "survS.h" 
#include "survproto.h"

SEXP agfit4(SEXP surv2,      SEXP covar2,    SEXP strata2,
	    SEXP weights2,  SEXP offset2,    SEXP ibeta2,
	    SEXP sort12,     SEXP sort22,    SEXP method2,
	    SEXP maxiter2,   SEXP  eps2,     SEXP tolerance2,
	    SEXP doscale2) { 

    int i,j,k, ii, person;
    int indx2, istrat, p;
    int nrisk;
    int nused, nvar;
 
    double **covar, **cmat, **imat;  /*ragged array versions*/
    double *a, *oldbeta;
    double *scale;
    double *a2, **cmat2;
    double *eta;
    double  denom, zbeta, risk;
    double  time;
    double  temp, temp2;
    double  newlk =0;
    int     halving;    /*are we doing step halving at the moment? */
    double  tol_chol, eps;
    double  meanwt;
    int deaths;
    double denom2, meaneta;

    /* inputs */
    double *start, *stop, *event;
    double *weights, *offset;
    int *sort1, *sort2, maxiter;
    int *strata, nstrat;
    double method;  /* saving this as double forces some double arithmetic */
    int doscale;

    /* returned objects */
    SEXP imat2, beta2, u2, loglik2;
    double *beta, *u, *loglik;
    SEXP sctest2, flag2, iter2;
    double *sctest;
    int *flag, *iter;
    SEXP rlist;
    static const char *outnames[]={"coef", "u", "imat", "loglik",
				   "sctest", "flag", "iter", ""};
    int nprotect;  /* number of protect calls I have issued */

    /* get sizes and constants */
    nused = nrows(covar2);
    nvar  = ncols(covar2);
    method= asInteger(method2);
    eps   = asReal(eps2);
    tol_chol = asReal(tolerance2);
    maxiter = asInteger(maxiter2);
    doscale = asInteger(doscale2);
    nstrat = LENGTH(strata2);
  
    /* input arguments */
    start = REAL(surv2);
    stop  = start + nused;
    event = stop + nused;
    weights = REAL(weights2);
    offset = REAL(offset2);
    sort1  = INTEGER(sort12);
    sort2  = INTEGER(sort22);
    strata = INTEGER(strata2);

    /*
    ** scratch space
    **  nvar: a, a2, oldbeta, scale
    **  nvar*nvar: cmat, cmat2
    **  n: eta
    */
    eta = (double *) R_alloc(nused + 5*nvar + 2*nvar*nvar, sizeof(double));
    a = eta + nused;
    a2= a + nvar;
    scale  = a2 + nvar;
    oldbeta = scale + nvar;   

    /*
    **  Set up the ragged arrays
    **  covar2 might not need to be duplicated, even though
    **  we are going to modify it, due to the way this routine was
    **  was called.  In this case NAMED(covar2) will =0
    */
    PROTECT(imat2 = allocVector(REALSXP, nvar*nvar));
    nprotect =1;
    if (NAMED(covar2)>0) {
	PROTECT(covar2 = duplicate(covar2)); 
	nprotect++;
	}
    covar= dmatrix(REAL(covar2), nused, nvar);
    imat = dmatrix(REAL(imat2),  nvar, nvar);
    cmat = dmatrix(oldbeta+ nvar,   nvar, nvar);
    cmat2= dmatrix(oldbeta+ nvar + nvar*nvar, nvar, nvar);

    /*
    ** create the output structures
    */
    PROTECT(rlist = mkNamed(VECSXP, outnames));
    nprotect++;
    beta2 = SET_VECTOR_ELT(rlist, 0, duplicate(ibeta2));
    beta  = REAL(beta2);

    u2 =    SET_VECTOR_ELT(rlist, 1, allocVector(REALSXP, nvar));
    u = REAL(u2);

    SET_VECTOR_ELT(rlist, 2, imat2);
    loglik2 = SET_VECTOR_ELT(rlist, 3, allocVector(REALSXP, 2)); 
    loglik  = REAL(loglik2);

    sctest2 = SET_VECTOR_ELT(rlist, 5, allocVector(REALSXP, 1));
    sctest =  REAL(sctest2);
    flag2  =  SET_VECTOR_ELT(rlist, 6, allocVector(INTSXP, 1));
    flag   =  INTEGER(flag2);
    iter2  =  SET_VECTOR_ELT(rlist, 7, allocVector(INTSXP, 1));
    iter = INTEGER(iter2);
    
    /*
    ** Subtract the mean from each covar, as this makes the variance
    **  computation much more stable.  The mean is taken per stratum,
    **  the scaling is overall.
    */
    for (i=0; i<nvar; i++) {
        person=0;
        for (istrat=0; istrat<nstrat; istrat++) {
            temp=0;
            temp2 =0;
            k = person;
            for (; person<strata[istrat]; person++) {
                j = sort2[person];
                temp += weights[j] * covar[i][j];
                temp2 += weights[j];
            }
            temp /= temp2;   /* mean for this covariate, this strata */
	    for (; k< person; k++) covar[i][person] -=temp;
	    }
	
        if (doscale ==1) { /* also scale the regression */
	    /* this cannot be done per stratum */
	    temp =0;
	    temp2 =0;
	    for (person=0; person<nused; person++) {
		temp += weights[person] * fabs(covar[i][person]);
		temp2 += weights[person];
		}
	    if (temp >0) temp = temp2/temp;  /* 1/scale */
	    else temp = 1.0;  /* rare case of a constant covariate */
            scale[i] = temp;
	    for (person=0; person<nused; person++) {
		covar[i][person] *= temp;
	    }
	}
    }
 
    if (doscale ==1) {
	for (i=0; i<nvar; i++) beta[i] /= scale[i]; /* rescale initial betas */
	}
    else {for (i=0; i<nvar; i++) scale[i] = 1.0;}

    <<agfit4-iter>>
    <<agfit4-finish>>
}
@ 

As we walk through the risk sets observations are both added and
removed from a set of running totals.  
In order to avoid catastrophic cancellation we need to make sure that
the terms being added are of modest size and that the weights are
not extreme.
We have 6 running totals: 
\begin{itemize}
  \item sum of the weights, denom = $\sum w_i r_i$
  \item totals for each covariate a[j] = $\sum w_ir_i x_{ij}$
  \item totals for each covariate pair cmat[j,k]=  $\sum w_ir_i x_{ij} x_{ik}$
  \item the same three quantities, but only for times that are exactly
    tied with the current death time,  named denom2, a2, cmat2.
    These are used for the Efron approximation.
\end{itemize}

The three primary quantities for the Cox model are the log-likelihood $L$,
the score vector $U$ and the Hessian matrix $H$.
\begin{align*}
  L &=  \sum_i w_i \delta_i \left[r_i - \log(d(t)) \right] \\
  d(t) &= \sum_j w_j r_j Y_j(t) \\
  U_k  &= \sum_i w_i \delta_i \left[ (X_{ik} - \mu_k(t_i)) \right] \\
  \mu_k(t) &= \frac{\sum_j w_j r_j Y_j(t) X_{jk}} {d(t)} \\
  H_{kl}  &= \sum_i w_i \delta_i V_{kl}(t_i) \\
  V_{kl}(t) &= \frac{\sum_j w_j r_j Y_j(t) [X_{jk} - \mu_k(t)]
     [X_{jl}- \mu_l(t)]} {d(t)} \\
            &= \frac{\sum_j w_j r_j Y_j(t) X_{jk}X_{jl}} {d(t)}
                  - d(t) \mu_k(t) \mu_l(t) 
\end{align*}
In the above $\delta_i =1$ for an event and 0 otherwise, $w_i$ is the per
subject weight, and $Y_i(t)$ is 1 if observation $i$ is at risk at time $t$.
The vector $\mu(t)$ is the weighted mean of the covariates at time $t$
using a weight of $w r Y(t)$ for each subject, and $V(t)$ is the weighted
variance matrix of $X$ at time $t$.

Tied deaths and the Efron approximation add a small complication to the
formula.  Say there are three tied deaths at some particular time $t$.
When calculating the denominator $d(t)$, mean $\mu(t)$ and variance
$V(t)$ at that time the inclusion value $Y_i(t)$ is 0 or 1 for all other
subjects, as usual, but for the three tied deaths Y(t) is taken to
be 1 for the first death, 2/3 for the second, and 1/3 for the third.
The idea is that if the tied death times were randomly broken by adding
a small random amount then each of these three would be in the first risk set,
have 2/3 chance of being in the second, and 1/3 chance of being in the risk
set for the third death.  
In the code this means that at a death time we add the \code{denom2},
\code{a2} and \code{c2} portions in a little at at time: add in 1/3 of
them, update totals, another 1/3, update totals, last 1/3, and update totals.

The variance formula is stable if $\mu$ is small relative to
the total variance.  This is guarranteed by having a working estimate $m$
of the mean along with the formula:
\begin{align*}
  (1/n) \sum w_ir_i(x_i- \bar x)^2 &= (1/n)\sum w_ir_i(x_i-m)^2 - 
           [{\rm mean}(x -m)]^2 \\
  {\rm mean}(x-m) &= (1/n) \sum w_ir_i (x_i -m)\\
    n &= \sum w_ir_i
\end{equation*}
A refinement of this is to scale the covariates, since the Cholesky
decomposition can lose precision when variables are on vastly differnt
scales.  We do this centering and scaling once at the beginning of the
calculation.
Centering is done per strata --- what if someone had two strata and
a covariate with mean 0 in the first but mean one million in the second?
(Users do amazing things).  Scaling is required to be a single
value,however.  

Weighted sums can still be unstable if the weights get out of hand.
Because of the exponential $r_i = exp(\eta_i)$ 
the original centering of the $X$ matrix may not be enough. 
A particular example was a data set on hospital adverse events with
``number of nurse shift changes to date'' as a time dependent covariate.
At any particular time point the covariate varied only by $\pm 3$ between
subjects (weekends often use 12 hour nurse shifts instead of 8 hour).  The
regression coefficient was around 1 and the data duration was 11 weeks
(about 200 shifts) so that $eta$ values could be over 100 even after
centering.  We keep a time dependent average of $\eta$ and renorm the
weights as necessary.

The last numerical problem is when one or more coefficients gets too
large, leading to a huge weight exp(eta).
This usually happens when a coefficient is tending to infinity, but can
also be due to a bad step in the intermediate Newton-Raphson path.
In the infinite coefficient case the
log-likelihood trends to an asymptote and there is a race between 3
conditions: convergence of the loglik, an overflow of exp for one or
more observations, or singularity of the variance matrix. 
In earlier versions of this code I had a check for each coefficient, but
this failed in an economics data set sent by a user which had extreme
correlation of the coefficients.  
We can normally stop the overflow by scaling: keep track of the average 
linear predictor and subtract it when creating weights.

The next section of code adds up the totals for a given iteration.
This is the workhorse.
<<agfit4-addup>>=
for (i=0; i<nvar; i++) {
    u[i] =0;
    a[i] =0;
    for (j=0; j<nvar; j++) {
        imat[i][j] =0 ;
        cmat[i][j] =0;
    }
}

for (person=0; person<nused; person++) {
    zbeta = 0;      /* form the term beta*z   (vector mult) */
    for (i=0; i<nvar; i++)
        zbeta += beta[i]*covar[i][person];
    eta[person] = zbeta + offset[person];
}

/*
**  'person' walks through the the data from 1 to n,
**     sort1[0] points to the largest stop time, sort1[1] the next, ...
**  'time' is a scratch variable holding the time of current interest
**  'indx2' walks through the start times.  It will be smaller than 
**    'person': if person=27 that means that 27 subjects have stop >=time,
**    and are thus potential members of the risk set.  If 'indx2' =9,
**    that means that 9 subjects have start >=time and thus are NOT part
**    of the risk set.  (stop > start for each subject guarrantees that
**    the 9 are a subset of the 27). 
**  Basic algorithm: move 'person' forward, adding the new subject into
**    the risk set.  If this is a new, unique death time, take selected
**    old obs out of the sums, add in obs tied at this time, then
**    add terms to the loglik, etc.
*/
istrat=0;
indx2 =0;
denom =0;
meaneta =0;
nrisk =0;
newlk =0;
for (person=0; person<nused;) {
    p = sort1[person];
    time = stop[p];
    /*
    ** subtract out the subjects whose start time is to the right
    ** first find out if everyone is removed (this happens often when
    ** the tt() function is used, so it is worth checking).
    */
    temp =0;
    for (; indx2<strata[istrat]; indx2++) {
	p = sort1[indx2];
	if (start[p] < time) break;
	nrisk--;
    }
    if (nrisk==0) { /*completely new risk set */
	denom=0; meaneta=0;
	for (i=0; i<nvar; i++) {
	    a[i] =0;
	    for (j=0; j<=i; j++) cmat[i][j] =0;
	}
    }
    else {
	for (; indx2<strata[istrat]; indx2++) {
	    p = sort1[indx2];
	    if (start[p] < time) break;
	    meaneta -= eta[p];
	    risk = exp(eta[p]) * weights[p];
	    denom -= risk;
	    temp += risk;
	    for (i=0; i<nvar; i++) {
		a[i] -= risk*covar[i][p];
		for (j=0; j<=i; j++)
		    cmat[i][j] -= risk*covar[i][p]*covar[j][p];
    	    }
	}
	if (denom/temp < 1e-9) {
            <<overflow-fix>>
	}
	<<fixeta>>
    }

    if (event[p]==0){
        nrisk++;
        meaneta += eta[p];
        risk = exp(eta[p]) * weights[p];
        denom += risk;
        for (i=0; i<nvar; i++) {
	    a[i] += risk*covar[i][p];
	    for (j=0; j<=i; j++)
		cmat[i][j] += risk*covar[i][p]*covar[j][p];
    	}
	person++;
	/* nothing more needs to be done for this obs */
    }
    else {   /* a death */
        /*
        ** compute the averages over subjects with
        **   exactly this death time (a2 & c2)
        */
        denom2 =0;
        meanwt =0;
        for (i=0; i<nvar; i++) {
	    a2[i]=0;
	    for (j=0; j<nvar; j++) {
		cmat2[i][j]=0;
    	    }
    	}
        deaths=0;
        for (k=person; k<strata[istrat]; k++) {
	    p = sort1[k];
	    if (stop[p] < time) break;
	    risk = exp(eta[p]) * weights[p];
            nrisk++;
            meaneta += eta[p];

	    if (event[p] ==0) {
		denom += risk;
 		for (i=0; i<nvar; i++) {
                    u[i] += weights[p] * covar[i][p];
                    a[i] += risk*covar[i][p];
		    for (j=0; j<=i; j++)
			cmat[i][j] += risk*covar[i][p]*covar[j][p];
		}
	    }
	    else {
		deaths ++;
		denom2 += risk*event[p];
		meanwt += weights[p];
		newlk += weights[p]* eta[p];
		for (i=0; i<nvar; i++) {
                    u[i] += weights[p] * covar[i][p];
		    a2[i]+= risk*covar[i][p];
		    for (j=0; j<=i; j++)
			cmat2[i][j] += risk*covar[i][p]*covar[j][p];
    		}
	    }
    	}
	<<breslow-efron>>
        <<fixeta>>
    }
}   /* end  of accumulation loop */

@ 

The last step in the above loop is to add the terms to the loglik, score and
information matrices.  Assume that there were 3 tied deaths.
The difference between the Efron and Breslow approximations is the for the
Efron the three tied subjects are given a weight of 1/3 for the first, 2/3 for
the second, and 3/3 for the third death; for the Breslow they get 3/3 for
all of them.  
<<breslow-efron>>= 
/*
** Add results into u and imat for all events at this time point
*/
if (method==0 || deaths ==1) { /*Breslow */
    denom += denom2;
    newlk -= meanwt*denom;  /* meanwt is the sum of weights for these deaths */ 
    for (i=0; i<nvar; i++) {
	a[i] += a2[i];
	temp = a[i]/denom;   /*mean covariate at this time */
	u[i] -= meanwt*temp;
	for (j=0; j<=i; j++) {
	    cmat[i][j] += cmat2[i][j];
	    imat[i][j] += meanwt*((cmat[i][j]/denom) - temp*temp);
	}
    }
    newlk -= nrisk*denom;
    person =k;
    }
else {
    meanwt /= deaths;
    for (k=0; k<deaths; k++) {
	p = sort2[person];
	if (event[p] ==1) {
	    denom += denom2/deaths;
	    newlk -= meanwt*log(denom);
	    for (i=0; i<nvar; i++) {
		a[i] += a2[i]/deaths;
		temp = a[i]/denom;
		u[i] -= meanwt*temp;
		for (j=0; j<=i; j++) {
		    cmat[i][j] += cmat2[i][j]/deaths;
		    imat[i][j] += meanwt*((cmat[i][j]/denom) - temp*temp);
    		}
    	    }
    	}
    }
}
  
/* zero subtotals at the end of a stratum */
if (person == strata[istrat]) {
    istrat++;
    denom =0;
    meaneta=0;
    nrisk =0;
    indx2 = person;
    for (i=0; i<nvar; i++) {
	a[i] =0;
	for (j=0; j<nvar; j++) cmat[i][j]=0;
    }
}
@ 

The next bit of code exists for the sake of rather rare data sets.
Assume that there is a time dependent covariate that rapidly climbs 
in such a way that the eta gets large but the range of eta stays
modest.  An example would be something like ``payments made to date'' for
a portfolio of loans.  Then even though the data has been centered and
the global mean is fine, the current values of eta are outrageous with
respect to the exp function.
Since replaceing eta with (eta -c) for any c does not change the likelihood,
do it.  Unfortunately, we can't do this once and for all: this is a step that 
will occur at least twice per iteration for those rare cases, e.g., eta is
too small at early times and too large at late ones.
<<fixeta>>=
/* 
** If the average eta value has gotton out of hand, fix it.
** We must avoid overflow in the exp function (~750 on Intel)
** and want to act well before that, but not take action very often.  
** One of the case-cohort papers suggests an offset of -100 meaning
** that etas of 50-100 can occur in "ok" data, so make it larger 
** than this.
*/
if (fabs(meaneta) > (nrisk *150)) {  
    meaneta = meaneta/nrisk;
    for (i=0; i<nused; i++) eta[i] -= meaneta;
    temp = exp(-meaneta);
    denom *= temp;
    for (i=0; i<nvar; i++) {
	a[i] *= temp;
	for (j=0; j<nvar; j++) {
	    cmat[i][j]*= temp;
	}
    }
    meaneta =0;
}
@         

This next bit of code is very rare. It occurs when the subjects who have
just been dropped from the risk set had essentially all the weight,
so much so that the running totals of \code{denom}, \code{a}, and \code{cmat}
have lost nearly all of their precision.
The only recourse is to rebuild them from scratch.
<<overflow-fix>>=
denom=0;
for (i=0; i<nvar; i++) {
    a[i] =0;
    for (j=0; j<=i; j++) cmat[i][j] =0;
    }
if (istrat==0) ii =0; else ii= strata[istrat-1];
for (k=ii; k<strata[istrat]; k++) {
    p = sort2[k];
    if (stop[p] <= time) break;  /* further subjects have not yet been added*/
    if (start[p] < time) {  
	risk = weights[p] *eta[p];
	denom += risk;
	for (i=0; i<nvar; i++) {
	    a[i] += risk * covar[i][p];
	    for (j=0; j<=i; j++) cmat[i][j] += risk * covar[i][p]*covar[i][p];
	}
    }
}
@ 

Now, I'm finally to the actual iteration steps.

<<agfit4-iter>>=
/* First iteration, which has different ending criteria */
<<agfit4-addup>>
loglik[0] = newlk;   /* save the loglik for iteration zero  */
loglik[1] = newlk;

/* Calculate the score test */
for (i=0; i<nvar; i++) /*use 'a' as a temp to save u0, for the score test*/
    a[i] = u[i];
*flag = cholesky2(imat, nvar, tol_chol);
chsolve2(imat,nvar,a);        /* a replaced by  a *inverse(i) */
*sctest=0;
for (i=0; i<nvar; i++)
    *sctest +=  u[i]*a[i];

if (maxiter ==0) {
    *iter =0;
    <<agfit4-finish>>
}
else {  
    /* Update beta for the next iteration
    **  Never complain about convergence on this first step or impose step
    **  halving.  That way someone can force one iter at a time.
    */
    for (i=0; i<nvar; i++) {
	oldbeta[i] = beta[i];
	beta[i] = beta[i] + a[i];
    }
}
@ 
The Cox model calculation rarely gets into numerical difficulty, and when it
does step halving has always been sufficient.
Let $\beta^{(0)}$, $\beta^{(1)}$, etc be the iteration steps in the search 
for the maximum likelihood solution $\hat \beta$.
The flow of the algorithm is 
\begin{enumerate} 
  \item For the $k$th iteration, start with the new trial estimate
    $\beta^{(k)}$.  This new estimate is [[beta]] in the code and the
    most recent successful estimate is [[oldbeta]].
  \item For this new trial estimate, compute the log-likelihood, and the
    first and second derivatives.
  \item Test if the log-likelihood if finite, has converged \emph{and} 
    the last estimate
    was not generated by step-halving.  In the latter case the algorithm may
    \emph{appear} to have converged but the solution is not sure.
    An infinite loglik is very rare: I've seen exactly one of these and it was
    due to overflow of the exp function.
    \begin{itemize}
      \item if so return beta and the the other information
      \item if this was the last iteration, return beta, the other information,
        and a warning flag.
        If even step halving has failed for the last iteration, return the
            last good one we had.
     \item otherwise, compute the next guess and return to the top
        \begin{itemize}
          \item if our latest trial guess [[beta]] made things worse use step
            halving: $\beta^{(k+1)}$ = oldbeta + (beta-oldbeta)/2.  
            The assumption is that the current trial step was in the right
            direction, it just went too far. 
          \item otherwise take a Newton-Raphson step
        \end{itemize}
    \end{itemize}
\end{enumerate}

I am particularly careful not to make a mistake that I have seen in several
other Cox model programs.  All the hard work is to calculate the first
and second derivatives $U$ (u) and $H$ (imat), once we have them the next
Newton-Rhapson update $UH^{-1}$ is just a little bit more.  Many programs
succumb to the temptation of this ``one more for free'' idea, and as a
consequence return $\beta^{(k+1)}$ along with the log-likelihood and
variance matrix for $\beta^{(k)}$.
If a user has specified
for instance only 1 or 2 iterations the answers can be seriously
out of joint, whereas  
if iteration has gone to completion they will differ by only a gnat's
eyelash (so what's the point of doing it).

<<agfit4-iter>>=
/* main loop */
halving =0 ;             /* =1 when in the midst of "step halving" */
for (*iter=1; *iter<= maxiter+1; (*iter)++) {
    <<agfit4-addup>>

    *flag = cholesky2(imat, nvar, tol_chol);
    if (isnan(loglik[1])==0 && isinf(loglik[1]) ==0 &&
        fabs(1-(loglik[1]/newlk))<= eps  && halving==0){ /* all done */
	<<agfit4-finish>> 
    }

    if (*iter == maxiter) {
	 if (newlk < loglik[1]) {
	     /* 
	     ** The routine has not made progress past the last good value.
	     **  recompute the information matrix for that value
	     */
	     for (i=0; i<nvar; i++) beta[i] = oldbeta[i];
	 } else {
	     <<agfit4-finish>>
	 }
    }
    else {
	if (newlk < loglik[1])   {    /*it is not converging ! */
	    halving =1;
	    for (i=0; i<nvar; i++)
		beta[i] = (oldbeta[i] + beta[i]) /2; /*half of old increment */
	}
	else {
	    halving=0;
	    loglik[1] = newlk;
	    chsolve2(imat,nvar,u);

	    for (i=0; i<nvar; i++) {
		oldbeta[i] = beta[i];
		beta[i] = beta[i] +  u[i];
	    }
	}
    }  
    R_CheckUserInterrupt();  /* be polite -- did the user hit cntrl-C? */
} /*return for another iteration */
@ 

Save away the final bits, compute the inverse of imat and symmetrize it,
release memory and return.
<<agfit4-finish>>= 
loglik[1] = newlk;
chinv2(imat, nvar);
for (i=0; i<nvar; i++) {
    beta[i] *= scale[i];  /* return to original scale */
    u[i] /= scale[i];
    imat[i][i] *= scale[i] * scale[i];
    for (j=0; j<i; j++) {
	imat[j][i] *= scale[i] * scale[j];
	imat[i][j] = imat[j][i];
    }
}
UNPROTECT(nprotect);
return(rlist);
@ 
