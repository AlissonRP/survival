\section{Concordance}
 The concordance statistic is the most used measure of goodness-of-fit
in survival models.  
In general let $y_i$ and $x_i$ be observed and predicted data values.
A pair of obervations $i$, $j$ is considered condordant if either
$y_i > y_j, x_i > x_j$ or $y_i < y_j, x_i < x_j$.
The concordance is the fraction of concordant pairs.
For a Cox model remember that the predicted survival $\hat y$ is longer if
the risk score $X\beta$ is lower, so we have to flip the definition and
count ``discordant'' pairs, this is done at the end of the routine.

One wrinkle is what to do with ties in either $y$ or $x$.  Such pairs
can be ignored in the count (treated as incomparable), treated as discordant,
or given a score of 1/2.
\begin{itemize}
  \item Kendall's $\tau$-a scores ties as 0.
  \item Kendall's $\tau$-b and the Goodman-Kruskal $\gamma$ ignore ties in 
    either $y$ or $x$.
  \item Somers' $d$ treats ties in $y$ as incomparable, pairs that are tied
    in $x$ (but not $y$) score as 1/2.  The AUC from logistic regression is
    equal to Somers' $d$.
\end{itemize}
All three of the above range from -1 to 1, the concordance is
$(d +1)/2$.  
For survival data any pairs which cannot be ranked with certainty are
considered incomparable.
For instance $y_i$ is censored at time 10 and $y_j$ is an event (or censor) 
at time 20.  Subject $i$ may or may not survive longer than subject $j$.  
Note that if $y_i$ is censored at time
10 and $y_j$ is an event at time 10 then $y_i > y_j$.  
Observations that are in different strata are also incomparable, 
since the Cox model only compares within strata.

The program creates 4 variables, which are the number of concordant pairs, 
discordant, tied on time, and tied on $x$ but not on time.  
The default concordance is 
based on the AUC definition, but all 4 values are reported back so that a user
can recreate the others if desired.

Here is the main routine.
<<concordance>>=
concordance <- function(x, ...) 
    UseMethod("concordance")

concordance.formula <- function(formula, data,
                                weights, subset, na.action, group,
                                ymin=NULL, ymax=NULL, 
                                timewt=c("S", "n", "S/G", "n/G", "n/G2"),
                                influence=0, reverse) {
    Call <- match.call()  # save a copy of of the call, as documentation
    timewt <- match.arg(timewt)
    
    index <- match(c("formula", "data", "weights", "subset", "na.action", 
                     "group"),
                   names(Call), nomatch=0)
    temp <- Call[c(1, index)]
    temp[[1L]] <-  quote(stats::model.frame)
    special <- c("strata", "cluster")
    temp$formula <- if(missing(data)) terms(formula, special)
                    else              terms(formula, special, data=data)
    mf <- eval(temp, parent.frame())  # model frame
    if (nrow(mf) ==0) stop("No (non-missing) observations")
    Terms <- terms(mf)

    Y <- model.response(mf)
    if (!inherits(Y, "Surv")) {
        if (is.numeric(Y) && is.vector(Y))  Y <- Surv(Y)
        else stop("left hand side of the formula  must be a numeric vector or a surival")
    }
    n <- nrow(Y)

    wt <- model.weights(mf)
    offset<- attr(Terms, "offset")
    if (length(offset)>0) stop("Offset terms not allowed")

    stemp <- untangle.specials(Terms, "strata")
    if (length(stemp$vars)) {
	if (length(stemp$vars)==1) strat <- m[[stemp$vars]]
	else strat <- strata(m[,stemp$vars], shortlabel=TRUE)
        Terms <- Terms[-stemp$terms]
    }
    else strat <- NULL
    
    group <- model.extract(mf, "(group)")
    cluster<- attr(Terms, "specials")$cluster
    if (length(cluster)) {
        tempc <- untangle.specials(Terms, 'cluster', 1:10)
        ord <- attr(Terms, 'order')[tempc$terms]
        if (any(ord>1)) stop ("Cluster can not be used in an interaction")
        cluster <- strata(mf[,tempc$vars], shortlabel=TRUE)  #allow multiples
        Terms <- Terms[-tempc$terms]  # toss it away
    }
    else if (length(group)==0) group <- rep.int(1,n)
    if (length(group)) cluster <- group
                                            
    x <- model.matrix(Terms, m)[,-1, drop=FALSE]  #remove the intercept
    if (ncol(x) > 1) stop("Only one predictor variable allowed")

    if (!is.null(ymin) & (length(ymin)> 1 || !is.numeric(ymin)))
        stop("ymin must be a single number")
    if (!is.null(ymax) & (length(ymax)> 1 || !is.numeric(ymax)))
        stop("ymax must be a single number")
    
    cfit <- concordance.fit(Y, x, strat, wt, ymin, ymax, timewt, influence,
                             cluster)
    
    concordance <- (cfit$tau +1)/2
    if (missing(reverse)) reverse <- (concordance < .5)
    if (!is.logical(reverse)) 
        stop ("the reverse argument must be TRUE/FALSE")
    if (reverse) concordance <- 1- concordance

    fit <- c(concordance= concordance, cfit, call=Call)
    na.action <- attr(m, "na.action")
    if (length(na.action)) fit$na.action <- na.action

    class(fit) <- 'concordance'
    fit
}

print.concordance <- function(x, ...) {
    if(!is.null(cl <- x$call)) {
        cat("Call:\n")
        dput(cl)
        cat("\n")
        }
    omit <- x$na.action
    if(length(omit))
        cat("  n=", x$n, " (", naprint(omit), ")\n", sep = "")
    else cat("  n=", x$n, "\n")
    cat("Concordance= ", format(x$concordance), " se= ", format(x$std.err),
        '\n', sep='')
    print(x$stats)

    invisible(x)
    }
@ 

The concordance.fit function is broken out separately, since it is called
by the \code{coxph} routine.
If $y$ is not a survival quantity, then all of the options for the
\code{timewt} parameter lead to the same result, so use the simplest one to
compute in that case.

<<concordancefit>>=
concordance.fit <- function(y, x, strata, weight, ymin, ymax, timewt,
                            influence, group) {
    # The coxph program may occassionally fail, and this will kill the C
    #  routine below.  So check for it.
    if (any(is.na(x)) || any(is.na(y))) return(NULL)

    # these should only occur if something outside survival calls this routine
    n <- length(y)
    if (length(x) != n) stop("x and y are not the same length")
    if (!missing(strata) && length(strata) != n)
        stop("y and strata are not the same length")
    if (missing(weight)) weight <- rep(1, n)
    else if (length(weight) != n) stop("y and weight are not the same length")
    if (!is.Surv(y)) {
        y <- Surv(y)
        timewt <- 'n'
    }

    type <- attr(y, "type")
    if (type %in% c("left", "interval"))
        stop("left or interval censored data is not supported")
    if (type %in% c("mright", "mcounting"))
        stop("multiple state survival is not supported")
   
    # This routine is called once per stratum
    docount <- function(stime, risk, wts, grp, timewt= 'n') {
        n <- length(risk)
        # this next line is mostly invoked in stratified logistic, where
        #  only 1 event per stratum occurs.  All time weightings are the same
        if (sum(stime[,ncol(stime)]) <2) timewt <- 'n'
        
        # order the data
        if (ncol(stime)==2) {
            ord <- order(-y[,2], y[,1])  # reverse time, censors before deaths
            stime <- stime[ord,]
            risk  <- risk[ord]
            wts   <- wts[ord]
        } else {
            sort.stop <- order(-stime[,2], stime[,3])
            sort.start <- order(-stime[,1])
        }

        sfit <- survfitKM(rep(1L, n), stime, se.fit=FALSE)
        etime <- sfit$time[sfit$nevent > 0]
        esurv <- c(1, sfit$surv)[which(sfit$nevent > 0)]
        
        if (timewt %in% c("S/G", "n/G", "n/G2")) {
            temp <- stime
            temp[,ncol(temp)] <- 1- temp[,ncol(temp)] # switch event/censor
            gfit <- survfitKM(rep(1L, n), temp, se.fit=FALSE)
            gsurv <- summary(gfit, times= etime + min(diff(etime))/2,
                             extend=TRUE)
        }

        twt <- switch(timewt) {
            "S" = esurv,
            "S/G" = esurv/gsurv,
            "n" =  sfit$nrisk[sfit$nevent>0],
            "n/G"= sfit$nrisk[sfit$nevent>0]/gsurv,
            "n/G2" =  sfit$nrisk[sfit$nevent>0]/gsurv^2
        }
    
        # match each prediction score to the unique set of scores
        # (to deal with ties)
        utemp <- match(risk, sort(unique(risk)))
        bindex <- btree(max(utemp))[utemp]
        grp <- match(group, sort(unique(group))
        if (ncol(y) ==2) 
            .Call(Cconcordance3, stime, bindex, wts, grp)
        else .Call(Ccondordance4, stime, bindex, wts, sort.start, 
                             sort.stop, grp)
    }
    
    if (missing(strata) || length(strata)==0 || all(strata==strata[1])) {
        fit <- docount(y, x, weight, grp)
        names(fit$count) <- c("concordant", "discordant", "tied.x", "tied.y",
                          "var", "avar")
    }
    else { 
        strata <- as.factor(strata)
        ustrat <- levels(strata)[table(strata) >0]  #some strata may have 0 obs
        count <- matrix(0., nrow=length(ustrat), ncol=6)
        dimnames(count) <- list(ustrat, c("concordant", "discordant", "tied.x", 
                                          "tied.y", "var", "avar")
        if (influence==1) itemp  <- double(length(x))
        else if (influence==2) itemp <- matrix(0., length(x), 4)
            
        for (i in 1:length(ustrat)) {
            keep <- which(strata == ustrat[i])
            tfit <- docount(y[keep,,drop=F], x[keep], weight[keep], grp[keep])
            count[i,] <- tfit$count
            if (influence==1) itemp[keep] <- tfit$influence
            else if (influence==2) itemp[keep,] <- tfit$influence
        }
        if (influence==0) fit <- list(count=count)
        else fit <- list(count=count, influence=influence)
    }
        fit
}
@ 

The C code looks a lot like a Cox model: walk forward through time, keep
track of the risk sets, and add something to the totals at each death.
What needs to be summed is the rank of the event subject's $x$ value, as
compared to the value for all others at risk at this time point.
For notational simplicity let $Y_j(t_i)$ be an indicator that subject $j$
is at risk at event time $t_i$, and $Y^*_j(t_i)$ the more restrictive one that
subject $j$ is both at risk and not a tied event time.
The values we want at time $t_i$ are
\begin{align}
  n_i &=  \sum_j w_j Y^*_j(t_i) \nonumber \\
  C_i &= (v_i/n_i) \delta_i w_i \sum_j w_j Y^*_j(t_i) \left[I(x_i < x_j) \right]
    \label{C} \\
  D_i &= (v_i/n_i) \delta_i w_i \sum_j w_j Y^*_j(t_i) \left[I(x_i > x_j)\right] 
     \label{D} \\
  T_i &= (v_i/n_i) \delta_i w_i \sum_j w_j Y^*_j(t_i) \left[I(x_i = x_j) \right]
     \label{T}  \\
  m_i &= \delta_i \sum_j w_j (Y_j - Y^*_j) \nonumber
\end{align} 

In the above $n$ is the number of comparable values at event time $t_i$ and
$m$ is the number of exact ties, $(v_i/n_i)$ is treated as a fixed 
time-dependent weight (with no censoring it is a constant).
$C$, $D$, and $T$ are the number of concordant, discordant, and tied
pairs, respectively.

The primary compuational question is how to do this efficiently, i.e., better
than a naive algorithm that loops across all $n(n-1)/2$ 
possible pairs.
There are two key ideas.
\begin{enumerate}
\item Rearrange the counting so that we do it by death times.
  For each death we count the number of other subjects in the risk set whose
  score is higher, lower, or tied and add it into the totals.
This neatly solves the question of time-dependent covariates.
\item Counting the number with higher, lower, and tied $x$ can be done in 
   $O(\log_2 n)$ time if the $x$ data is kept in a binary tree.
\end{enumerate}

\begin{figure}
  \myfig{balance}
  \caption{A balanced tree of 13 nodes.}
  \label{treefig}
\end{figure}

Figure  \ref{treefig} shows a balanced binary tree containing  
13 risk scores.  For each node the left child and all its descendants
have a smaller value than the parent, the right child and all its
descendents have a larger value.
Each node in figure \ref{treefig} is also annotated with the total weight
of observations in that node and the weight for itself plus all its children 
(not shown on graph).  
Assume that the tree shown represents all of the subjects still alive at the
time a particular subject ``Smith'' expires, and that Smith has the risk score
of 19 in the tree.
The concordant pairs are those with a risk score $>19$, i.e., both $\hat y=x$
and $y$ are larger, discordant are $<19$, and we have no ties.
The totals can be found by
\begin{enumerate}
  \item Initialize the counts for discordant, concordant and tied to the
    values from the left children, right children, and ties at this node,
    respectively, which will be $(C,D,T) = (1,1,0)$.
  \item Walk up the tree, and at each step add the (parent + left child) or
    (parent + right child) to either D or C, depending on what part of the
    tree has not yet been totaled.  
    At the next node (8) $D= D+4$, and at the top node $C=C + 6$.
\end{enumerate}

There are 5 concordant and 7 discordant pairs.
This takes a little less than $\log_2(n)$ steps on average, as compared to an
average of $n/2$ for the naive method.  The difference can matter when $n$ is
large since this traversal must be done for each event.

The classic way to store trees is as a linked list.  There are several 
algorithms for adding and subtracting nodes from a tree while maintaining
the balance (red-black trees, AA trees, etc) but we take a different 
approach.  Since we need to deal with case weights in the model and we
know all the risk score at the outset, the full set of risk scores is
organised into a tree at the beginning, updating the sums of weights at
each node as observations are added or removed from the risk set.

If we internally index the nodes of the tree as 1 for the top, 
2--3 for the next 
horizontal row, 4--7 for the next, \ldots then the parent-child 
traversal becomes particularly easy.
The parent of node $i$ is $i/2$ (integer arithmetic) and the children of
node $i$ are $2i$ and $2i +1$.  In C code the indices start at 0 of course.
The following bit of code arranges data into such a tree.
<<btree>>=
btree <- function(n) {
   tfun <- function(n, id, power) {
       if (n==1L) id
       else if (n==2L) c(2L *id + 1L, id)
       else if (n==3L) c(2L*id + 1L, id, 2L*id +2L)
       else {
           nleft <- if (n== power*2L) power  else min(power-1L, n-power%/%2L)
           c(tfun(nleft, 2L *id + 1L, power%/%2), id,
             tfun(n-(nleft+1L), 2L*id +2L, power%/%2))
       }
   }
   tfun(as.integer(n), 0L, as.integer(2^(floor(logb(n-1,2)))))
}
@ 

Referring again to figure \ref{treefig}, \code{btree(13)} yields the vector
\code{7  3  8  1  9  4 10  0 11  5 12  2  6}
meaning that the smallest element
will be in position 8 of the tree, the next smallest in position 2, etc,
using indexing that starts at 0 since the results will be passed to a C
routine.
The routine above takes care to do all arithmetic as integer.  
This actually made almost no difference in the compute time, but it was an
interesting exercise to find that out.

The next question is how to compute a variance for the result.
One approach is to compute an infinitesimal jackknife (IJ) estimate,
for which we need derivatives with respect to the weights.
Looking back at equation \eqref{C} we have
\begin{align}
  C  &= \sum_i (v_i/m_i) w_i \delta_i \sum_j Y^*_j(t_i) w_j I(x_i < x_j) 
  \nonumber\\
% \frac{\partial C}{\partial w_k} &= 
%    (v_k/m_k)\delta_k \sum_j Y^*_{j}(t_k) I(x_k < x_j) +
%    \sum_i (v_i/m_i) w_i Y^*_k(t_i) I(x_i < x_k) \label{partialC}
\end{align}
A given subject's weight appears multiple times, once when they are an
event ($w_i \delta_i)$, and then as part of the risk set for other's
events.  I avoided this for some time because it looked like an $O(nd)$
process to separately update each subject's influence for each risk set
they inhabit, but David Watson pointed out a path forward.
The solution is to keep two trees.  
Tree 1 contains all of the subjects at risk.  We traverse it when each subject
is added in, updating the tree, 
and traverse it again at each death, pulling off values to update our sums. 
The second tree holds only the deaths and is updated at each death;
it is read out twice per subject,
once just before they enter the risk set and once when they leave.

The basic algorithm is to move through an outer and inner loop.  The
outer loop moves across unique times, the inner for all obs that
share a death time.  We progress from largest to smallest time.

A second viewpoint treats the data as a Cox model.
Create zero-centered scores for all subjects in the risk set:
\begin{align}
  z_i(t) &= \sum_{j \in R(t)} w_j \sign(x_i - x_j) \nonumber
  D-C = \sum_i \delta_i z_i(t_i)              \label{zcord}
\end{align}
At any event time $\sum w_i z_i =0$.  
Equation \eqref{zcord} is the score equation
for a Cox model with time-dependent covariate $z$.
When two subjects have an event at the same time, this formulation treats
each of them as being in the other's risk set whereas the concordance
treats them as incomparable --- how can they be the same?
The trick is that $D-C$ does not change: the tied pairs add equally to the
two terms. 
Under the null hypothesis that the risk score is not related to outcome,
each term in \eqref{zcord} is a random selection from the $z$ scores in
the risk set, and the variance of the addition is the variance of $z$,
the sum of these over deaths is the Cox model information matrix. 
use the inverse of the information matrix for said Cox model as a variance.
The mean of $z$ is always zero, so we need to keep track of 
$\sum w_i z^2$. 

How can we do this efficiently?  First note that $z_i$ can be written
as sum(weights for smaller x) - sum(weights for larger x), and in fact the
weighted mean for any slice of $x$ values $a < x < b$ is exactly the
same: sum(weights for x values below the range) - sum(weights above the range).
The second trick is to use an ANOVA decomposition of the variance of $z$ into
within-slice and between-slice sums of squares, where the 3 slices are the
weights at a given $x$ value (node of the tree), below that, and above.
Assume that a new observation $k$ has just been added to the tree.  
This will add $w_k$ to all the $z$ values above, and to the weighted mean of
all those above, $-w_k$ to the values and means below, and 0 to the values and
means of any tied observations.  Thus none of the current `within'
SS change.  
Let $s_a$, $s_b$ and $s_0$ be the current sum of weights above, below, and
at the node of the tree.  The mean for the above group was $(s_b + s_0) with
between SS contribution of $s_a (s_b + s_0)^2$.
Adding $w_k$ to the above mean the change in between SS is
$s_a (2w_k (s_b + s_0) + w_k^2)$, while for the below group the change
is $s_b(-2w_k(s_a + s_0) + w_k^2)$, and there is no change for the middle
group.
Last we add $z_k^2 = (s_b- s_a)^2$ to the sum for the new observation.
Putting all this together the change is
$$
  (s_a + s_b) w_k^2 + 2 (s_a-s_b) s_0 w_k + (s_b - s_a)^2
$$

We can now define the C-routine that does the bulk of the work.
First we give the outline shell of the code and then discuss the
parts one by one.  This routine  is for ordinary survival data, and
will be called once per stratum.
Input variables are
\begin{description}
  \item[n] the number of observations
  \item[y] matrix containing the time and status, data is sorted by descending 
    time, with censorings precedint deaths.
  \item[x] the tree node at which this observation's risk score resides  %'
  \item[wt] case weight for the observation
  \item[group] which IJ group each observation will be counted into
\end{description}
The routine will return list with three components:
\begin{itemize}
  \item count, a vector containing the weighted number of concordant, 
    discordant, tied on $x$ but not $y$, and tied on y pairs.  
    The weight for a pair is $w_iw_j$.
  \item resid, a three column matrix with one row per event, containing the 
    score residual at that event, its variance, and the sum of weights.
    The score residual is
    a rescaled $z_i$ so as to lie between 0 and 1: $(1+ z/\sum(w))/2$.
    The concordance is then a weighted sum of the residuals.
  \item influence, a matrix with one row per observation and 4 columns, giving
    that observation's first derivative with respect to the count vector.
\end{itemize}    

<<concordance1>>=
#include "survS.h"
SEXP concordance3(SEXP y, SEXP x2, SEXP wt2, SEXP group2) {
    int i, j, k, index;
    int lchild, rchild, parent;
    int n, ngrp, ntree, nevent;
    double *time, *status;
    /* first are the count, sum of weights, count for node and all children,
    **  and sum for all
    ** second are 4 matrices which apply only to the deaths, and have the
    **  same info, but with one row for each of concordant, discordant, and tied
    **  on x.
    */
    double *ncount, *nwt, *tcount, *twt;
    double *dncount, *dnwt, *dtcount, *dtwt;  /* deaths: node count etc */
    double *twt, *nwt, *count;
    double vss, myrank, wsum1, wsum2, wsum3; /*sum of wts below, tied, above*/
    double lmean, umean, oldmean, newmean;
        
    double ndeath;   /* weighted number of deaths at this point */
    double wtemp[3]; /* used when walking up a tree */

    SEXP rlist, count2, imat2, resid2;
    double *count, *imat[6], *resid;
    double *wt;
    int    *x;
    int    *group;
    static const char *outnames[]={"count", "resid", "influence", ""};
    
    n = nrows(y);
    x = INTEGER(x2);
    wt = REAL(wt2);
    ngrp = length(group2)
    group = INTEGER(group2);
    influence = asInteger(influence2);
    time = REAL(y);
    status = time + n;
   
    /* if there are tied predictors, the total size of the tree will be < n */
    ntree =0; nevent =0;
    for (i=0; i<n; i++) {
	if (x[i] >= ntree) ntree = x[i] +1;
        nevent += status[i];
    }
        
    PROTECT(count2 = allocVector(REALSXP, 6));
    count = REAL(count2);  /* counts 5-6 contains the variances */
    ncount = (double *) R_alloc(8*ntree, sizeof(double));
    nwt = ncount + ntree;
    tcount = nwt + ntree;
    twt = tcount + ntree;
    dncount = twt + ntree;
    dnwt = dncount + ntree;
    dtcount = dnwt + ntree;
    dtwt = dtcount + ntree;
    
    for (i=0; i< 8*ntree; i++) ncount[i] =0.0;
    for (i=0; i<6; i++) count[i]=0.0;
    vss=0;
    
    PROTECT(rlist = mkNamed(VECSXP, outnames));
    count2 = SET_VECTOR_ELT(rlist, 0, allocVector(REALSXP, 6));
    count = REAL(count2);  /* counts 5-6 contains the variances */
    resid2 = SET_VECTOR_ELT(rlist, 1, allocVector(REALSXP, nevent));
    resid = REAL(resid2);
    imat2 = SET_VECTOR_ELT(rlist, 2, allocMatrix(REALSXP, ngroup, 4));
    for (i=0; i<4; i++) imat[i] = imat2 + i*ngroup;

    <<concordance3-work>>
        
    UNPROTECT(1);
    return(rlist);
}
@ 

The key part of our computation is to update the vectors of weights.
We don't actually pass the risk score values $r$ into the routine,   %'
it is enough for each observation to point to the appropriate tree
node.
The tree contains the weights for everyone whose survival is larger
than the time currently under review, so starts with all weights
equal to zero.  
For any pair of observations $i,j$ we need to add [[wt[i]*wt[j]]]
to the appropriate count.
Starting at the largest time (which is sorted last), walk through the tree.
\begin{itemize}
  \item For either a death or censoring time, first add -1 times the current
    death information into the influence matrix.  
  \item If it is a censored subject, add them into the total tree.
    If it is a death time, we need to make 3 passes over the set of
    deaths tied at this time.
    \begin{itemize}
      \item Pass 1. Compare each new obs to the current tree, and update
        the concordance counts.
        \begin{enumerate}
          \item The addition to tied-on-x will be the weight for this 
            observation, times
            the sum of weights for all others with the same risk score (x) and a
            a greater time, i.e., the weight found at \code{x[i]} in the tree.
          \item Walk the tree to count concordant and discordant. 
            First add in the children of this node.  The left child will be 
            smaller risk scores and longer times, adding to the discordant
            pairs, the right child adds to concordant.
            Then walk up the tree to the root. 
            At each step up we add in data for new node plus the 'not me' branch.
            If we were the right branch (odd number node) of a parent
            then the parent + parent's left child will add to the
            discordant, and to concordant if we are a left branch.
        \end{enumerate}
      \item Pass 2: Add each of the tied subjects' weights into both the total
        tree and the death tree.  Also update $\sum z^2$.
      \item Pass 3: Now that the tree includes all the risk set, fill in rows
        of the residuals matrix.
    \end{itemize}
\end{itemize}
When all the subjects have been added to the tree, then add the death tree
data for each subject to the influence matrix.  

<<concordance3-work>>=
z2 =0;
for (i=0; i<n;) {
    /* Initialize the influence */
    walkup(dnwt, dtwt, x[j], wtemp, ntree);
    for (j=0; j<3; j++) imat[j][group[i]] -= wtemp[j];

    if (status[i]==0) { /* censored, simply add them into the tree */
       addin(nwt, twt, x[i], wt[i]);
       i++
    }
    else { /* process all tied deaths at this point */
	ndeath=0; temp =0; dwt=0;
        /* update the counts */
        for (j=i; j<n && time[j]==time[i]; j++) {
           temp += wt[j] * dwt;	    
           ndeath++; dwt += wt[j];   /* count of deaths and sum of wts */
	   walkup(nwt, twt, x[j], wtemp, ntree);
	   for (k=0; k<3; k++) count[k] += wt[k]* wtemp[k];
	}

	if (ndeath>1) {
	    /* Add tied time information into the counts and influence */
	    count[4] += temp;
	    for (j=i; j< i+ndeath; j++) imat[4][group[j]] += dwt - wt[j];
	}
	 
	/* Add deaths into the tree and update z2*/
	for (j=i; j< ndeath+i; j++) {
            walkup(nwt, twt, x[j], wtemp, ntree);
            z2 += (wtemp[1] + wtemp[2])*w[j]*w[j] + 
                  (wtemp[1] - wtemp[2])*wtemp[3] * w[j] +
                  (wtemp[2]- wtemp[1]) *(wtemp[2]- wtemp[1]);
	    addin(nwt, twt, x[j], wt[j]);
	    addin(dnwt, dtwt, x[j], wt[j]);
        }
        
        /* fill in the residuals matrix */
        for(j=0; j<ndeath; j++) {
            walkup(nwt, twt, x[i], wtemp, ntree);
            resid[1][dcount] = (wtemp[1] + wtemp[3]/2)/nwt[0];
            resid[2][dcount] = z2/(4 * nwt[0]*nwt[0]);
            resid[3][dcount] = nwt[0];
            dcount++;
	    i++;
	}
     }

/* Now finish off the influence for each observation */
for (i=0; i<n; i++) {
    walkup(dnwt, dtwt, x[i], wtemp);
    for (k=0; k<3; k++) influence[k, i] += wtemp[k] - wt[i];
}
@ 

<<walkup>>=
void walkup(nwt, twt, index, sums, ntree) {
    double *nwt, *twt, sums[3];
    int i, j, k, index, parent;

    for (i=0; i<3; i++) sums[i] = 0.0;
    sums[2] = nwt[index];   /* tied on x */
    
    j = 2*index +2;  /* right child */
    if (j < tmax) sum[0] += twt[j];
    if (j <=tmax) sums[1]+= twt[j-1]; /*left child */

    while(index > 0) {
        parent <- (index-1)/2;
        if (index%2 == 1) sums[0] += twt[parent] - twt[index]; /* left child */
	else sums[1] += twt[parent] - twt[index]; /* I am a right child */
	index = parent;
    }
}

void addin(nwt, twt, index, wt) {
    integer index;
    double *nwt, *twt, wt;

    while (index > 0) {
	nwt[index] += wt;
	twt[index] += wt;
	index = (index-1)/2;
    }
}

@ 
The code for [start, stop) data is quite similar.  
As in the agreg routines there are two sort indices, the first indexes
the data by stop time, longest to earliest, and the second by start time. 
The [[y]] variable now has three columns.
<<concordance1>>= 
SEXP concordance2(SEXP y,     SEXP wt2,  SEXP indx2, SEXP ntree2,
                  SEXP sortstop, SEXP sortstart) {
    int i, j, k, index;
    int child, parent;
    int n, ntree;
    int istart, iptr, jptr;
    double *time1, *time2, *status, dtime;
    double *twt, *nwt, *count;
    int *sort1, *sort2;
    double vss, myrank;
    double wsum1, wsum2, wsum3; /*sum of wts below, tied, above*/
    double lmean, umean, oldmean, newmean;
    double ndeath;
    SEXP count2;
    double *wt;
    int    *indx;
    
    n = nrows(y);
    ntree = asInteger(ntree2);
    wt = REAL(wt2);
    indx = INTEGER(indx2);
    sort2 = INTEGER(sortstop);
    sort1 = INTEGER(sortstart);
    
    time1 = REAL(y);
    time2 = time1 + n;
    status= time2 + n;
    PROTECT(count2 = allocVector(REALSXP, 5));
    count = REAL(count2);
    twt = (double *) R_alloc(2*ntree, sizeof(double));
    nwt = twt + ntree;
    for (i=0; i< 2*ntree; i++) twt[i] =0.0;
    for (i=0; i<5; i++) count[i]=0.0;
    vss =0;
    <<concordance2-work>>
        
    UNPROTECT(1);
    return(count2);
}
@ 

The processing changes in 2 ways
\begin{itemize}
  \item The loops go from $0$ to $n-1$ instead of $n-1$ to 0.  We need
    to use [[sort1[i]]] instead of [[i]] as the subscript for the time2 and wt
    vectors.  (The sort vectors go backwards in time.)
    This happens enough that we use a temporary variables [[iptr]] and [[jptr]]
    to avoid the double subscript.
  \item As we move from the longest time to the shortest observations are added
    into the tree of weights whenever we encounter their stop time. 
    This is just as before.  Weights now also need to be removed from the 
    tree whenever we encounter an observation's start time.              %'
    It is convenient ``catch up'' on this second task whenever we encounter 
    a death.
\end{itemize}

<<concordance2-work>>=
istart = 0;  /* where we are with start times */
for (i=0; i<n; ) {
    iptr = sort2[i];  /* In  reverse death time order */
    ndeath =0;
    if (status[iptr]==1) {
	/* Toss people out of the tree  and update variance */
	dtime = time2[iptr];
	for (; istart < n && time1[sort1[istart]] >= dtime; istart++) {
            wsum1 =0;
	    oldmean = twt[0]/2;
	    jptr = sort1[istart];
	    index = indx[jptr];
	    nwt[index] -= wt[jptr];
            twt[index] -= wt[jptr];
            wsum2 = nwt[index];
            child = 2*index +1;  /* left child */
	    if (child < ntree) wsum1 += twt[child];
	    while (index >0) {
		parent = (index-1)/2;
		twt[parent] -= wt[jptr];
		if (!(index&1)) /* I am a right child */
		    wsum1 += (twt[parent] - twt[index]);
		index=parent;
		}
	    wsum3 = twt[0] - (wsum1 + wsum2);
	    lmean = wsum1/2;
	    umean = wsum1 + wsum2 + wsum3/2;  /* new upper mean */
	    newmean = twt[0]/2;
	    myrank = wsum1 + wsum2/2;
            vss += wsum1*(newmean+ oldmean - 2*lmean) * (newmean-oldmean);
            oldmean -= wt[jptr];  /* the z in equations above */
            vss += wsum3*(newmean+ oldmean -2*umean) * (newmean-oldmean);
	    vss -= wt[jptr]* (myrank -newmean)*(myrank -newmean);
            }
	    
	/* Process deaths */
	for (j=i; j <n && status[sort2[j]]==1 && time2[sort2[j]]==dtime; j++) {
	    jptr =  sort2[j];
	    ndeath += wt[jptr];
            index = indx[jptr];
            for (k=i; k<j; k++) count[3] += wt[jptr]*wt[sort2[k]]; 
            count[2] += wt[jptr] * nwt[index];            /* tied on x */
            child = (2*index) +1;   /* left child */
            if (child < ntree) count[0] += wt[jptr] * twt[child];
            child++;
            if (child < ntree) count[1] += wt[jptr] * twt[child];

            while (index >0) {  /* walk up the tree  */
                parent = (index-1)/2;
                if (index &1)   /* I am the left child */
                     count[1] += wt[jptr] * (twt[parent] - twt[index]);
                else count[0] += wt[jptr] * (twt[parent] - twt[index]);
                index = parent;
                }
	    }                    
        }
    else j = i+1;

    /* Add the weights for these obs into the tree and compute variance */
    for (; i<j; i++) {
        wsum1 =0;
        oldmean = twt[0]/2;
        iptr = sort2[i];
	index = indx[iptr];
        nwt[index] += wt[iptr];
	twt[index] += wt[iptr];
        wsum2 = nwt[index];
        child = 2*index +1;  /* left child */
        if (child < ntree) wsum1 += twt[child];
	while (index >0) {
            parent = (index-1)/2;
            twt[parent] += wt[iptr];
            if (!(index&1)) /* I am a right child */
                wsum1 += (twt[parent] - twt[index]);
            index=parent;
	    }
        wsum3 = twt[0] - (wsum1 + wsum2);
        lmean = wsum1/2;
        umean = wsum1 + wsum2 + wsum3/2;  /* new upper mean */
        newmean = twt[0]/2;
        myrank = wsum1 + wsum2/2;
        vss += wsum1*(newmean+ oldmean - 2*lmean) * (newmean-oldmean);
        vss += wsum3*(newmean+ oldmean +wt[iptr] - 2*umean) * (oldmean-newmean);
        vss += wt[iptr]* (myrank -newmean)*(myrank -newmean);
        }
    count[4] += ndeath * vss/twt[0];
    }
@ 

One last wrinkle is tied risk scores: they are all set to point to
the same node of the tree.

This part of the compuation is a separate function, since it is
also called by the coxph routines. 
Although we are very careful to create integers and/or doubles for the
arguments to .Call I still wrap them in the appropriate as.xxx 
construction: ``belt and suspenders''.
Also, referring to the the mathematics many paragraphs ago, the C routine
returns the variance of $(C-D)/2$ and we return the standard deviation of
$(C-D)$.
If this routine is called with all the x values identical, then $C$ and $D$
will both be zero, but the calculated variance of $C-D$ can be a nonzero
tiny number due to round off error.  Since this can cause a warning message
from the sqrt function we check and correct this.
<<survConcordance.fit>>=
survConcordance.fit <- function(y, x, strata, weight) { 
    # The coxph program may occassionally fail, and this will kill the C
    #  routine below
    if (any(is.na(x)) || any(is.na(y))) return(NULL)   
    <<btree>>
        
    docount <- function(stime, risk, wts) {
        if (attr(stime, 'type') == 'right') {
            ord <- order(stime[,1], -stime[,2])
            ux <- sort(unique(risk))
            n2 <- length(ux)
            index <- btree(n2)[match(risk[ord], ux)]
             .Call(Cconcordance1, stime[ord,], 
                   as.double(wts[ord]), 
                   as.integer(index), 
                   as.integer(length(ux)))
        }
        else if (attr(stime, 'type') == "counting") {
            sort.stop <- order(-stime[,2], stime[,3])
            sort.start <- order(-stime[,1])
            ux <- sort(unique(risk))
            n2 <- length(ux)
            index <- btree(n2)[match(risk, ux)] - 1L

            .Call(Cconcordance2, stime, 
                  as.double(wts), 
                  as.integer(index), 
                  as.integer(length(ux)),
                  as.integer(sort.stop-1L), 
                  as.integer(sort.start-1L))
        }
        else stop("Invalid survival type for concordance")
    }
        
    if (missing(weight) || length(weight)==0)
        weight <- rep(1.0, length(x))
    storage.mode(y) <- "double"
    
    if (missing(strata) || length(strata)==0) {
        count <- docount(y, x, weight)
        if (count[1]==0 && count[2]==0) count[5]<-0
        else count[5] <- 2*sqrt(count[5])
        names(count) <- c("concordant", "discordant", "tied.risk", "tied.time",
                          "std(c-d)")
    }
    else {
        strata <- as.factor(strata)
        ustrat <- levels(strata)[table(strata) >0]  #some strata may have 0 obs
        count <- matrix(0., nrow=length(ustrat), ncol=5)
        for (i in 1:length(ustrat)) {
            keep <- which(strata == ustrat[i])
            count[i,] <- docount(y[keep,,drop=F], x[keep], weight[keep])
        }
        
        count[,5] <- 2*sqrt(ifelse(count[,1]+count[,2]==0, 0, count[,5]))
        dimnames(count) <- list(ustrat,  c("concordant", "discordant",
                                           "tied.risk", "tied.time",
                                           "std(c-d)"))
    }
    count
}
@ 
