\section{Concordance}
 The concordance statistic is the most used measure of goodness-of-fit
in survival models.  
In general let $y_i$ and $x_i$ be observed and predicted data values.
A pair of obervations $i$, $j$ is considered condordant if either
$y_i > y_j, x_i > x_j$ or $y_i < y_j, x_i < x_j$.
The concordance is the fraction of concordant pairs.
For a Cox model remember that the predicted survival $\hat y$ is longer if
the risk score $X\beta$ is lower, so we have to flip the definition and
count ``discordant'' pairs, this is done at the end of the routine.

One wrinkle is what to do with ties in either $y$ or $x$.  Such pairs
can be ignored in the count (treated as incomparable), treated as discordant,
or given a score of 1/2.
\begin{itemize}
  \item Kendall's $\tau$-a scores ties as 0.
  \item Kendall's $\tau$-b and the Goodman-Kruskal $\gamma$ ignore ties in 
    either $y$ or $x$.
  \item Somers' $d$ treats ties in $y$ as incomparable, pairs that are tied
    in $x$ (but not $y$) score as 1/2.  The AUC from logistic regression is
    equal to Somers' $d$.
\end{itemize}
All three of the above range from -1 to 1, the concordance is
$(d +1)/2$.  
For survival data any pairs which cannot be ranked with certainty are
considered incomparable.
For instance $y_i$ is censored at time 10 and $y_j$ is an event (or censor) 
at time 20.  Subject $i$ may or may not survive longer than subject $j$.  
Note that if $y_i$ is censored at time
10 and $y_j$ is an event at time 10 then $y_i > y_j$.  
Observations that are in different strata are also incomparable, 
since the Cox model only compares within strata.

The program creates 4 variables, which are the number of concordant pairs, 
discordant, tied on time, and tied on $x$ but not on time.  
The default concordance is based on the Somers'/AUC definition,
but all 4 values are reported back so that a user
can recreate Kendall's or Goodmans values if desired.

Here is the main routine.
<<concordance>>=
concordance <- function(x, ...) 
    UseMethod("concordance")

concordance.formula <- function(formula, data,
                                weights, subset, na.action, group,
                                ymin=NULL, ymax=NULL, 
                                timewt=c("S", "n", "S/G", "n/G", "n/G2"),
                                influence=0, residuals=TRUE, reverse=FALSE) {
    Call <- match.call()  # save a copy of of the call, as documentation
    timewt <- match.arg(timewt)
    
    index <- match(c("formula", "data", "weights", "subset", "na.action", 
                     "group"),
                   names(Call), nomatch=0)
    temp <- Call[c(1, index)]
    temp[[1L]] <-  quote(stats::model.frame)
    special <- c("strata", "cluster")
    temp$formula <- if(missing(data)) terms(formula, special)
                    else              terms(formula, special, data=data)
    mf <- eval(temp, parent.frame())  # model frame
    if (nrow(mf) ==0) stop("No (non-missing) observations")
    Terms <- terms(mf)

    Y <- model.response(mf)
    if (!inherits(Y, "Surv")) {
        if (is.numeric(Y) && is.vector(Y))  Y <- Surv(Y)
        else stop("left hand side of the formula  must be a numeric vector or a surival")
    }
    n <- nrow(Y)

    wt <- model.weights(mf)
    offset<- attr(Terms, "offset")
    if (length(offset)>0) stop("Offset terms not allowed")

    stemp <- untangle.specials(Terms, "strata")
    if (length(stemp$vars)) {
	if (length(stemp$vars)==1) strat <- m[[stemp$vars]]
	else strat <- strata(m[,stemp$vars], shortlabel=TRUE)
        Terms <- Terms[-stemp$terms]
    }
    else strat <- NULL
    
    group <- model.extract(mf, "(group)")
    cluster<- attr(Terms, "specials")$cluster
    if (length(cluster)) {
        tempc <- untangle.specials(Terms, 'cluster', 1:10)
        ord <- attr(Terms, 'order')[tempc$terms]
        if (any(ord>1)) stop ("Cluster can not be used in an interaction")
        cluster <- strata(mf[,tempc$vars], shortlabel=TRUE)  #allow multiples
        Terms <- Terms[-tempc$terms]  # toss it away
    }
    else if (length(group)==0) group <- seq.int(1,n)
    if (length(group)) cluster <- group
                                            
    x <- model.matrix(Terms, mf)[,-1, drop=FALSE]  #remove the intercept
    if (ncol(x) > 1) stop("Only one predictor variable allowed")

    if (!is.null(ymin) & (length(ymin)> 1 || !is.numeric(ymin)))
        stop("ymin must be a single number")
    if (!is.null(ymax) & (length(ymax)> 1 || !is.numeric(ymax)))
        stop("ymax must be a single number")
    
    fit <- concordance.fit(Y, x, strat, wt, ymin, ymax, timewt, cluster,
                           influence, residuals)
 
    if (!is.logical(reverse)) 
        stop ("the reverse argument must be TRUE/FALSE")
    if (reverse) {
        fit$concordance <- 1- fit$concordance
        if (is.matrix(fit$influence)) fit$influence <- fit$influence[,c(2,1,3,4)]
        if (is.matrix(cfit$count)) fit$count <- fit$count[, c(2,1,3,4)]
        else fit$count <- fit$count[c(2,1,3,4)]
    }

    na.action <- attr(mf, "na.action")
    if (length(na.action)) fit$na.action <- na.action
    fit$call <- Call

    class(fit) <- 'concordance'
    fit
}

print.concordance <- function(x, ...) {
    if(!is.null(cl <- x$call)) {
        cat("Call:\n")
        dput(cl)
        cat("\n")
        }
    omit <- x$na.action
    if(length(omit))
        cat("n=", x$n, " (", naprint(omit), ")\n", sep = "")
    else cat("n=", x$n, "\n")
    cat("Concordance= ", format(x$concordance), " se= ", format(sqrt(x$var[1])),
        '\n', sep='')

    if (!is.matrix(x$count) || nrow(fit$count < 11)) print(x$count)
    invisible(x)
    }

<<concordancefit>>

<<btree>>
@ 

The concordance.fit function is broken out separately, since it is called
by the \code{coxph} routine.  This also allows other packages to build on
it.
If $y$ is not a survival quantity, then all of the options for the
\code{timewt} parameter lead to the same result.

<<concordancefit>>=
concordance.fit <- function(y, x, strata, weight, ymin, ymax, timewt,
                            group, influence=0, residuals=FALSE) {
    # The coxph program may occassionally fail, and this will kill the C
    #  routine below.  So check for it.
    if (any(is.na(x)) || any(is.na(y))) return(NULL)

    # these should only occur if something outside survival calls this routine
    n <- length(y)
    if (length(x) != n) stop("x and y are not the same length")
    if (missing(strata) || length(strata)==0) strata <- rep(1L, n)
    if (length(strata) != n)
        stop("y and strata are not the same length")
    if (missing(weight) || length(weight)==0) weight <- rep(1, n)
    else if (length(weight) != n) stop("y and weight are not the same length")
    if (!is.Surv(y)) {
        y <- Surv(y)
        timewt <- 'n'
    }

    type <- attr(y, "type")
    if (type %in% c("left", "interval"))
        stop("left or interval censored data is not supported")
    if (type %in% c("mright", "mcounting"))
        stop("multiple state survival is not supported")
   
    # This routine is called once per stratum
    docount <- function(stime, risk, wts, group, timeopt= 'n') {
        n <- length(risk)
        # this next line is mostly invoked in stratified logistic, where
        #  only 1 event per stratum occurs.  All time weightings are the same
        if (sum(stime[,ncol(stime)]) <2) timewt <- 'n'
        
        sfit <- survfit(stime~1, weight=wts, se.fit=FALSE)
        etime <- sfit$time[sfit$n.event > 0]
        esurv <- sfit$surv[sfit$n.event > 0]
        
        if (timeopt %in% c("S/G", "n/G", "n/G2")) {
            temp <- stime
            temp[,ncol(temp)] <- 1- temp[,ncol(temp)] # switch event/censor
            gfit <- survfit(temp~1, weight=wts, se.fit=FALSE)
            # gfit is not a survfit object
            gsurv <- approx(c(0, gfit$time), c(1, gfit$surv), etime,
                            method="constant", f=0, rule=2)$y
        }

        timewt <- switch(timeopt,
            "S" = sum(wts)*esurv,
            "S/G" = sum(wts)* esurv/gsurv,
            "n" =   (sfit$n.risk- sfit$n.event)[sfit$n.event>0],
            "n/G" = (sfit$n.risk -sfit$n.event)[sfit$n.event>0]/gsurv,
            "n/G2"= (sfit$n.risk -sfit$n.event)[sfit$n.event>0]/gsurv^2)
            
         # order the data
        if (ncol(stime)==2) {
            esort <- order(-y[,1], y[,2])  # reverse time, censors before deaths
            stime <- stime[esort,]
            risk  <- risk[esort]
            wts   <- wts[esort]
            group <- group[esort]
        } else {
            sort.stop  <- order(-stime[,2], stime[,3])   #order by endpoint
            sort.start <- order(-stime[,1])
        }
 
        # match each prediction score to the unique set of scores
        # (to deal with ties)
        utemp <- match(risk, sort(unique(risk)))
        bindex <- btree(max(utemp))[utemp]
        if (is.factor(group)) gtemp <- group[1:n, drop=TRUE]
        else gtemp <- as.factor(group)
        
        storage.mode(stime) <- "double"  # just in case y is integer
        if (ncol(y) ==2) 
            fit <- .Call(Cconcordance3, stime, bindex, wts, timewt, 
                         as.integer(gtemp) -1L)
        else fit <- .Call(Cconcordance4, stime, bindex, wts, timewt, 
                          sort.start, sort.stop, as.integer(gtemp) -1L)

        dimnames(fit$resid) <- list(etime, 
                                    c("rank", "npair", "wt", "var"))
        dimnames(fit$influence) <- list(levels(gtemp),
                          c("concordant", "discordant", "tied.x", "tied.y"))
       fit
    }
    
    if (missing(strata) || length(strata)==0 || all(strata==strata[1])) {
        fit <- docount(y, x, weight, group)
        npair <- sum(fit$count[1:3])
        d <- (fit$count[1] - fit$count[2])/npair  # Somers' d 
        dfbeta <- (fit$influence[,1]- fit$influence[,2])/npair -
            (d/npair)*rowSums(fit$influence[,1:3])
 
        rval <- list(concordance= (d+1)/2, count=fit$count[1:4], n=n,
                     var = c(IJ= sum(dfbeta^2)/4, 
                             PH= fit$count[5]/(4*npair^2)), resid=fit$resid)
        names(rval$count) <- c("concordant", "discordant", "tied.x", "tied.y")
        if (influence==1) rval$dfbeta <- dfbeta
        else if (influence==2) rval$influence <- fit$influence
    }
    else { 
        strata <- as.factor(strata)
        ustrat <- levels(strata)[table(strata) >0]  #some strata may have 0 obs
        tfit <- lapply(ustrat, function(i) {
            keep <- which(strata== i)
            docount(y[keep,,drop=F], x[keep], weight[keep], group[keep])
        })
        
        count <- t(sapply(tfit, function(x) x$count))
        dimnames(count) <- list(ustrat, c("concordant", "discordant", "tied.x", 
                                          "tied.y", "std(c-d"))
        resid <- do.call("rbind", lapply(tfit, function(x) x$resid))
        resid <- cbind(resid, strata= rep(ustrat, 
                                       sapply(tfit, function(x) nrow(x$resid))))
        influence <- do.call("rbind", lapply(tfit, function(x) x$influence))
        influence <- cbind(influence, strata=rep(ustrat,
                                    sapply(tfit, function(x) nrow(x$influence))))

        npair <- sum(fit$count[,1:3])
        d <- sum(fit$count[,1] - fit$count[,2])/npair  # Somers' d 
        dfbeta <- (fit$influence[,1]- fit$influence[,2])/npair -
            (d/npair)*rowSums(fit$influence[,1:3])
 
        rval <- list(concordance= (d+1)/2, count=fit$count[,1:4], n=n, 
                     var = c(IJ= sum(dfbeta^2)/4, 
                             PH= fit$count[5]/(4*npair^2)), resid=fit$resid)

        if (influence > 1) {
            # reorder the influence to be data set order
            unsort <- integer(nrow(influence))
            unsort[order(as.numeric(strata))] <- 1:nrow(influence)
            if (influence==1) rval$dfbeta <- dfbeta[unsort]
            else rval$dfbeta <- influence[unsort,]
        }
    }
    rval
}
@ 

The C routine returns an influence matrix with one row per subject $i$, 
and columns giving the partial with respect to $w_i$ for the number of
concordant, discordant, tied on $x$ and ties on $y$ pairs.
Somers' $d$ is $(C-D)/m$ where $m= C + D + T$ is the total number of %'
comparable pairs, which does not count the tied-on-y column.
For any given subject or group $k$ (for grouped jackknife) the
IJ estimate of the variance is
\begin{align*}
  V &\ \sum_k  \left(\frac{\partial d}{\parial w_k}\right)^2 \\
  \frac{\partial d}{\parial w_k} &= 
      \frac{1}{m} \left[\frac{\partial{C-D}}{\partial w_k} -
        d \frac{\partial C+D+T}{\partial w_k} \right] \\
\end{equation*}




The C code looks a lot like a Cox model: walk forward through time, keep
track of the risk sets, and add something to the totals at each death.
What needs to be summed is the rank of the event subject's $x$ value, as
compared to the value for all others at risk at this time point.
For notational simplicity let $Y_j(t_i)$ be an indicator that subject $j$
is at risk at event time $t_i$, and $Y^*_j(t_i)$ the more restrictive one that
subject $j$ is both at risk and not a tied event time.
The values we want at time $t_i$ are
\begin{align}
  C_i &= v_i \delta_i w_i \sum_j w_j Y^*_j(t_i) \left[I(x_i < x_j) \right]
    \label{C} \\
  D_i &= v_i \delta_i w_i \sum_j w_j Y^*_j(t_i) \left[I(x_i > x_j)\right] 
     \label{D} \\
  T_i &= v_i \delta_i w_i \sum_j w_j Y^*_j(t_i) \left[I(x_i = x_j) \right]
     \label{T}  \\
\end{align} 

In the above $v$ is an optional time weight, which we will discuss later.
The normal concordance definition has $v=1$.
$C$, $D$, and $T$ are the number of concordant, discordant, and tied
pairs, respectively,
and $m= C+D+T$ will be the total number of concordant pairs.
Somers' $d$ is $(C-D)/m$ and the concordance is $(d+1)/2 = (C + T/2)/m$.

The primary compuational question is how to do this efficiently, i.e., better
than a naive algorithm that loops across all $n(n-1)/2$ 
possible pairs.
There are two key ideas.
\begin{enumerate}
\item Rearrange the counting so that we do it by death times.
  For each death we count the number of other subjects in the risk set whose
  score is higher, lower, or tied and add it into the totals.
  This neatly solves the question of time-dependent covariates.
\item Counting the number with higher, lower, and tied $x$ can be done in 
   $O(\log_2 n)$ time if the $x$ data is kept in a binary tree.
\end{enumerate}

\begin{figure}
  \myfig{balance}
  \caption{A balanced tree of 13 nodes.}
  \label{treefig}
\end{figure}

Figure  \ref{treefig} shows a balanced binary tree containing  
13 risk scores.  For each node the left child and all its descendants
have a smaller value than the parent, the right child and all its
descendents have a larger value.
Each node in figure \ref{treefig} is also annotated with the total weight
of observations in that node and the weight for itself plus all its children 
(not shown on graph).  
Assume that the tree shown represents all of the subjects still alive at the
time a particular subject ``Smith'' expires, and that Smith has the risk score
of 19 in the tree.
The concordant pairs are those with a risk score $>19$, i.e., both $\hat y=x$
and $y$ are larger, discordant are $<19$, and we have no ties.
The totals can be found by
\begin{enumerate}
  \item Initialize the counts for discordant, concordant and tied to the
    values from the left children, right children, and ties at this node,
    respectively, which will be $(C,D,T) = (1,1,0)$.
  \item Walk up the tree, and at each step add the (parent + left child) or
    (parent + right child) to either D or C, depending on what part of the
    tree has not yet been totaled.  
    At the next node (8) $D= D+4$, and at the top node $C=C + 6$.
\end{enumerate}

There are 5 concordant and 7 discordant pairs.
This takes a little less than $\log_2(n)$ steps on average, as compared to an
average of $n/2$ for the naive method.  The difference can matter when $n$ is
large since this traversal must be done for each event.

The classic way to store trees is as a linked list.  There are several 
algorithms for adding and subtracting nodes from a tree while maintaining
the balance (red-black trees, AA trees, etc) but we take a different 
approach.  Since we need to deal with case weights in the model and we
know all the risk score at the outset, the full set of risk scores is
organised into a tree at the beginning, updating the sums of weights at
each node as observations are added or removed from the risk set.

If we internally index the nodes of the tree as 1 for the top, 
2--3 for the next 
horizontal row, 4--7 for the next, \ldots then the parent-child 
traversal becomes particularly easy.
The parent of node $i$ is $i/2$ (integer arithmetic) and the children of
node $i$ are $2i$ and $2i +1$.  In C code the indices start at 0 of course.
The following bit of code arranges data into such a tree.
<<btree>>=
btree <- function(n) {
   tfun <- function(n, id, power) {
       if (n==1L) id
       else if (n==2L) c(2L *id + 1L, id)
       else if (n==3L) c(2L*id + 1L, id, 2L*id +2L)
       else {
           nleft <- if (n== power*2L) power  else min(power-1L, n-power%/%2L)
           c(tfun(nleft, 2L *id + 1L, power%/%2), id,
             tfun(n-(nleft+1L), 2L*id +2L, power%/%2))
       }
   }
   tfun(as.integer(n), 0L, as.integer(2^(floor(logb(n-1,2)))))
}
@ 

Referring again to figure \ref{treefig}, \code{btree(13)} yields the vector
\code{7  3  8  1  9  4 10  0 11  5 12  2  6}
meaning that the smallest element
will be in position 8 of the tree, the next smallest in position 2, etc,
and using indexing that starts at 0 since the results will be passed to a C
routine.
The code just above takes care to do all arithmetic as integer.  
This actually made almost no difference in the compute time, but it was an
interesting exercise to find that out.

The next question is how to compute a variance for the result.
One approach is to compute an infinitesimal jackknife (IJ) estimate,
for which we need derivatives with respect to the weights.
Looking back at equation \eqref{C} we have
\begin{align}
  C  &= \sum_i  w_i \delta_i \sum_j Y^*_j(t_i) w_j I(x_i < x_j) 
  \nonumber\\
% \frac{\partial C}{\partial w_k} &= 
%    (v_k/m_k)\delta_k \sum_j Y^*_{j}(t_k) I(x_k < x_j) +
%    \sum_i (v_i/m_i) w_i Y^*_k(t_i) I(x_i < x_k) \label{partialC}
\end{align}
A given subject's weight appears multiple times, once when they are an
event ($w_i \delta_i)$, and then as part of the risk set for other's
events.  I avoided this for some time because it looked like an $O(nd)$
process to separately update each subject's influence for each risk set
they inhabit, but David Watson pointed out a path forward.
The solution is to keep two trees.  
Tree 1 contains all of the subjects at risk.  We traverse it when each subject
is added in, updating the tree, 
and traverse it again at each death, pulling off values to update our sums. 
The second tree holds only the deaths and is updated at each death;
it is read out twice per subject,
once just before they enter the risk set and once when they leave.

The basic algorithm is to move through an outer and inner loop.  The
outer loop moves across unique times, the inner for all obs that
share a death time.  We progress from largest to smallest time.

A second viewpoint treats the data as a Cox model.
Create zero-centered scores for all subjects in the risk set:
\begin{align}
  z_i(t) &= \sum_{j \in R(t)} w_j \sign(x_i - x_j) \nonumber
  D-C = \sum_i \delta_i z_i(t_i)              \label{zcord}
\end{align}
At any event time $\sum w_i z_i =0$.  
Equation \eqref{zcord} is the score equation
for a Cox model with time-dependent covariate $z$.
When two subjects have an event at the same time, this formulation treats
each of them as being in the other's risk set whereas the concordance
treats them as incomparable --- how can they be the same?
The trick is that $D-C$ does not change: the tied pairs add equally to
$D$ and $C$.
Under the null hypothesis that the risk score is not related to outcome,
each term in \eqref{zcord} is a random selection from the $z$ scores in
the risk set, and the variance of the addition is the variance of $z$,
the sum of these over deaths is the Cox model information matrix,
which is also the variance of the score statistic.
The mean of $z$ is always zero, so we need to keep track of 
$\sum w_i z^2$. 

How can we do this efficiently?  First note that $z_i$ can be written
as sum(weights for smaller x) - sum(weights for larger x), and in fact the
weighted mean for any slice of $x$, $a < x < b$, is exactly the
same: mean = sum(weights for x values below the range) - 
 sum(weights above the range).
The second trick is to use an ANOVA decomposition of the variance of $z$ into
within-slice and between-slice sums of squares, where the 3 slices are the
$z$ scores at a given $x$ value (node of the tree), weights for score below that
cutpoint, and above.
Assume that a new observation $k$ has just been added to the tree.  
This will add $w_k$ to all the $z$ values above, and to the weighted mean of
all those above, $-w_k$ to the values and means below, and 0 to the values and
means of any tied observations.  Thus none of the current `within'
SS change.  
Let $s_a$, $s_b$ and $s_0$ be the current sum of weights above, below, and
at the node of the tree.  The mean for the above group was $(s_b + s_0)$ with
between SS contribution of $s_a (s_b + s_0)^2$.  The below mean was 
$-(s_a + s_0)$  with between SS contribution of $s_b(s_a + s_0)^2$.
The change to the between SS from adding the new subject is
$$
s_a\left( (s_b+s_0 + w_k)^2 - (s_b + s_0)^2 \right) =
s_a (2w_k (s_b + s_0) + w_k^2)
$$
while the change in between SS for the below group 
is $s_b(2w_k(s_a + s_0) + w_k^2)$, and there is no change for the 
prior observations in the middle group.
Last we add $w_kz_k^2 = w_k(s_b- s_a)^2$ to the sum for the new observation.
Putting all this together the change is
$$
  w_k \left(s_a (w_k + (s_b + s_c)) + s_b(w_k + (s_a + s_c)) + (s_a-s_b)^2 \right)
$$

We can now define the C-routine that does the bulk of the work.
First we give the outline shell of the code and then discuss the
parts one by one.  This routine  is for ordinary survival data, and
will be called once per stratum.
Input variables are
\begin{description}
  \item[n] the number of observations
  \item[y] matrix containing the time and status, data is sorted by descending 
    time, with censorings precedint deaths.
  \item[x] the tree node at which this observation's risk score resides  %'
  \item[wt] case weight for the observation
  \item[group] which IJ group each observation will be counted into
\end{description}
The routine will return list with three components:
\begin{itemize}
  \item count, a vector containing the weighted number of concordant, 
    discordant, tied on $x$ but not $y$, and tied on y pairs.  
    The weight for a pair is $w_iw_j$.
  \item resid, a three column matrix with one row per event, containing the 
    score residual at that event, its variance, and the sum of weights.
    The score residual is
    a rescaled $z_i$ so as to lie between 0 and 1: $(1+ z/\sum(w))/2$.
    The concordance is then a weighted sum of the residuals.
  \item influence, a matrix with one row per observation and 4 columns, giving
    that observation's first derivative with respect to the count vector.
\end{itemize}    

<<concordance3>>=
#include "survS.h"

<<walkup>>
    
SEXP concordance3(SEXP y, SEXP x2, SEXP wt2, SEXP timewt2, SEXP group2) {
    int i, j, k;
    int n, ngroup, ntree, nevent;
    double *time, *status;

    /* sum of weights for a node (nwt), sum of weights for the node and
    **  all of its children (twt), then the same again for the subset of
    **  deaths
    */
    double *nwt, *twt, *dnwt, *dtwt;
    double z2;  /* sum of z^2 values */    
        
    int ndeath;   /* total number of deaths at this point */    
    double dwt;   /* weighted number of deaths at this point */
    double wsum[3]; /* the sum of weights that are > current, <, or equal  */
    double temp;

    SEXP rlist, count2, imat2, resid2;
    double *count, *imat[4], *resid[4];
    double *wt, *timewt;
    int    *x, *group;
    static const char *outnames[]={"count", "resid", "influence", ""};
    
    n = nrows(y);
    x = INTEGER(x2);
    wt = REAL(wt2);
    timewt = REAL(timewt2);
    group = INTEGER(group2);
    time = REAL(y);
    status = time + n;
   
    /* if there are tied predictors, the total size of the tree will be < n */
    /* for a grouped jackknife the number of unique groups is < n */
    ntree =0; nevent =0; ngroup =0;
    for (i=0; i<n; i++) {
	if (x[i] >= ntree) ntree = x[i] +1;  
        if (group[i] >= ngroup) ngroup = group[i] +1; 
        nevent += status[i];
    }
        
    nwt = (double *) R_alloc(4*ntree, sizeof(double));
    twt = nwt + ntree;
    dnwt = twt + ntree;
    dtwt = dnwt + ntree;
    
    for (i=0; i< 4*ntree; i++) nwt[i] =0.0;
    
    PROTECT(rlist = mkNamed(VECSXP, outnames));
    count2 = SET_VECTOR_ELT(rlist, 0, allocVector(REALSXP, 5));
    count = REAL(count2); 
    for (i=0; i<5; i++) count[i]=0.0;
    resid2 = SET_VECTOR_ELT(rlist, 1, allocMatrix(REALSXP, nevent, 4));
    for (i=0; i<4; i++) resid[i] = REAL(resid2) + i*nevent;
    imat2 = SET_VECTOR_ELT(rlist, 2, allocMatrix(REALSXP, ngroup, 4));
    for (i=0; i<4; i++) {
        imat[i] = REAL(imat2) + i*ngroup;
        for (j=0; j<ngroup; j++) imat[i][j] =0;
    }
    
    <<concordance3-work>>
        
    UNPROTECT(1);
    return(rlist);
}
@ 

The key part of our computation is to update the vectors of weights.
We don't actually pass the risk score values $r$ into the routine,   %'
it is enough for each observation to point to the appropriate tree
node.
The tree contains the weights for everyone whose survival is larger
than the time currently under review, so starts with all weights
equal to zero.  
For any pair of observations $i,j$ we need to add [[wt[i]*wt[j]]]
to the appropriate count.
Starting at the largest time (which is sorted last), walk through the tree.
\begin{itemize}
  \item For either a death or censoring time, first add -1 times the current
    death information into the influence matrix.  
  \item If it is a censored subject, add them into the total tree.
    If it is a death time, we need to make 3 passes over the set of
    deaths tied at this time.
    \begin{itemize}
      \item Pass 1. Compare each new obs to the current tree, and update
        the concordance counts.
        \begin{enumerate}
          \item The addition to tied-on-x will be the weight for this 
            observation, times
            the sum of weights for all others with the same risk score (x) and a
            a greater time, i.e., the weight found at \code{x[i]} in the tree.
          \item Walk the tree to count concordant and discordant. 
            First add in the children of this node.  The left child will be 
            smaller risk scores and longer times, adding to the discordant
            pairs, the right child adds to concordant.
            Then walk up the tree to the root. 
            At each step up we add in data for new node plus the 'not me' branch.
            If we were the right branch (odd number node) of a parent
            then the parent + parent's left child will add to the
            discordant, and to concordant if we are a left branch.
        \end{enumerate}
      \item Pass 2: Add each of the tied subjects' weights into both the total
        tree and the death tree.  Also update $\sum z^2$.
      \item Pass 3: Now that the tree includes all the risk set, fill in rows
        of the residuals matrix.
    \end{itemize}
\end{itemize}
When all the subjects have been added to the tree, then add the death tree
data for each subject to the influence matrix.  

<<concordance3-work>>=
z2 =0; 
for (i=0; i<n;) {
    /* Initialize the influence */
    walkup(dnwt, dtwt, x[i], wsum, ntree);
    imat[0][group[i]] -= wsum[1];
    imat[1][group[i]] -= wsum[0];
    imat[2][group[i]] -= wsum[2];

    if (status[i]==0) { /* censored, simply add them into the tree */
        walkup(nwt, twt, x[i], wsum, ntree);
	z2 += wt[i]*(wsum[0]*(wt[i] + 2*(wsum[1] + wsum[2])) +
		     wsum[1]*(wt[i] + 2*(wsum[0] + wsum[2])) +
		     (wsum[0]-wsum[1])*(wsum[0]-wsum[1]));
        addin(nwt, twt, x[i], wt[i]);
        i++;
    }
    else { /* process all tied deaths at this point */
	ndeath=0; dwt=0; 
        /* update the counts and residuals */
        for (j=i; j<n && time[j]==time[i]; j++) {
           count[3] += wt[j] * dwt;	    
           ndeath++; dwt += wt[j];   /* count of deaths and sum of wts */

           walkup(nwt, twt, x[j], wsum, ntree);
	   nevent--;  /* backwards on the survival curve */
           temp = timewt[nevent]/twt[0]; /* adjusted time weight */
	   for (k=0; k<3; k++) {
	       count[k] += wt[j]* wsum[k] *temp;
	       imat[k][group[j]] += wsum[k]*temp;
	   }
           imat[2][group[j]] -= wt[j]*temp;
           addin(dnwt, dtwt, x[j], temp*wt[j]);  /* add weighted deaths */

           /* residuals */
	   z2 += wt[j]*(wsum[0]*(wt[j] + 2*(wsum[1] + wsum[2])) +
                        wsum[1]*(wt[j] + 2*(wsum[0] + wsum[2])) +
                        (wsum[0]-wsum[1])*(wsum[0]-wsum[1]));
	   resid[0][nevent] = (wsum[0] - wsum[1])/twt[0]; /* -1 to 1 */
	   resid[1][nevent] = twt[0];
	   resid[2][nevent] = wt[j];
	}

	/* 
        ** Add deaths into the tree, tied time information into the
        **  influence, and Cox variance to the residuals.  All tied resids
        **  get the same variance.
        */
        temp = twt[0] + dwt;  /* total weight at risk */
        count[4] += dwt * z2/temp;  /* weighted variance in risk set*/
	for (j=0; j< ndeath; j++) {
            resid[3][nevent-j] = z2/(temp*temp*temp);
            imat[3][group[i]] += dwt - wt[i];
	    addin(nwt, twt, x[i], wt[i]);
	    i++;
        }
    }
}

/* 
** Now finish off the influence for each observation 
**  Since times flip (looking backwards) the wsum contributions flip too
*/
for (i=0; i<n; i++) {
    walkup(dnwt, dtwt, x[i], wsum, ntree);
    imat[0][group[i]] += wsum[1];
    imat[1][group[i]] += wsum[0];
    imat[2][group[i]] += wsum[2];
}
@ 

<<walkup>>=
void walkup(double *nwt, double* twt, int index, double sums[3], int ntree) {
    int i, j, parent;

    for (i=0; i<3; i++) sums[i] = 0.0;
    sums[2] = nwt[index];   /* tied on x */
    
    j = 2*index +2;  /* right child */
    if (j < ntree) sums[0] += twt[j];
    if (j <=ntree) sums[1]+= twt[j-1]; /*left child */

    while(index > 0) { /* for as long as I have a parent... */
        parent = (index-1)/2;
        if (index%2 == 1) sums[0] += twt[parent] - twt[index]; /* left child */
	else sums[1] += twt[parent] - twt[index]; /* I am a right child */
	index = parent;
    }
}

void addin(double *nwt, double *twt, int index, double wt) {
    nwt[index] += wt;
    while (index >0) {
	twt[index] += wt;
	index = (index-1)/2;
    }
    twt[0] += wt;
}

@ 
The code for [start, stop) data is quite similar.  
As in the agreg routines there are two sort indices, the first indexes
the data by stop time, longest to earliest, and the second by start time. 
The [[y]] variable now has three columns.
<<concordance3>>= 
SEXP concordance4(SEXP y, SEXP x2, SEXP wt2, SEXP timewt2, 
                  SEXP sortstop, SEXP sortstart, SEXP group2) {
    int i, j, k, ii, jj, i2;
    int n, ngroup, ntree, nevent;
    double *time1, *time2, *status;

    /* sum of weights for a node (nwt), sum of weights for the node and
    **  all of its children (twt), then the same again for the subset of
    **  deaths
    */
    double *nwt, *twt, *dnwt, *dtwt;
    double z2;  /* sum of z^2 values */    
 
    int ndeath;   /* total number of deaths at this point */    
    double dwt;   /* weighted number of deaths at this point */
    double wsum[3]; /* the sum of weights that are > current, <, or equal  */
    double temp, dtime;

    SEXP rlist, count2, imat2, resid2;
    double *count, *imat[4], *resid[4];
    double *wt, *timewt;
    int    *x, *group, *sort1, *sort2;
    static const char *outnames[]={"count", "resid", "influence", ""};
     
    n = nrows(y);
    x = INTEGER(x2);
    wt = REAL(wt2);
    timewt = REAL(timewt2);
    group = INTEGER(group2);
    sort2 = INTEGER(sortstop);
    sort1 = INTEGER(sortstart);
    
    time1 = REAL(y);
    time2 = time1 + n;
    status= time2 + n;

    /* 
    ** if there are tied predictors, the total size of the tree will be < n 
    ** influence matrix is 4 by ngroup, ngroup <n with grouped jackknife
    ** residual is 4 x number of events
    */
    ntree =0; nevent =0; ngroup =0;
    for (i=0; i<n; i++) {
        if (x[i] >= ntree) ntree = x[i] +1;  
        if (group[i] >= ngroup) ngroup = group[i] +1; 
        nevent += status[i];
    }
        
    nwt = (double *) R_alloc(4*ntree, sizeof(double));
    twt = nwt + ntree;
    dnwt = twt + ntree;
    dtwt = dnwt + ntree;
    
    for (i=0; i< 4*ntree; i++) nwt[i] =0.0; /* zero the trees */
    
    PROTECT(rlist = mkNamed(VECSXP, outnames));
    count2 = SET_VECTOR_ELT(rlist, 0, allocVector(REALSXP, 5));
    count = REAL(count2); 
    for (i=0; i<5; i++) count[i]=0.0;
    resid2 = SET_VECTOR_ELT(rlist, 1, allocMatrix(REALSXP, nevent, 4));
    for (i=0; i<4; i++) resid[i] = REAL(resid2) + i*nevent;
    imat2 = SET_VECTOR_ELT(rlist, 2, allocMatrix(REALSXP, ngroup, 4));
    for (i=0; i<4; i++) {
        imat[i] = REAL(imat2) + i*ngroup;
        for (j=0; j<ngroup; j++) imat[i][j] =0;
    }
 
    <<concordance4-work>>
        
    UNPROTECT(1);
    return(rlist);
}
@ 

The processing changes in 2 ways
\begin{itemize}
  \item Use [[sort1[i]]] instead of [[i]] as the subscript for all the data
    vectors (time, status, x, wt, group).
    (The sort vectors go backwards in time.)
    This happens enough that we use temporary variables \code{ii} and \code{jj}}
    to avoid the double subscript.
  \item As we move from the longest time to the shortest observations are added
    into the tree of weights whenever we encounter their stop time. 
    This is just as before.  Weights now also need to be removed from the 
    tree whenever we encounter an observation's start time.              %'
    It is convenient ``catch up'' on this second task whenever we encounter 
    a death.
\end{itemize}

<<concordance4-work>>=
z2 =0; i2=0; 
for (i=0; i<n;) {
    ii = sort1[i];
    /* Initialize the influence */
    walkup(dnwt, dtwt, x[i], wsum, ntree);
    imat[0][group[ii]] -= wsum[1];
    imat[1][group[ii]] -= wsum[0];
    imat[2][group[ii]] -= wsum[2];

    if (status[ii]==0) { /* censored, add new subject into the tree */
        walkup(nwt, twt, x[ii], wsum, ntree);
	z2 += wt[ii]*(wsum[0]*(wt[i] + 2*(wsum[1] + wsum[2])) +
		     wsum[1]*(wt[i] + 2*(wsum[0] + wsum[2])) +
		     (wsum[0]-wsum[1])*(wsum[0]-wsum[1]));
        addin(nwt, twt, x[ii], wt[ii]);
        i++;
    }
    else { /* subject is a death */
	dtime = time2[ii];

	/* remove those who are no longer at risk */
	for (; time1[sort2[i2]] >= dtime; i2++) {
	    jj = sort2[i2];
	    walkup(dnwt, dtwt, x[jj], wsum, ntree);
	    imat[0][group[jj]] += wsum[1];
	    imat[1][group[jj]] += wsum[0];
	    imat[2][group[jj]] += wsum[2];
	    addin(nwt, twt, x[jj], -wt[jj]);
	}

        /* update the counts and residuals */
	ndeath=0; dwt=0; 
        for (j=i; j<n && time2[sort2[j]]== dtime; j++) {
            jj = sort1[j];
	    count[3] += wt[jj] * dwt;	    
	    ndeath++; dwt += wt[jj];   /* count of deaths and sum of wts */

	    walkup(nwt, twt, x[jj], wsum, ntree);
	    nevent--;  /* backwards on the survival curve */
	    temp = timewt[nevent]/twt[0]; /* adjusted time weight */
	    for (k=0; k<3; k++) {
		count[k] += wt[jj]* wsum[k] *temp;
		imat[k][group[jj]] += wsum[k]*temp;
	    }
	    imat[2][group[jj]] -= wt[jj]*temp;
	    addin(dnwt, dtwt, x[jj], temp*wt[jj]);  /* add weighted deaths */

	    /* residuals */
	    z2 += wt[jj]*(wsum[0]*(wt[jj] + 2*(wsum[1] + wsum[2])) +
                          wsum[1]*(wt[jj] + 2*(wsum[0] + wsum[2])) +
                          (wsum[0]-wsum[1])*(wsum[0]-wsum[1]));
	    resid[0][nevent] = (wsum[0] - wsum[1])/twt[0]; /* -1 to 1 */
	    resid[1][nevent] = twt[0];
	    resid[2][nevent] = wt[jj];
	}

	/* 
        ** Add deaths into the tree, tied time information into the
        **  influence, and Cox variance to the residuals.  All tied resids
        **  get the same variance.
        */
        temp = twt[0] + dwt;  /* total number at risk */ 
        count[4] += dwt * z2/tem
 	for (j=0; j< ndeath; j++) {
            ii = sort2[i];
            resid[3][nevent-j] = z2/(temp*temp*temp);
            imat[3][group[ii]] += dwt - wt[ii];
	    addin(nwt, twt, x[ii], wt[ii]);
	    i++;
        }
    }
}

/* 
** Now finish off the influence for those not yet removed
**  Since times flip (looking backwards) the wsum contributions flip also


*/
for (; i2<2; i2++) {
    ii = sort2[i2];
    walkup(dnwt, dtwt, x[ii], wsum, ntree);
    imat[0][group[ii]] += wsum[1];
    imat[1][group[ii]] += wsum[0];
    imat[2][group[ii]] += wsum[2];
}
@ 
