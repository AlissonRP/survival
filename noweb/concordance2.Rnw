\section{Concordance}
 The concordance statistic is the most used measure of goodness-of-fit
in survival models.  
In general let $y_i$ and $x_i$ be observed and predicted data values.
A pair of obervations $i$, $j$ is considered condordant if either
$y_i > y_j, x_i > x_j$ or $y_i < y_j, x_i < x_j$.
The concordance is the fraction of concordant pairs.
For a Cox model remember that the predicted survival $\hat y$ is longer if
the risk score $X\beta$ is lower, so we have to flip the definition and
count ``discordant'' pairs, this is done at the end of the routine.

One wrinkle is what to do with ties in either $y$ or $x$.  Such pairs
can be ignored in the count (treated as incomparable), treated as discordant,
or given a score of 1/2.
\begin{itemize}
  \item Kendall's $\tau$-a scores ties as 0.
  \item Kendall's $\tau$-b and the Goodman-Kruskal $\gamma$ ignore ties in 
    either $y$ or $x$.
  \item Somers' $d$ treats ties in $y$ as incomparable, pairs that are tied
    in $x$ (but not $y$) score as 1/2.  The AUC from logistic regression is
    equal to Somers' $d$.
\end{itemize}
All three of the above range from -1 to 1, the concordance is
$(d +1)/2$.  
For survival data any pairs which cannot be ranked with certainty are
considered incomparable.
For instance $y_i$ is censored at time 10 and $y_j$ is an event (or censor) 
at time 20.  Subject $i$ may or may not survive longer than subject $j$.  
Note that if $y_i$ is censored at time
10 and $y_j$ is an event at time 10 then $y_i > y_j$.  
Observations that are in different strata are also incomparable, 
since the Cox model only compares within strata.

The program creates 4 variables, which are the number of concordant pairs, 
discordant, tied on time, and tied on $x$ but not on time.  
The default concordance is based on the Somers'/AUC definition,
but all 4 values are reported back so that a user
can recreate Kendall's or Goodmans values if desired.

Here is the main routine.
<<concordance>>=
concordance <- function(x, ...) 
    UseMethod("concordance")

concordance.formula <- function(formula, data,
                                weights, subset, na.action, group,
                                ymin=NULL, ymax=NULL, 
                                timewt=c("S", "n", "S/G", "n/G", "n/G2"),
                                influence=0, residuals=TRUE, reverse=FALSE,
                                timefix=TRUE) {
    Call <- match.call()  # save a copy of of the call, as documentation
    timewt <- match.arg(timewt)
    
    index <- match(c("formula", "data", "weights", "subset", "na.action", 
                     "group"),
                   names(Call), nomatch=0)
    temp <- Call[c(1, index)]
    temp[[1L]] <-  quote(stats::model.frame)
    special <- c("strata", "cluster")
    temp$formula <- if(missing(data)) terms(formula, special)
                    else              terms(formula, special, data=data)
    mf <- eval(temp, parent.frame())  # model frame
    if (nrow(mf) ==0) stop("No (non-missing) observations")
    Terms <- terms(mf)

    Y <- model.response(mf)
    if (!inherits(Y, "Surv")) {
        if (is.numeric(Y) && is.vector(Y))  Y <- Surv(Y)
        else stop("left hand side of the formula  must be a numeric vector or a surival")
    }
    n <- nrow(Y)
    if (timefix) Y <- aeqSurv(Y)
    
    wt <- model.weights(mf)
    offset<- attr(Terms, "offset")
    if (length(offset)>0) stop("Offset terms not allowed")

    stemp <- untangle.specials(Terms, "strata")
    if (length(stemp$vars)) {
	if (length(stemp$vars)==1) strat <- m[[stemp$vars]]
	else strat <- strata(m[,stemp$vars], shortlabel=TRUE)
        Terms <- Terms[-stemp$terms]
    }
    else strat <- NULL
    
    group <- model.extract(mf, "(group)")
    cluster<- attr(Terms, "specials")$cluster
    if (length(cluster)) {
        tempc <- untangle.specials(Terms, 'cluster', 1:10)
        ord <- attr(Terms, 'order')[tempc$terms]
        if (any(ord>1)) stop ("Cluster can not be used in an interaction")
        cluster <- strata(mf[,tempc$vars], shortlabel=TRUE)  #allow multiples
        Terms <- Terms[-tempc$terms]  # toss it away
    }
    else if (length(group)==0) group <- seq.int(1,n)
    if (length(group)) cluster <- group
                                            
    x <- model.matrix(Terms, mf)[,-1, drop=FALSE]  #remove the intercept
    if (ncol(x) > 1) stop("Only one predictor variable allowed")

    if (!is.null(ymin) & (length(ymin)> 1 || !is.numeric(ymin)))
        stop("ymin must be a single number")
    if (!is.null(ymax) & (length(ymax)> 1 || !is.numeric(ymax)))
        stop("ymax must be a single number")
    
    fit <- concordance.fit(Y, x, strat, wt, ymin, ymax, timewt, cluster,
                           influence, residuals)
 
    if (!is.logical(reverse)) 
        stop ("the reverse argument must be TRUE/FALSE")
    if (reverse) {
        fit$concordance <- 1- fit$concordance
        if (is.matrix(fit$influence)) fit$influence <- fit$influence[,c(2,1,3,4)]
        if (is.matrix(cfit$count)) fit$count <- fit$count[, c(2,1,3,4)]
        else fit$count <- fit$count[c(2,1,3,4)]
    }

    na.action <- attr(mf, "na.action")
    if (length(na.action)) fit$na.action <- na.action
    fit$call <- Call

    class(fit) <- 'concordance'
    fit
}

print.concordance <- function(x, ...) {
    if(!is.null(cl <- x$call)) {
        cat("Call:\n")
        dput(cl)
        cat("\n")
        }
    omit <- x$na.action
    if(length(omit))
        cat("n=", x$n, " (", naprint(omit), ")\n", sep = "")
    else cat("n=", x$n, "\n")
    cat("Concordance= ", format(x$concordance), " se= ", format(sqrt(x$var[1])),
        '\n', sep='')

    if (!is.matrix(x$count) || nrow(fit$count < 11)) print(x$count)
    invisible(x)
    }

<<concordancefit>>

<<btree>>
@ 

The concordance.fit function is broken out separately, since it is called
by the \code{coxph} routine.  This also allows other packages to build on
it.
If $y$ is not a survival quantity, then all of the options for the
\code{timewt} parameter lead to the same result.

<<concordancefit>>=
concordance.fit <- function(y, x, strata, weight, ymin, ymax, timewt,
                            group, influence=0, residuals=TRUE) {
    # The coxph program may occassionally fail, and this will kill the C
    #  routine below.  So check for it.
    if (any(is.na(x)) || any(is.na(y))) return(NULL)

    # these should only occur if something outside survival calls this routine
    n <- length(y)
    if (length(x) != n) stop("x and y are not the same length")
    if (missing(strata) || length(strata)==0) strata <- rep(1L, n)
    if (length(strata) != n)
        stop("y and strata are not the same length")
    if (missing(weight) || length(weight)==0) weight <- rep(1.0, n)
    else if (length(weight) != n) stop("y and weight are not the same length")
    if (!is.Surv(y)) {
        y <- Surv(y)
        timewt <- 'n'
    }

    type <- attr(y, "type")
    if (type %in% c("left", "interval"))
        stop("left or interval censored data is not supported")
    if (type %in% c("mright", "mcounting"))
        stop("multiple state survival is not supported")
   
    # This routine is called once per stratum
    docount <- function(stime, risk, wts, group, timeopt= 'n') {
        n <- length(risk)
        # this next line is mostly invoked in stratified logistic, where
        #  only 1 event per stratum occurs.  All time weightings are the same
        if (sum(stime[,ncol(stime)]) <2) timewt <- 'n'
        
        sfit <- survfit(stime~1, weight=wts, se.fit=FALSE)
        etime <- sfit$time[sfit$n.event > 0]
        esurv <- sfit$surv[sfit$n.event > 0]
        
        if (timeopt %in% c("S/G", "n/G", "n/G2")) {
            temp <- stime
            temp[,ncol(temp)] <- 1- temp[,ncol(temp)] # switch event/censor
            gfit <- survfit(temp~1, weight=wts, se.fit=FALSE)
            # gfit is not a survfit object
            gsurv <- approx(c(0, gfit$time), c(1, gfit$surv), etime,
                            method="constant", f=0, rule=2)$y
        }

        npair <- (sfit$n.risk- sfit$n.event)[sfit$n.event>0]
        timewt <- switch(timeopt,
            "S" =  sum(wt)*esurv/npair,
            "S/G" = sum(wts)* esurv/ (gsurv* npair),
            "n" =   rep(1.0, length(npair)),
            "n/G" = 1/gsurv,
            "n/G2"= 1/gsurv^2)

        # order the data: reverse time, censors before deaths
        if (ncol(stime)==2) { 
            sort.stop <- order(-y[,1], y[,2]) -1L 
        } else {
            sort.stop  <- order(-stime[,2], stime[,3]) -1L   #order by endpoint
            sort.start <- order(-stime[,1]) -1L       
        }
 
        # match each prediction score to the unique set of scores
        # (to deal with ties)
        utemp <- match(risk, sort(unique(risk)))
        bindex <- btree(max(utemp))[utemp]
        
        storage.mode(stime) <- "double"  # just in case y is integer
        if (ncol(y) ==2)
            fit <- .Call(Cconcordance3, stime, bindex, wts, rev(timewt), 
                         sort.stop, residuals)
        else fit <- .Call(Cconcordance4, stime, bindex, wts, rev(timewt), 
                          sort.start, sort.stop, residuals)
        # The C routine gives back an influence matrix which has columns for
        #  concordant, discordant, tied on x but not y, tied on y, and tied on
        #  both x and y.  Subtract col 5 from col 4 to get tied on y but not x.
        fit$count[4] <- fit$count[4]- fit$count[5]
        fit$influence[,4] <- fit$influence[,4] - fit$influence[,5]
        dimnames(fit$influence) <- list(NULL, 
                   c("concordant", "discordant", "tied.x", "tied.y", "tied.xy"))
        if (residuals) dimnames(fit$resid) <- list(rep(sfit$time, sfit$n.event),
                                    c("rank", "npair", "wt", "var"))
        fit
    }
    
    dfun <- function(count, imat, v1, v2) {
        # Compute a statistic and its variance
        denom <- sum(count*v2)
        statistic <-  sum(count*v1)/denom
        dfbeta <- (imat %*% v1)/denom - (imat %*% v2)*statistic/denom
        list(x=c(value=statistic, variance= sum(dfbeta^2)), dfbeta=dfbeta)
    }
        
    if (missing(strata) || length(strata)==0 || all(strata==strata[1])) {
        is.strata <-FALSE
        fit <- docount(y, x, weight, timewt)
        count2 <- fit$count[1:5]
        vcox <- fit$count[6]
        fit$count <- fit$count[1:5]
        imat <- fit$influence
        if (residuals) resid <- fit$resid
    } else {
        is.strata <- TRUE
        strata <- as.factor(strata)
        ustrat <- levels(strata)[table(strata) >0]  #some strata may have 0 obs
        tfit <- lapply(ustrat, function(i) {
            keep <- which(strata== i)
            docount(y[keep,,drop=F], x[keep], weight[keep], timewt)
        })
        count2 <- colSums(fit$count)[,1:5]
        vcox <- sum(fit$count[,6])
        fit$count <- fit$count[,1:5]
        imat <- do.call("rbind", lapply(tfit, function(x) x$influence))
        # put it back into data order
        index <- match(1:n, (1:n)[order(strata)])
        imat <- imat[index,]
        if (residuals)
            resid <- do.call("rbind", lapply(tfit, function(x) x$resid))
    }

    npair <- sum(count2[1:3])
    somer <- (count2[1] - count2[2])/npair
    dfbeta <- weight*((imat[,1]- imat[,2])/npair -
                      (somer/npair)* rowSums(imat[,1:3]))
    if (!missing(group) && length(group)>0) {
        dfbeta <- tapply(dfbeta, group, sum)
        dfbeta <- ifelse(is.na(dfbeta),0, dfbeta)  # if group is a factor
    }
    var.somer <- sum(dfbeta^2)
    rval <- list(concordance = (somer+1)/2, count=fit$count, n=n,
                 var = c(IJ= var.somer/4, PH= vcox/(4*npair^2)))

    if (influence == 1) rval$dfbeta= dfbeta
    else if (influence ==2) {
        if (!missing(group) && length(group) > 0) 
            rval$influence <- rowsum(weight*imat, group)
        else rval$influence <- imat
    }
        
    if (residuals) rval$residuals <- resid
    rval
}
@ 

The C routine returns an influence matrix with one row per subject $i$, 
and columns giving the partial with respect to $w_i$ for the number of
concordant, discordant, tied on $x$ and ties on $y$ pairs.
Somers' $d$ is $(C-D)/m$ where $m= C + D + T$ is the total number of %'
comparable pairs, which does not count the tied-on-y column.
For any given subject or group $k$ (for grouped jackknife) the
IJ estimate of the variance is
\begin{align*}
  V &\ \sum_k  \left(\frac{\partial d}{\parial w_k}\right)^2 \\
  \frac{\partial d}{\parial w_k} &= 
      \frac{1}{m} \left[\frac{\partial{C-D}}{\partial w_k} -
        d \frac{\partial C+D+T}{\partial w_k} \right] \\
\end{equation*}

The C code looks a lot like a Cox model: walk forward through time, keep
track of the risk sets, and add something to the totals at each death.
What needs to be summed is the rank of the event subject's $x$ value, as
compared to the value for all others at risk at this time point.
For notational simplicity let $Y_j(t_i)$ be an indicator that subject $j$
is at risk at event time $t_i$, and $Y^*_j(t_i)$ the more restrictive one that
subject $j$ is both at risk and not a tied event time.
The values we want at time $t_i$ are
\begin{align}
  C_i &= v_i \delta_i w_i \sum_j w_j Y^*_j(t_i) \left[I(x_i < x_j) \right]
    \label{C} \\
  D_i &= v_i \delta_i w_i \sum_j w_j Y^*_j(t_i) \left[I(x_i > x_j)\right] 
     \label{D} \\
  T_i &= v_i \delta_i w_i \sum_j w_j Y^*_j(t_i) \left[I(x_i = x_j) \right]
     \label{T}  \\
\end{align} 

In the above $v$ is an optional time weight, which we will discuss later.
The normal concordance definition has $v=1$.
$C$, $D$, and $T$ are the number of concordant, discordant, and tied
pairs, respectively,
and $m= C+D+T$ will be the total number of concordant pairs.
Somers' $d$ is $(C-D)/m$ and the concordance is $(d+1)/2 = (C + T/2)/m$.

The primary compuational question is how to do this efficiently, i.e., better
than a naive algorithm that loops across all $n(n-1)/2$ 
possible pairs.
There are two key ideas.
\begin{enumerate}
\item Rearrange the counting so that we do it by death times.
  For each death we count the number of other subjects in the risk set whose
  score is higher, lower, or tied and add it into the totals.
  This neatly solves the question of time-dependent covariates.
\item Counting the number with higher, lower, and tied $x$ can be done in 
   $O(\log_2 n)$ time if the $x$ data is kept in a binary tree.
\end{enumerate}

\begin{figure}
  \myfig{balance}
  \caption{A balanced tree of 13 nodes.}
  \label{treefig}
\end{figure}

Figure  \ref{treefig} shows a balanced binary tree containing  
13 risk scores.  For each node the left child and all its descendants
have a smaller value than the parent, the right child and all its
descendents have a larger value.
Each node in figure \ref{treefig} is also annotated with the total weight
of observations in that node and the weight for itself plus all its children 
(not shown on graph).  
Assume that the tree shown represents all of the subjects still alive at the
time a particular subject ``Smith'' expires, and that Smith has the risk score
of 19 in the tree.
The concordant pairs are those with a risk score $>19$, i.e., both $\hat y=x$
and $y$ are larger, discordant are $<19$, and we have no ties.
The totals can be found by
\begin{enumerate}
  \item Initialize the counts for discordant, concordant and tied to the
    values from the left children, right children, and ties at this node,
    respectively, which will be $(C,D,T) = (1,1,0)$.
  \item Walk up the tree, and at each step add the (parent + left child) or
    (parent + right child) to either D or C, depending on what part of the
    tree has not yet been totaled.  
    At the next node (8) $D= D+4$, and at the top node $C=C + 6$.
\end{enumerate}

There are 5 concordant and 7 discordant pairs.
This takes a little less than $\log_2(n)$ steps on average, as compared to an
average of $n/2$ for the naive method.  The difference can matter when $n$ is
large since this traversal must be done for each event.

The classic way to store trees is as a linked list.  There are several 
algorithms for adding and subtracting nodes from a tree while maintaining
the balance (red-black trees, AA trees, etc) but we take a different 
approach.  Since we need to deal with case weights in the model and we
know all the risk score at the outset, the full set of risk scores is
organised into a tree at the beginning, updating the sums of weights at
each node as observations are added or removed from the risk set.

If we internally index the nodes of the tree as 1 for the top, 
2--3 for the next 
horizontal row, 4--7 for the next, \ldots then the parent-child 
traversal becomes particularly easy.
The parent of node $i$ is $i/2$ (integer arithmetic) and the children of
node $i$ are $2i$ and $2i +1$.  In C code the indices start at 0 of course.
The following bit of code arranges data into such a tree.
<<btree>>=
btree <- function(n) {
   tfun <- function(n, id, power) {
       if (n==1L) id
       else if (n==2L) c(2L *id + 1L, id)
       else if (n==3L) c(2L*id + 1L, id, 2L*id +2L)
       else {
           nleft <- if (n== power*2L) power  else min(power-1L, n-power%/%2L)
           c(tfun(nleft, 2L *id + 1L, power%/%2), id,
             tfun(n-(nleft+1L), 2L*id +2L, power%/%2))
       }
   }
   tfun(as.integer(n), 0L, as.integer(2^(floor(logb(n-1,2)))))
}
@ 

Referring again to figure \ref{treefig}, \code{btree(13)} yields the vector
\code{7  3  8  1  9  4 10  0 11  5 12  2  6}
meaning that the smallest element
will be in position 8 of the tree, the next smallest in position 2, etc,
and using indexing that starts at 0 since the results will be passed to a C
routine.
The code just above takes care to do all arithmetic as integer.  
This actually made almost no difference in the compute time, but it was an
interesting exercise to find that out.

The next question is how to compute a variance for the result.
One approach is to compute an infinitesimal jackknife (IJ) estimate,
for which we need derivatives with respect to the weights.
Looking back at equation \eqref{C} we have
\begin{align}
  C  &= \sum_i  w_i \delta_i \sum_j Y^*_j(t_i) w_j I(x_i < x_j) 
  \nonumber\\
% \frac{\partial C}{\partial w_k} &= 
%    (v_k/m_k)\delta_k \sum_j Y^*_{j}(t_k) I(x_k < x_j) +
%    \sum_i (v_i/m_i) w_i Y^*_k(t_i) I(x_i < x_k) \label{partialC}
\end{align}
A given subject's weight appears multiple times, once when they are an
event ($w_i \delta_i)$, and then as part of the risk set for other's
events.  I avoided this for some time because it looked like an $O(nd)$
process to separately update each subject's influence for each risk set
they inhabit, but David Watson pointed out a path forward.
The solution is to keep two trees.  
Tree 1 contains all of the subjects at risk.  We traverse it when each subject
is added in, updating the tree, 
and traverse it again at each death, pulling off values to update our sums. 
The second tree holds only the deaths and is updated at each death;
it is read out twice per subject,
once just after they enter the risk set and once when they leave.

The basic algorithm is to move through an outer and inner loop.  The
outer loop moves across unique times, the inner for all obs that
share a death time.  We progress from largest to smallest time.
Dealing with tied deaths is  a bit subtle.
\begin{itemize}
  \item All of the tied deaths need to be added to the event tree before
    subtracting the tree values from the ``initial'' influence matrix, since
    none of the tied subjects are in the comparison set for each other.
  \item Changes to the overall concordance/discordance counts need to be done
    for all the ties before adding them into the main tree, for the same reason.
  \item The Cox model variance just below has to be added up sequentially,
    one terms after each addition to the main tree.
\end{itemize}
Thus the inner loop must be repeated at least twice.

A second viewpoint treats the data as a Cox model.
Create zero-centered scores for all subjects in the risk set:
\begin{align}
  z_i(t) &= \sum_{j \in R(t)} w_j \sign(x_i - x_j) \nonumber
  D-C = \sum_i \delta_i z_i(t_i)              \label{zcord}
\end{align}
At any event time $\sum w_i z_i =0$.  
Equation \eqref{zcord} is the score equation
for a Cox model with time-dependent covariate $z$.
When two subjects have an event at the same time, this formulation treats
each of them as being in the other's risk set whereas the concordance
treats them as incomparable --- how can they be the same?
The trick is that $D-C$ does not change: the tied pairs add equally to
$D$ and $C$.
Under the null hypothesis that the risk score is not related to outcome,
each term in \eqref{zcord} is a random selection from the $z$ scores in
the risk set, and the variance of the addition is the variance of $z$,
the sum of these over deaths is the Cox model information matrix,
which is also the variance of the score statistic.
The mean of $z$ is always zero, so we need to keep track of 
$\sum w_i z^2$. 

How can we do this efficiently?  First note that $z_i$ can be written
as sum(weights for smaller x) - sum(weights for larger x), and in fact the
weighted mean for any slice of $x$, $a < x < b$, is exactly the
same: mean = sum(weights for x values below the range) - 
 sum(weights above the range).
The second trick is to use an ANOVA decomposition of the variance of $z$ into
within-slice and between-slice sums of squares, where the 3 slices are the
$z$ scores at a given $x$ value (node of the tree), weights for score below that
cutpoint, and above.
Assume that a new observation $k$ has just been added to the tree.  
This will add $w_k$ to all the $z$ values above, and to the weighted mean of
all those above, $-w_k$ to the values and means below, and 0 to the values and
means of any tied observations.  Thus none of the current `within'
SS change.  
Let $s_a$, $s_b$ and $s_0$ be the current sum of weights above, below, and
at the node of the tree.  The mean for the above group was $(s_b + s_0)$ with
between SS contribution of $s_a (s_b + s_0)^2$.  The below mean was 
$-(s_a + s_0)$  with between SS contribution of $s_b(s_a + s_0)^2$.
The change to the between SS from adding the new subject is
$$
s_a\left( (s_b+s_0 + w_k)^2 - (s_b + s_0)^2 \right) =
s_a (2w_k (s_b + s_0) + w_k^2)
$$
while the change in between SS for the below group 
is $s_b(2w_k(s_a + s_0) + w_k^2)$, and there is no change for the 
prior observations in the middle group.
Last we add $w_kz_k^2 = w_k(s_b- s_a)^2$ to the sum for the new observation.
Putting all this together the change is
$$
  w_k \left(s_a (w_k + (s_b + s_c)) + s_b(w_k + (s_a + s_c)) + (s_a-s_b)^2 \right)
$$

We can now define the C-routine that does the bulk of the work.
First we give the outline shell of the code and then discuss the
parts one by one.  This routine  is for ordinary survival data, and
will be called once per stratum.
Input variables are
\begin{description}
  \item[n] the number of observations
  \item[y] matrix containing the time and status, data is sorted by descending 
    time, with censorings precedint deaths.
  \item[x] the tree node at which this observation's risk score resides  %'
  \item[wt] case weight for the observation
  \item[group] which IJ group each observation will be counted into
\end{description}
The routine will return list with three components:
\begin{itemize}
  \item count, a vector containing the weighted number of concordant, 
    discordant, tied on $x$ but not $y$, and tied on y pairs.  
    The weight for a pair is $w_iw_j$.
  \item resid, a three column matrix with one row per event, containing the 
    score residual at that event, its variance, and the sum of weights.
    The score residual is
    a rescaled $z_i$ so as to lie between 0 and 1: $(1+ z/\sum(w))/2$.
    The concordance is then a weighted sum of the residuals.
  \item influence, a matrix with one row per observation and 4 columns, giving
    that observation's first derivative with respect to the count vector.
\end{itemize}    

<<concordance3>>=
#include "survS.h"

<<walkup>>
    
    SEXP concordance3(SEXP y, SEXP x2, SEXP wt2, SEXP timewt2, 
                      SEXP sortstop, SEXP doresid2) {
    int i, j, k, ii, jj;
    int n, ntree, nevent;
    double *time, *status;

    /* sum of weights for a node (nwt), sum of weights for the node and
    **  all of its children (twt), then the same again for the subset of
    **  deaths
    */
    double *nwt, *twt, *dnwt, *dtwt;
    double z2;  /* sum of z^2 values */    
        
    int ndeath;   /* total number of deaths at this point */    
    int utime;    /* number of unique event times seen so far */
    double dwt;   /* weighted number of deaths at this point */
    double wsum[3]; /* the sum of weights that are > current, <, or equal  */
    double temp, adjtimewt;  /* the second accounts for npair and timewt*/

    SEXP rlist, count2, imat2, resid2;
    double *count, *imat[5], *resid[4];
    double *wt, *timewt;
    int    *x, *sort2;
    int doresid;
    static const char *outnames1[]={"count", "influence", "resid", ""},
	              *outnames2[]={"count", "influence", ""};
      
    n = nrows(y);
    doresid = asLogical(doresid2);
    x = INTEGER(x2);
    wt = REAL(wt2);
    timewt = REAL(timewt2);
    sort2 = INTEGER(sortstop);
    time = REAL(y);
    status = time + n;
   
    /* if there are tied predictors, the total size of the tree will be < n */
    ntree =0; nevent =0;
    for (i=0; i<n; i++) {
	if (x[i] >= ntree) ntree = x[i] +1;  
        nevent += status[i];
    }
        
    nwt = (double *) R_alloc(4*ntree, sizeof(double));
    twt = nwt + ntree;
    dnwt = twt + ntree;
    dtwt = dnwt + ntree;
    
    for (i=0; i< 4*ntree; i++) nwt[i] =0.0;
    
    if (doresid) PROTECT(rlist = mkNamed(VECSXP, outnames1));
    else  PROTECT(rlist = mkNamed(VECSXP, outnames2));
    count2 = SET_VECTOR_ELT(rlist, 0, allocVector(REALSXP, 6));
    count = REAL(count2); 
    for (i=0; i<6; i++) count[i]=0.0;
    imat2 = SET_VECTOR_ELT(rlist, 1, allocMatrix(REALSXP, n, 5));
    for (i=0; i<5; i++) {
        imat[i] = REAL(imat2) + i*n;
        for (j=0; j<n; j++) imat[i][j] =0;
    }
    if (doresid==1) {
        resid2 = SET_VECTOR_ELT(rlist, 2, allocMatrix(REALSXP, nevent, 4));
        for (i=0; i<4; i++) resid[i] = REAL(resid2) + i*nevent;
        }
    
    <<concordance3-work>>
        
    UNPROTECT(1);
    return(rlist);
}
@ 

The key part of our computation is to update the vectors of weights.
We don't actually pass the risk score values $r$ into the routine,   %'
it is enough for each observation to point to the appropriate tree
node.
The tree contains the weights for everyone whose survival is larger
than the time currently under review, so starts with all weights
equal to zero.  
For any pair of observations $i,j$ we need to add [[wt[i]*wt[j]]]
to the appropriate count, $w_j$ to subject $i$'s row of the leverage
matrix and $w_i$ to subject $j$'s row.  We use two trees to do this 
efficiently, one with all the observations to date, one with the events to
date.
Starting at the largest time (which is sorted last), walk through the tree.
\begin{itemize}
  \item If the current observation is a censoring time, in order:
    \begin{itemize}
      \item Subtract event tree information from the influence matrix
      \item Update the Cox variance
      \item Add them into the main tree
    \end{itemize}
  \item If the current observation is a death, care for all deaths tied
    at this time point.  Each pass covers all the deaths.
    \begin{itemize}
      \item Pass 1: subtract the current death tree counts, tied on x and
        not tied on x, from the tied.x and tied.xy totals and influence.
        Count up total number of deaths.
      \item Pass 2: In any order
        \begin{itemize}
          \item Update the tied.y count
          \item Count concordant, discordant, tied.x counts, both total
            and for the observation's influence
          \item Add the subject to the event tree
          \item Compute the first 3 columns of the residuals.
        \end{itemize}
      \item Pass 3: 
        \begin{itemize}
          \item Subtract the event tree information from the influence matrix
          \item Add the tied.y part of the influence for each obs
          \item Increment the Cox variance
          \item Add the subject into the main tree
        \end{itemize}
    \end{itemize}

    \item When all the subjects have been added to the tree, then add the final
      death tree's data for to the influence matrix.  
\end{itemize}

Part of the nuisance above is that for tied.xy counts 
I need to compare a
readout (walkup routine) of the
death tree before adding a batch of events, to the same readout just after
adding all of them; while for concordant, discordant, and tied.x there are three
readouts: the total tree before any additions, the death tree after the 
addition of the tied events, and the death tree at the very end.
Increments to the Cox variance occur just before each addition to the total 
tree, and are saved out after each batch of events.
It is easy to keep a count of tied.y = tied on y, whether x is tied or not,
but (tied on y, not on x) is harder.  We fix this up at the end in the
R code.

<<concordance3-work>>=
z2 =0; utime=0;
for (i=0; i<n;) {
    ii = sort2[i];  
    if (status[ii]==0) { /* censored, simply add them into the tree */
	/* Initialize the influence */
	walkup(dnwt, dtwt, x[ii], wsum, ntree);
	imat[0][ii] -= wsum[1];
	imat[1][ii] -= wsum[0];
	imat[2][ii] -= wsum[2];
	
	/* Cox variance */
        walkup(nwt, twt, x[ii], wsum, ntree);
	z2 += wt[ii]*(wsum[0]*(wt[ii] + 2*(wsum[1] + wsum[2])) +
                      wsum[1]*(wt[ii] + 2*(wsum[0] + wsum[2])) +
                      (wsum[0]-wsum[1])*(wsum[0]-wsum[1]));
	/* add them to the tree */
        addin(nwt, twt, x[ii], wt[ii]);
        i++;
    }
    else {  /* process all tied deaths at this point */
	ndeath=0; dwt=0; 
        adjtimewt = timewt[utime++];

	/* pass 1 */
	for (j=i; j<n && time[sort2[j]]==time[ii]; j++) {
	    jj = sort2[j];
	    ndeath++; 
	    imat[4][jj] -= dnwt[x[jj]];  /* tied on x, prior to additions*/
            count[4] -=  dnwt[x[jj]]/2;
	}

        /* pass 2 */   
        for (j=i; j< (i+ndeath); j++) {
	    jj = sort2[j];
	    count[3] += wt[jj] * dwt;  /* update total tied on y */
            dwt += wt[jj];   /* count of deaths and sum of wts */

	    /* Count concordant, discordant, etc. */
	    walkup(nwt, twt, x[jj], wsum, ntree);
	    for (k=0; k<3; k++) {
		count[k] += wt[jj]* wsum[k] * adjtimewt;
		imat[k][jj] += wsum[k]*adjtimewt;
	    }

	    /* add to the event tree */
	    addin(dnwt, dtwt, x[jj], adjtimewt*wt[jj]);  /* weighted deaths */

	    /* first part of residuals */
	    if (doresid) {
		nevent--;
		resid[0][nevent] = (wsum[0] - wsum[1])/twt[0]; /* -1 to 1 */
		resid[1][nevent] = twt[0] * adjtimewt;
		resid[2][nevent] = wt[jj];
	    }
	}

	/* pass 3 */
	for (j=i; j< (i+ndeath); j++) {
	    jj = sort2[j];
	    /* Update influence */
	    walkup(dnwt, dtwt, x[jj], wsum, ntree);
	    imat[0][jj] -= wsum[1];
	    imat[1][jj] -= wsum[0];
	    imat[2][jj] -= wsum[2];  /* tied.x */
	    imat[3][jj] += (dwt- wt[jj])* adjtimewt;
            imat[4][jj] += (wsum[2] - wt[jj] * adjtimewt); 
	    count[4] += wt[jj] *(wsum[2] - wt[jj] * adjtimewt) /2;

	    /* increment Cox var and add obs into the tree */
            walkup(nwt, twt, x[j], wsum, ntree);
	    z2 += wt[jj]*(wsum[0]*(wt[jj] + 2*(wsum[1] + wsum[2])) +
			  wsum[1]*(wt[jj] + 2*(wsum[0] + wsum[2])) +
			  (wsum[0]-wsum[1])*(wsum[0]-wsum[1]));

	    addin(nwt, twt, x[jj], wt[jj]); 
	}
        count[5] += dwt * adjtimewt* z2/twt[0]; /* weighted var in risk set*/
	i += ndeath;

	if (doresid) { /*Add the last part of the residuals */
	    temp = twt[0]*twt[0]*twt[0];
	    for (j=0; j<ndeath; j++)
		resid[3][nevent+j] = z2/temp;
	}
    }
}

/* 
** Now finish off the influence for each observation 
**  Since times flip (looking backwards) the wsum contributions flip too
*/
for (i=0; i<n; i++) {
    ii = sort2[i];
    walkup(dnwt, dtwt, x[ii], wsum, ntree);
    imat[0][ii] += wsum[1];
    imat[1][ii] += wsum[0];
    imat[2][ii] += wsum[2];
}
@ 

<<walkup>>=
void walkup(double *nwt, double* twt, int index, double sums[3], int ntree) {
    int i, j, parent;

    for (i=0; i<3; i++) sums[i] = 0.0;
    sums[2] = nwt[index];   /* tied on x */
    
    j = 2*index +2;  /* right child */
    if (j < ntree) sums[0] += twt[j];
    if (j <=ntree) sums[1]+= twt[j-1]; /*left child */

    while(index > 0) { /* for as long as I have a parent... */
        parent = (index-1)/2;
        if (index%2 == 1) sums[0] += twt[parent] - twt[index]; /* left child */
	else sums[1] += twt[parent] - twt[index]; /* I am a right child */
	index = parent;
    }
}

void addin(double *nwt, double *twt, int index, double wt) {
    nwt[index] += wt;
    while (index >0) {
	twt[index] += wt;
	index = (index-1)/2;
    }
    twt[0] += wt;
}
@ 

The code for [start, stop) data is quite similar.  
As in the agreg routines there are two sort indices, the first indexes
the data by stop time, longest to earliest, and the second by start time. 
The [[y]] variable now has three columns.
<<concordance3>>= 
SEXP concordance4(SEXP y, SEXP x2, SEXP wt2, SEXP timewt2, 
                  SEXP sortstop, SEXP sortstart, SEXP group2) {
    int i, j, k, ii, jj, i2;
    int n, ngroup, ntree, nevent;
    double *time1, *time2, *status;

    /* sum of weights for a node (nwt), sum of weights for the node and
    **  all of its children (twt), then the same again for the subset of
    **  deaths
    */
    double *nwt, *twt, *dnwt, *dtwt;
    double z2;  /* sum of z^2 values */    
 
    int ndeath;   /* total number of deaths at this point */    
    double dwt;   /* weighted number of deaths at this point */
    double wsum[3]; /* the sum of weights that are > current, <, or equal  */
    double temp, dtime;

    SEXP rlist, count2, imat2, resid2;
    double *count, *imat[4], *resid[4];
    double *wt, *timewt;
    int    *x, *group, *sort1, *sort2;
    static const char *outnames[]={"count", "resid", "influence", ""};
     
    n = nrows(y);
    x = INTEGER(x2);
    wt = REAL(wt2);
    timewt = REAL(timewt2);
    group = INTEGER(group2);
    sort2 = INTEGER(sortstop);
    sort1 = INTEGER(sortstart);
    
    time1 = REAL(y);
    time2 = time1 + n;
    status= time2 + n;

    /* 
    ** if there are tied predictors, the total size of the tree will be < n 
    ** influence matrix is 4 by ngroup, ngroup <n with grouped jackknife
    ** residual is 4 x number of events
    */
    ntree =0; nevent =0; ngroup =0;
    for (i=0; i<n; i++) {
        if (x[i] >= ntree) ntree = x[i] +1;  
        if (group[i] >= ngroup) ngroup = group[i] +1; 
        nevent += status[i];
    }
        
    nwt = (double *) R_alloc(4*ntree, sizeof(double));
    twt = nwt + ntree;
    dnwt = twt + ntree;
    dtwt = dnwt + ntree;
    
    for (i=0; i< 4*ntree; i++) nwt[i] =0.0; /* zero the trees */
    
    PROTECT(rlist = mkNamed(VECSXP, outnames));
    count2 = SET_VECTOR_ELT(rlist, 0, allocVector(REALSXP, 5));
    count = REAL(count2); 
    for (i=0; i<5; i++) count[i]=0.0;
    resid2 = SET_VECTOR_ELT(rlist, 1, allocMatrix(REALSXP, nevent, 4));
    for (i=0; i<4; i++) resid[i] = REAL(resid2) + i*nevent;
    imat2 = SET_VECTOR_ELT(rlist, 2, allocMatrix(REALSXP, ngroup, 4));
    for (i=0; i<4; i++) {
        imat[i] = REAL(imat2) + i*ngroup;
        for (j=0; j<ngroup; j++) imat[i][j] =0;
    }
 
    <<concordance4-work>>
        
    UNPROTECT(1);
    return(rlist);
}
@ 

The processing changes in 2 ways
\begin{itemize}
  \item Use [[sort1[i]]] instead of [[i]] as the subscript for all the data
    vectors (time, status, x, wt, group).
    (The sort vectors go backwards in time.)
    This happens enough that we use temporary variables \code{ii} and \code{jj}}
    to avoid the double subscript.
  \item As we move from the longest time to the shortest observations are added
    into the tree of weights whenever we encounter their stop time. 
    This is just as before.  Weights now also need to be removed from the 
    tree whenever we encounter an observation's start time.              %'
    It is convenient ``catch up'' on this second task whenever we encounter 
    a death.
\end{itemize}

<<concordance4-work>>=
z2 =0; i2=0; 
for (i=0; i<n;) {
    ii = sort1[i];
    /* Initialize the influence */
    walkup(dnwt, dtwt, x[i], wsum, ntree);
    imat[0][group[ii]] -= wsum[1];
    imat[1][group[ii]] -= wsum[0];
    imat[2][group[ii]] -= wsum[2];

    if (status[ii]==0) { /* censored, add new subject into the tree */
        walkup(nwt, twt, x[ii], wsum, ntree);
	z2 += wt[ii]*(wsum[0]*(wt[i] + 2*(wsum[1] + wsum[2])) +
		     wsum[1]*(wt[i] + 2*(wsum[0] + wsum[2])) +
		     (wsum[0]-wsum[1])*(wsum[0]-wsum[1]));
        addin(nwt, twt, x[ii], wt[ii]);
        i++;
    }
    else { /* subject is a death */
	dtime = time2[ii];

	/* remove those who are no longer at risk */
	for (; time1[sort2[i2]] >= dtime; i2++) {
	    jj = sort2[i2];
	    walkup(dnwt, dtwt, x[jj], wsum, ntree);
	    imat[0][group[jj]] += wsum[1];
	    imat[1][group[jj]] += wsum[0];
	    imat[2][group[jj]] += wsum[2];
	    addin(nwt, twt, x[jj], -wt[jj]);
	}

        /* update the counts and residuals */
	ndeath=0; dwt=0; 
        for (j=i; j<n && time2[sort2[j]]== dtime; j++) {
            jj = sort1[j];
	    count[3] += wt[jj] * dwt;	    
	    ndeath++; dwt += wt[jj];   /* count of deaths and sum of wts */

	    walkup(nwt, twt, x[jj], wsum, ntree);
	    nevent--;  /* backwards on the survival curve */
	    temp = timewt[nevent]/twt[0]; /* adjusted time weight */
	    for (k=0; k<3; k++) {
		count[k] += wt[jj]* wsum[k] *temp;
		imat[k][group[jj]] += wsum[k]*temp;
	    }
	    imat[2][group[jj]] -= wt[jj]*temp;
	    addin(dnwt, dtwt, x[jj], temp*wt[jj]);  /* add weighted deaths */

	    /* residuals */
	    z2 += wt[jj]*(wsum[0]*(wt[jj] + 2*(wsum[1] + wsum[2])) +
                          wsum[1]*(wt[jj] + 2*(wsum[0] + wsum[2])) +
                          (wsum[0]-wsum[1])*(wsum[0]-wsum[1]));
	    resid[0][nevent] = (wsum[0] - wsum[1])/twt[0]; /* -1 to 1 */
	    resid[1][nevent] = twt[0];
	    resid[2][nevent] = wt[jj];
	}

	/* 
        ** Add deaths into the tree, tied time information into the
        **  influence, and Cox variance to the residuals.  All tied resids
        **  get the same variance.
        */
        temp = twt[0] + dwt;  /* total number at risk */ 
        count[4] += dwt * z2/temp;
 	for (j=0; j< ndeath; j++) {
            ii = sort2[i];
            resid[3][nevent-j] = z2/(temp*temp*temp);
            imat[3][group[ii]] += dwt - wt[ii];
	    addin(nwt, twt, x[ii], wt[ii]);
	    i++;
        }
    }
}

/* 
** Now finish off the influence for those not yet removed
**  Since times flip (looking backwards) the wsum contributions flip also
*/
for (; i2<2; i2++) {
    ii = sort2[i2];
    walkup(dnwt, dtwt, x[ii], wsum, ntree);
    imat[0][group[ii]] += wsum[1];
    imat[1][group[ii]] += wsum[0];
    imat[2][group[ii]] += wsum[2];
}
@ 
