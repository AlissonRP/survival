\subsection{Kaplan-Meier}
This routine has been rewritten more times than any other in the package,
as we trade off simplicty of the code with execution speed.  
This version does all of the oranizational work in S and calls a C
routine for each separate curve. 
The first code did everything in C but was too hard to maintain and the most
recent function did nearly everything in S. 
Introduction of robust variance
prompted a movement of more of the code into C since that calculation
is computationally intensive.

<<survfitKM>>= 
survfitKM <- function(x, y, weights=rep(1.0,length(x)), 
                      stype=1, ctype=1,
                      se.fit=TRUE,
                      conf.int= .95,
                      conf.type=c('log',  'log-log',  'plain', 'none', 
                                  'logit', "arcsin"),
                      conf.lower=c('usual', 'peto', 'modified'),
                      start.time, id, cluster, influence=FALSE,
                      inftime, type) {
    
    if (!missing(type)) {
        if (!is.character(type)) stop("type argument must be character")
        # older style argument is allowed
        temp <- charmatch(type, c("kaplan-meier", "fleming-harrington", "fh2"))
        if (is.na(temp)) stop("invalid value for 'type'")
        type <- c(1,3,4)[temp]
    }
    else {
        if (!(ctype %in% 1:2)) stop("ctype must be 1 or 2")
        if (!(stype %in% 1:2)) stop("stype must be 1 or 2")
        type <- as.integer(2*stype + ctype  -2)
    }
 
    conf.type <- match.arg(conf.type)
    conf.lower<- match.arg(conf.lower)
    if (is.logical(conf.int)) {
        # A common error is for users to use "conf.int = FALSE"
        #  it's not correct, but allow it
        if (!conf.int) conf.type <- "none"
        conf.int <- .95
    }
      
    # The user can call with cluster, id, both, or neither
    # If only id, treat it as the cluster too
    # If influence is requested, this implies clustering
    has.cluster <-  !(missing(cluster) || length(cluster)==0) 
    has.id <-       !(missing(id) || length(id)==0)
    if (has.id) id <- as.factor(id)
    if (influence && !(has.cluster || has.id)) {
        cluster <- seq(along=x)
        has.cluster <- TRUE
    }
    if (has.cluster) {
        if (is.factor(cluster)) {
            clname <- levels(cluster)
            cluster <- as.integer(cluster)
        } else {
            clname  <- sort(unique(cluster))
            cluster <- match(cluster, clname)
        }
        ncluster <- length(clname)
    } else {
        if (has.id) {
            # treat the id as both identifier and clustering
            clname <- levels(id)
            cluster <- as.integer(id)
            ncluster <- length(clname)
        }
        else {
            ncluster <- 0  # has neither
            clname <- NULL
        }
    }
 
    if (is.logical(influence)) {
        # TRUE/FALSE is treated as all or nothing
        if (!influence) influence <- 0L
        else influence <- 7L
    }
    else if (!is.numeric(influence))
        stop("influence argument must be numeric or logical")
    if (!(influence %in% 0:7)) stop("influence argument must be 0 to 7")
    else influence <- as.integer(influence)

    if (!missing(inftime)) {
        if (!is.numeric(inftime))
            stop("inftime must be a vector of survival times")
        inftime <- sort(inftime)  # don't trust users
        # other checks?
    } else inftime <- double(0)  # length 0

    if (!se.fit) {
        # if the user asked for no standard error, skip any robust computation
        ncluster <- 0L
        influence <- 0L
    }

    if (!is.Surv(y)) stop("y must be a Surv object")
    if (attr(y, 'type') != 'right' && attr(y, 'type') != 'counting')
	    stop("Can only handle right censored or counting data")
    ny <- ncol(y)       # Will be 2 for right censored, 3 for counting
    # The calling routine has used 'strata' on x, so it is a factor with
    #  no unused levels.  But just in case a user called this...
    if (!is.factor(x)) stop("x must be a factor")
    xlev <- levels(x)   # Will supply names for the curves
    x <- as.integer(x)  # keep the integer index

    if (missing(start.time)) time0 <- min(0, y[,ny-1])
    else time0 <- start.time

    <<survfitKM-compute>>
    <<survfitKM-finish>>
}
@ 

At each event time we have 
\begin{itemize}
  \item n(t) = number at risk = sum of weigths for those at risk
  \item d(t) = number of events = sum of weights for the deaths
  \item e(t) = unweighted number of events
\end{itemize}
From this we can calculate the Kapan-Meier and Nelson-Aalen estimates.
The Fleming-Harrington estimate is the analog of the Efron approximation
in a Cox model.
When there are no case weights the FH idea is quite simple.
Assume that the real data is not tied, but we saw a coarsened version.
If we see 3 events out of 10 subjects at risk the NA increment is 3/10 but the
FH is 1/10 + 1/9 + 1/8, it is what we would have seen with the 
uncoarsened data.
If there are case weights we give each of the 3 terms a 1/3 chance of being
the first, second, or third event
\begin{align*}
  KM(t) &= KM(t-) (1- d(t)/n(t) \\
  NA(t) &= NA(t-) + d(t)/n(t) \\
  FH(t) &= FH(t-) + \sum_{i=1}^{3} \frac{(d(t)/3}{n(t)- d(t)(i-1)/3}
\end{align*}

When one of these 3 subjects has an event but continues, which can happen with
start/stop data, then this gets trickier: the second $d$ in the last equation
above should include only the other 2.  The idea is that each of those will
certainly be present for the first event, has 2/3 chance of being present
for the second, and 1/3 for the third.
If we think of the size of the denominator as a random variable $Z$, an
exact solution would use $E(1/Z)$, the FH uses $1/E(Z)$ and the NA uses
$1/\max(Z)$ as the denominator for each of the 3 deaths.  

One problem with survival is near ties in Y: table, unique, ==, etc. can
do different things in this case.  Luckily, the parent survfit routine 
has dealt with that by using the \code{aeqSurv} function.

The underlying C code allows the sort1/sort2 vectors to be a different
length than y, weights, and cluster.  
When there is only one curve we use that to our advantage to avoid creating
a new copy of the last 3, passing in the original data.
When there are multiple curves  I had an internal debate about efficiency.
Is is better to make a subset of y for each curve = more memory, or keep the
original y and address a different subset in each C call = worse memory
cache performance?   I don't know the answer.
In either case the cluster vector needs to be re-done for each group.
Say that curve 1 uses subjects 1-10 and curve 2 uses 11-n: we don't want
the first curve to compute or keep the zero influence values for all the
subjects who are not in it.  
Especially when returning the influence matrix, which can get too large
for memory.

If ny==3 and has.id is true, then do some extra setup work, which is to
create a position vector of 1=first obs for the subject, 2 = last, 3=both,
0= other, for each set of back to back times.
This is used to prevent counting a subject with data of (0,10], (10,15] in
both the censored at 10 and entered at 10 totals.
We assume the data has been vetted to prevent overlapping intervals, so that
it suffices to sort by ending time. If a subject has holes in their timeline
they will have more than one first and last indicator.

<<survfitKM-compute>>=
if (ny==3 & has.id) position <- survflag(y, id)
else position <- integer(0)

if (length(xlev) ==1) {# only one group
    if (ny==2) {
        sort1 <- NULL
        sort2 <- order(y[,1]) 
    }
    else {
        sort2 <- order(y[,2])
        sort1 <- order(y[,1])
    }
    toss <- (y[sort2, ny-1] < time0)
    if (any(toss)) {
        # Some obs were removed by the start.time argument
        sort2 <- sort2[!toss]
        if (ny ==3) {
            index <- match(which(toss), sort1)
            sort1 <- sort1[-index]
        }
    }  
    n.used <- length(sort2)
    if (length(inftime)==0) itime <- sort(unique(y[,ny-1L]))
    else itime <- inftime
    storage.mode(itime) <- "double"
                                          
    if (ncluster > 0)
        cfit <- .Call(Csurvfitkm, y, weights, sort1-1L, sort2-1L, type, 
                      cluster-1L, ncluster, position, influence, itime, time0)
     else cfit <- .Call(Csurvfitkm, y, weights, sort1-1L, sort2-1L, type,
                        0L, 0L, position, influence, itime, time0)
} else {
    # multiple groups
    ngroup <- length(xlev)
    cfit <- vector("list", ngroup)
    n.used <- integer(ngroup)
    if (influence) clusterid <- cfit # empty list of group id values
    for (i in 1:ngroup) {
        keep <- which(x==i & y[,ny-1L] >= time0)
        if (length(keep) ==0) next;  # rare case where all are < start.time
        ytemp <- y[keep,]
        n.used[i] <- nrow(ytemp)
        if (ny==2) {
            sort1 <- NULL
            sort2 <- order(ytemp[,1]) 
        }
        else {
            sort2 <- order(ytemp[,2])
            sort1 <- order(ytemp[,1])
        }
 
        # Cluster is a nuisance: every curve might have a different set
        #  We need to relabel them from 1 to "number of unique clusters in this
        #  curve" for the C routine
        if (ncluster > 0) {
            c2 <- cluster[keep]
            c.unique <- sort(unique(c2))
            nc <- length(c.unique)
            c2 <- match(c2, c.unique)  # renumber them
            if (influence >0) {
                clusterid[[i]] <-c.unique
            }
        }
        
        if (length(inftime) ==0) itime <- sort(unique(ytemp[, ny-1L]))
        else itime <- inftime
        storage.mode(itime) <- "double"
        if (ncluster > 0) 
            cfit[[i]] <- .Call(Csurvfitkm, ytemp, weights[keep], sort1 -1L, 
                           sort2 -1L, type,  c2- 1L,
                           length(c.unique), position, influence, itime, time0)
        else cfit[[i]] <- .Call(Csurvfitkm, ytemp, weights[keep], sort1 -1L, 
                           sort2 -1L, type,
                           0L, 0L, position, influence, itime, time0)
    }
}
@ 

<<survfitKM-finish>>=
# create the survfit object
if (length(n.used) == 1) {
    rval <- list(n= length(x),
                 time= cfit$time,
                 n.risk = cfit$n[,4],
                 n.event= cfit$n[,5],
                 n.censor=cfit$n[,6],
                 surv = cfit$estimate[,1],
                 std.err = cfit$std[,1],
                 cumhaz  = cfit$estimate[,2],
                 std.chaz = cfit$std[,2])
 } else {
     strata <- sapply(cfit, function(x) nrow(x$n))
     names(strata) <- xlev
     # we need to collapse the curves
     rval <- list(n= as.vector(table(x)),
                  time =   unlist(lapply(cfit, function(x) x$time)),
                  n.risk=  unlist(lapply(cfit, function(x) x$n[,4])),
                  n.event= unlist(lapply(cfit, function(x) x$n[,5])),
                  n.censor=unlist(lapply(cfit, function(x) x$n[,6])),
                  surv =   unlist(lapply(cfit, function(x) x$estimate[,1])),
                  std.err =unlist(lapply(cfit, function(x) x$std[,1])),
                  cumhaz  =unlist(lapply(cfit, function(x) x$estimate[,2])),
                  std.chaz=unlist(lapply(cfit, function(x) x$std[,2])),
                  strata=strata)
      if (ny==3) rval$n.enter <- unlist(lapply(cfit, function(x) x$n[,8]))
}
    
if (ny ==3) {
        rval$n.enter <- cfit$n[,8]
        rval$type <- "counting"
}
else rval$type <- "right"

if (se.fit) {
    rval$logse = (ncluster==0 || (type==2 || type==4))  # se(log S) or se(S)
    rval$conf.int   = conf.int
    rval$conf.type= conf.type
    if (conf.lower != "usual") rval$conf.lower = conf.lower

    if (conf.lower == "modified") {
        nstrat = length(n.used)
	events <- rval$n.event >0
	if (nstrat ==1) events[1] <- TRUE
	else           events[1 + cumsum(c(0, rval$strata[-nstrat]))] <- TRUE
	zz <- 1:length(events)
	n.lag <- rep(rval$n.risk[events], diff(c(zz[events], 1+max(zz))))
	#
	# n.lag = the # at risk the last time there was an event (or
	#   the first time of a strata)
	#
    }
    std.low <- switch(conf.lower,
                      'usual' = rval$std.err,
                      'peto' = sqrt((1-rval$surv)/ rval$n.risk),
                      'modified' = rval$std.err * sqrt(n.lag/rval$n.risk))
        
    if (conf.type != "none") {
        ci <- survfit_confint(rval$surv, rval$std.err, logse=rval$logse,
                              conf.type, conf.int, std.low)
        rval <- c(rval, list(lower=ci$lower, upper=ci$upper))
     }
} else {
    # for consistency don't return the se if std.err=FALSE
    rval$std.err <- NULL  
    rval$std.chaz <- NULL
}

# Add the influence, if requested by the user
if (influence > 0) {
    if (influence%%2 ==1) {
        if (length(xlev)==1) {
            rval$influence.surv <- cfit$influence1
            row.names(rval$influence.surv) <- clname
        } 
        else {
            temp <- vector("list", ngroup)
            for (i in 1:ngroup) {
                temp[[i]] <- cfit[[i]]$influence1
                row.names(temp[[i]]) <- clname[clusterid[[i]]]
            }
            rval$influence.surv <- temp
        }
    }
    if (floor(influence/2) %%2 ==1) {
        if (length(xlev)==1) {
            rval$influence.chaz <- cfit$influence2
            row.names(rval$influence.chaz) <- clname
        }
        else {
            temp <- vector("list", ngroup)
            for (i in 1:ngroup) {
                temp[[i]] <- cfit[[i]]$influence2
                row.names(temp[[i]]) <- clname[clusterid[[i]]]
            }
            rval$influence.chaz <- temp
        }
    }
    if (influence >=4) { # RMST influence
        if (length(xlev)==1) {
            rval$influence.rmst <- cfit$influence3
            row.names(rval$influence.rmst) <- clname
        } 
        else {
            temp <- vector("list", ngroup)
            for (i in 1:ngroup) {
                temp[[i]] <- cfit[[i]]$influence3
                row.names(temp[[i]]) <- clname[clusterid[[i]]]
            }
            rval$influence.rmst <- temp
        }
    }
    if (length(inftime) >0)   rval$inftime <- inftime
}

if (!missing(start.time)) rval$start.time <- start.time
rval  
@ 

