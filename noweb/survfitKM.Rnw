\subsection{Kaplan-Meier}
Note -- this chunk is in development.  We still use ../R/survfitKM.S

This routine has been rewritten more times than any other in the package,
as we trade off simplicty of the code with execution speed.  
This version does all of the oranizational work in S and calls a C
routine for each separate curve. 
The first did everything in C but was too hard to maintain and the most
recent did nearly everything in S; introduction of robust variance
prompted a movement of more of the code into C since that is computationally
intensive.

<<survfitKM>>= 
survfitKM <- function(x, y, casewt=rep(1,length(x)),
		       type=c('kaplan-meier', 'fleming-harrington', 'fh2'),
		       error=c('greenwood', "aalen", "robust"), 
                       se.fit=TRUE,
		       conf.int= .95,
		       conf.type=c('log',  'log-log',  'plain', 'none','logit'),
		       conf.lower=c('usual', 'peto', 'modified'),
		       start.time, new.time, group) {
    type <- match.arg(type)
    method <- match(type, c("kaplan-meier", "fleming-harrington", "fh2"))

    if (missing(error) & any(casewt != floor(casewt))) error <- 'robust'
    else error <- match.arg(error)
    error.int <- match(error, c("greenwood", "aalen", "robust", "tsiatis"))
    if (error.int==4) error.int <- 2  # these are synonyms
    
    conf.type <- match.arg(conf.type)
    conf.lower<- match.arg(conf.lower)
    if (is.logical(conf.int)) {
        # A common error is for users to use "conf.int = FALSE"
        #  it's illegal, but allow it
        if (!conf.int) conf.type <- "none"
        conf.int <- .95
    }

    if (!is.Surv(y)) stop("y must be a Surv object")
    if (!is.factor(x)) stop("x must be a factor")
    if (attr(y, 'type') != 'right' && attr(y, 'type') != 'counting')
	    stop("Can only handle right censored or counting data")
    ny <- ncol(y)       # Will be 2 for right censored, 3 for counting
    xlev <- levels(x)   # Will supply names for the curves
    x <- as.integer(x)  # keep only the levels

 
    <<survfitKM-sort>>
    n.used <- as.vector(table(x))    # This is for the printout
    nstrat <- length(n.used)         # number of curves

    <<survfitKM-compute>>
    <<survfitKM-finish>>
}
@ 

We want to be efficient for very large data sets, so have gone with
an indexing approach.  That is, efficient wrt not making new copies
of the data within R,
I'm not worrying about cache coherency; people can do their own
sorting to deal with that issue.
<<survfitKM-sort>>=
if (nc==2) sort2 <- order(x, y[,1], y[,2]) 
else {
    sort2 <- order(x, y[,2], y[,3])
    sort1 <- order(x, y[,1], y[,3])
}

#  Allow "new.time" as a synonym for start.time
if (missing(start.time) && !missing(new.time)) start.time <- new.time
if (!missing(start.time)) { 
    n.all <- c(table(x))   # remember the original data size
    # remove any obs whose end time is <= start.time
    keep <- (y[sort2, ny-1] >= start.time)
    if (!any(keep)) stop(paste("start.time =", start.time,
                               " is greater than all time points."))
    if (!all(keep)) {
        sort2 <- sort2[keep]   # never look at these
        if (nc==3) sort1 <- sort1[y[sort1, ny-1] >= start.time]
    }
}
@

At each event time we have 
\begin{itemize}
  \item n(t) = number at risk = sum of weigths for those at risk
  \item d(t) = number of events = sum of weights for the deaths
  \item e(t) = unweighted number of events
\end{itemize}
From this we can calculate the Kapan-Meier and Nelson-Aalen estimates.
When there are no case weights the Fleming-Harrington hazard estimate
is easy; it is an analog of the Efron approximation in a Cox model.
Assume that the real data is not tied, but we saw a coarsened version.
If we see 3 events out of 10 events the NA increment is 3/10 but the
FH is 1/10 + 1/9 + 1/8, it is what we would have seen with the real data.
If there are case weights we give each of the 3 terms a 1/3 chance of being
the first, second, or third event
\begin{align*}
  KM(t) &= KM(t-) (1- d(t)/n(t) \\
  NA(t) &= NA(t-) + d(t)/n(t) \\
  FH(t) &= FH(t-) + \sum_{i=1}^{e(t)} \frac{(d(t)/e(t)}{n(t)- d(t)(i-1)/e(t)}
\end{align*}

One problem with survival is near ties in Y: table, unique, and others
do different things in this case.  Luckily, the parent routine to
this one has dealt with that by using the \code{aeqSurv} function.

The difficult part of the routine is the robust variance.  
Define
\begin{align*}
u_i(t) &= \frac{\partial KM(t)}{\partial w_k
       &= \frac{\partial KM(t-)}{\partial w_i} (1- d(t)/n(t)) - 
           KM(t-) \frac{\partial 1- d(t)/n(t)}{\partial w_i} \nonumber \\
         &= u_i(t-)(1-d(t)/n(t)) - Y_i(t) \frac{KM(t-)}{n(t)} 
           \left( dN_i(t) - d(t)/n(t)\right)
           \label{ukm1}
\end{align*}  
where $Y_i(t)$ is 1 if subject $i$ is at risk at time $t$ and $dN_i(t)$ is 1
if subject $i$ had an event at time $t$.
For the NA and FH estimates we have sums of very similar terms.

<<survfitKM-compute>>=
if (nc==3) sfit <- .Call("survkm2", y, x, wt, sort2, method)
else {  # FALSE for debug mode
    if (FALSE) sfit <- .Call("survkm1", y, x, wt, sort2, method)
    else {
        # R code, for debugging, assumes a single value for x
        stime <- y[sort2,1]
        sstat <- y[sort2,2]
        utime <- unique(stime)
        ntime <- length(utime)
        n <- length(stime)
        n.risk <- n.event <- surv <- cumhaz <- double(ntime)
        KM <- 1; NA <-0; 
        U <- matrix(0, n, 3)  # the three robust influence estimates
        V <- matrix(0, ntime, 3)  # variances
        for (i in 1:ntime) {
            j <- match(utime[i], stime)  # first matching time
            index <- sort2[j:n]
            n.risk[i] <- sum(wt[index])
            deaths <- (stime==utime[i] & sstat==1)                 
            n.event[i] <- sum(wt[deaths])
            nd <- sum(deaths)

            haz <- n.event[i]/n.risk[i]
            atrisk <- (stime >= utime[i])
            dhaz <- (ifelse(deaths,1,0) - ifelse(atrisk, haz, 0))/n.risk[i]
            U[,1] <- U[,1]*(1-haz) - KM*dhaz
            V[i,1] <- sum(U[,1]^2)
            
            U[,2] <- U[,2] * dhaz
            V[,2] <- sum(U[,2]^2)
            # var[,3] not done
            if (n.event[i] >0 ) {
                KM <- KM*(1-haz)
                NA <- NA + haz
                FH <- FH +(n.event[i]/nd)*sum(1/(n.risk - (1:nd -1)*n.event/nd))
            }
        }
    }
}
browser()        
@ 
 
<<survfitKM-finish>>=
@ 

<<survfitKM-check>>= 
indx <- order(id, Y[,2])  #ordered event times within subject
indx1 <- c(NA, indx)  #a pair of lagged indices
indx2 <- c(indx, NA)
same <- (id[indx1] == id[indx2] & !is.na(indx1) & !is.na(indx2)) #indx1, indx2= same id?
if (any(same & X[indx1] != X[indx2])) {
    who <- 1 + min(which(same & X[indx1] != X[indx2]))
    stop("subject is in two different groups, id ", (id[indx1])[who])
}
if (any(same & Y[indx1,2] != Y[indx2,1])) {
    who <- 1 + min(which(same & Y[indx1,2] != Y[indx2,1]))
    stop("gap in follow-up, id ", (id[indx1])[who])
}
if (any(Y[,1] == Y[,2])) 
    stop("cannot have start time == stop time")

if (any(same & Y[indx1,3] == Y[indx2,3] & Y[indx1,3] !=0)) {
    who <-  1 + min(which(same & Y[indx1,1] != Y[indx2,2]))
    stop("subject changes to the same state, id ", (id[indx1])[who])
}
if (any(same & weights[indx1] != weights[indx2])) {
    who <-  1 + min(which(same & weights[indx1] != weights[indx2]))
    stop("subject changes case weights, id ", (id[indx1])[who])
}
@

Now for the real work using C routines. 
My standard for a variable named ``zed'' is to use zed2 for the S object
and zed for the data part of the object; the latter is what the C code
works with.
<<survfitkm1.c>>=
#include <math.h>
#include "survS.h
#include "survproto.h"

SEXP survkm1(SEXP itime2,  SEXP status2, SEXP wt2, SEXP method2,  
	     SEXP error2,  SEXP ntime2 {
    int i, j, k;
    int n;        /* number of observations */
    /* Data passed in */
    int ntime;  /* number of unique times = length of output vectors */
    int method, error;
    int *itime, *status;
    double *wt;
    
    /*
    ** output vectors
    */
    SEXP nrisk2, nevent2, ncensor2, surv, cumhaz;
    double *nrisk, *nevent, *ncensor, *surv, *cumhaz;
    SEXP ievent2, icensor2; /* integer counts, in case of weights */
    int *ievent, *icensor;
    const char *rnames[]={"nrisk", "nevent", "ncensor", "surv",
			  "cumhaz", "std", "ievent", "icensor", ""};
    /*
    ** Get copies of the input data
    */
    ntime = asInteger(ntime2);
    method= asInteger(method2);
    error = asInteger(error2);
    itime = INTEGER(itime2);
    status= INTEGER(status2);
    wt    = REAL(wt2);
    if (method==4) id = INTEGER(id2);
    n = LENGTH(itime);
    
    /*
    ** create output objects
    */
    PROTECT(nrisk2 = allocVector(REALSXP, ntime));
    nrisk = REAL(nrisk2);
    PROTECT(nevent2 = allocVector(REALSXP, ntime));
    nevent = REAL(nevent2);
    PROTECT(ncensor2 = allocVector(REALSXP, ntime));
    ncensor = REAL(ncensor2);
    PROTECT(surv2 = allocVector(REALSXP, ntime));
    surv = REAL(surv2);
    PROTECT(cumhaz2 = allocVector(REALSXP, ntime));
    cumhaz = REAL(cumhaz2);
    PROTECT(ievent2 = allocVector(INTSXP, ntime));
    ievent = INTEGER(ivent2);
    PROTECT(icensor2 = allocVector(INTSXP, ntime));
    icensor = INTEGER(icensor2);
    if (error=0) PROTECT(var2 = allocVector(REALSXP, 1));/* no std wanted */
    else PROTECT(var2= allocVector(REALSXP, ntime));
    var = REAL(var2);
    
    /*
    ** first pass, from largest time to smallest, count up
    **  number events, number at risk, number censored
    */
    i = n -1;
    temp =0;  /* accumulates number at risk */
    for (j=ntime-1; j>=0; j--) {
	nevent[j] = 0; ievent[j]=0;
	ncensor[j]= 0; icensor[j]=0;
	ctime = itime[i];  /* current time of interest */
	while(itime[i]== ctime && i>=0) {
	    temp += wt[i];
	    if (status[i]==1){
                nevent[j] += wt[i];
                ievent[j]++;
	    }
	    else{
              ncensor[j] += wt[i];
              icensor[j]++;
	    }
	    i--;
	}
	nrisk[j] = temp;
    }
    
    /*
    ** Second pass, from smallest time to largest, accumulate
    **  the cumulative hazard and survival
    */
    <<km-compute>>
    if (error >0) {
	<<km-var>>
    }

    /*
    ** create the output structure
    */
    PROTECT(rlist = mkNamed(VEXSXP, rnames));
    SET_VECTOR_ELT(rlist, 0, nrisk2);
    SET_VECTOR_ELT(rlist, 1, nevent2);
    SET_VECTOR_ELT(rlist, 2, ncensor2);
    SET_VECTOR_ELT(rlist, 3, surv2);
    SET_VECTOR_ELT(rlist, 4, cumhaz2);
    SET_VECTOR_ELT(rlist, 5, var2);
    SET_VECTOR_ELT(rlist, 1, ievent2);
    SET_VECTOR_ELT(rlist, 2, icensor2);
    UNPROTECT(9); /*once there is NO chance of a memory allocation, we let go*/
    return(rlist);
}
@ 

Let $Y_i(t)=1$ if observation $i$ is at risk at time $t$ and 0 otherwise,
$s_i(t)$ be 1 at the point of an event for subject $i$ (status of 1),
$w_i$ be the case weight for observation $i$,
and $u_j$ $j=1,2, \ldots$ be the unique death times.

Define
\begin{center}
  \begin{tabular}{r@{=}rl}
    $r_j$& $\sum Y_i(u_j) w_i$ & weighted number at risk \\
    $d_j$& $\sum s_i(u_j) w__i$ & weighted number of events \\
    $c_j)$& $\sum (Y_i(u_j)-s_i(u_j)) w_i$ & weighted number of censored values
  \end{tabular}
\end{center}
For (start, stop] data there will also be $e_j$ which counts the number
of subjects who enter at time $t$.
Also, let $f_j$ be the total number of failures (deaths) at time $t$, not
weighted.

The cumulative hazard estimates are the Nelson-Aalen-Breslow (same estimate,
three different papers in three places) or the Fleming-Harrington.
\begin{align*}
  \Lambda_A(t) &\ \sum{u_j \le t} d_j/r_j \\
  \Lambda_{FH}(t) &= \sum{u_j \le t} \frac{d_j}
         {(1/f_j) \sum_{k=0}^{f_j-1} (r_j - kd_j/f_j)}
\end{align*}
To understand the Fleming-Harrington estimate, suppose that at some time
point we had three deaths out of 10 at risk.  The Aalen estimate gives a
hazard estimate of 3/10.  
The FH estimate assumes that the deaths didn't actually all happen at once,
even though rounding in the data collection process makes it appear that
way, so the better estimate is 1/10 + 1/9 + 1/8.  The third person to die,
whoever that was, would have had only 8 at risk when thier event happened.

The estimate of survival is either the Kaplan-Meier or the exponential
of the hazard.
\begin{equation*}
  KM(t) = \prod_{u_j \le t} \frac{r_j - d_j}{r_j}
\end{equation*}

<<km-compute>>=
tempc =0;  /*accumulates the chaz */
temps =1;  /*accumulates the survival */
tempv =0;  /*accumulates the variance */
if (method==1) {  /*KM survival and Aalen hazard, 99% of the calls */
    for (j=0; j<ntime; j++) {
        if (nevent[j] >0) {
    	temps *= (nrisk[j] - nevent[j])/nevent[j];
    	tempc += nevent[j]/nrisk[j];
        }
        surv[j] = temps;
        chaz[j] = tempc;
    }
}
else if (method==2) { /*Aalen hazard and exponent for survival */
    for (j=0; j<ntime; j++) {
        if (nevent[j] >0) tempc += nevent[j]/nrisk[j];
        chaz[j] = tempc;
        surv[j] = exp(-tempc);
    }
}
else { /* FH hazard, rarest call */
    for (j=0; j<ntime; j++) {
        if (nevent[j] >0) {
    	 temp <- nevent[j]/ievent[j]; 
    	 for (k=0; i<ievent[j]; k++) 
    	    tempc += temp/(nnrisk[j] - k*temp);
        }
        chaz[j] = tempc;
        surv[j] = exp(-tempc);
    }
}
@ 

The variance of the cumulative hazard comes in 3 flavors.
The Greenwood and the Aalen estimates are simple.
\begin{align*}
  V_A(t) &= \sum_{u_j \le t}  \frac{d_j}{r_j^2} \\
  V_G(t) &= \sum_{u_j \le t}  \frac{d_j}{r_j(r_j-d_j)} \\
  V_J(t) &= \sum_i [w_i J_i(t)]^2
\end{align*}
The infinitesimal jackknife estimate is a sum over all the subjects of
that subject's influence on $\Lambda(t)$.  The individual influences
are 
\begin{align*}
  J_i(t) &= \frac{\partial \Lambda(t)}{\partial w_i} \\
         &= J_i(t-) +  \frac{\partial \sum_k w_k d_k(t)/sum_k w_k Y_k(t)}
                {\partial w_i} \\
         &= J_i(t-) + \frac{d_k(t) - Y_k(t) \lambda(t)}{/sum_k w_k Y_k(t)}
\end{align*}
The influence terms accumulate just as the cumulative hazard does.
Computation of the IJ estimate requires an extra scratch vector.
With the FH hazard estimate things are bit more bother.  
If there are 3 tied deaths, say, then the computation has to add up
3 increments.

<<km-var>>=
temp =0;
if (error==1) { /* Greenwood */
    for (j=0; j<ntime; j++) 
        if (nevent[j] >0) temp += nevent[j]/(nrisk[j] * (nrisk[j]-nevent[j]));
    var[j] = temp;
}
else if (error==2) { /* Aalen */
    for (j=0; j<ntime; j++)
	if (nevent[j]>0) temp += nevent[j]/(nrisk[j] * nrisk[j]);
    var[j] = temp;
}
else {
    inf  = (double *) Ralloc(n, sizeof(double));  /* scratch space */
    for (i=0; i<n; i++) inf[i] =0;
    i =0;
    v0 =0;  /* sum of squares that don't need updating */
    for (j=0; j<ntime; j++) {
	if (nevent[j] >0) {  /*nothing changes at non-event times */
	    ctime = time[i];  /*current event time */
	    if (method <3) { /* variance of the Aalen hazard */
		for (;i<n && time[i]==ctime; i++) 
		    inf[i] += (status[i] - nevent[j]/nrisk[j])/ nrisk[j];
		for (k=i; k<n; k++)
		    inf[k] -= nevent[j]/(nrisk[j]*nrisk[j]);
		}
	    else { /* variance for the FH hazard */
		temp = nevent[j]/ ievent[j];
		inc1 =0; /* influence at this time for non-deaths*/
		inc2 =0;  /* influence contribution for deaths */
		for (k=0; k<ievent[j]; k++) {
		    inc1 -= temp/((nrisk - k*nevent[j])*(nrisk-k*nevent[j]));
		    inc2 += (1/(nrisk - k*nevent[j]) + inc1*(ievent[j]-k))/
			ievent[k];
		}
		for (; i<n && time[i]==ctime; i++) {
		    if (status[i]==1) inf[i] += inc2;
                    else inf[i] += inc1;
		    v0 += wt[i]*wt[i]*inf[i]*inf[i];
		}
		for (k=i; k<n; k++) inf[k] += inc2;
	    }
	    /* add it up */
	    var[j] = v0;
	    for (k=i; k<n; i++) var[j] += wt[i]*wt[i]*inf[i]*inf[i];
	}
        else var[j] = v0;     
    }
}
@ 
  
The routine for start-stop data has to do more work to count up the number
at risk, but once that is done the computations are the same.
There is, however, a bit more bookkeeping for the infinitesimal jackknife,
due to the fact that one subject may be represented in multiple observations.
<<survfitkm2.c>>=
/*  -*- c -*-  */
#include <math.h>
#include "survS.h
#include "survproto.h"

SEXP survkm2(SEXP itime2,  SEXP status2, SEXP wt2, SEXP method2,  
	     SEXP error2,  SEXP ntime2,  SEXP id2, SEXP nid2,
             SEXP sort1) {
    int i, j, k;
    int n;        /* number of observations */
    int nid;      /* number of unique id values */
    /* Data passed in */
    int ntime;  /* number of unique times = length of output vectors */
    int method, error;
    int *start, *stop, *status, *id;
    double *wt;
    
    /*
    ** output vectors
    */
    SEXP nrisk2, nevent2, ncensor2, surv, cumhaz;
    double *nrisk, *nevent, *ncensor, *surv, *cumhaz;
    SEXP ievent2, icensor2; /* integer counts, in case of weights */
    int *ievent, *icensor;
    const char *rnames[]={"nrisk", "nevent", "ncensor", "surv",
			  "cumhaz", "std", "ievent", "icensor", ""};
    /*
    ** Get copies of the input data
    */
    ntime = asInteger(ntime2);
    n = LENGTH(itime);
    nid= asInteger(nid);
    method= asInteger(method2);
    error = asInteger(error2);
    start = INTEGER(itime2);
    stop =  start + n;
    status= INTEGER(status2);
    wt    = REAL(wt2);
    if (method==4) id = INTEGER(id2);
    n = LENGTH(itime);
    id = INTEGER(id2);
    nid= asInteger(nid);
    
    /*
    ** create output objects
    */
    PROTECT(nrisk2 = allocVector(REALSXP, ntime));
    nrisk = REAL(nrisk2);
    PROTECT(nevent2 = allocVector(REALSXP, ntime));
    nevent = REAL(nevent2);
    PROTECT(ncensor2 = allocVector(REALSXP, ntime));
    ncensor = REAL(ncensor2);
    PROTECT(surv2 = allocVector(REALSXP, ntime));
    surv = REAL(surv2);
    PROTECT(cumhaz2 = allocVector(REALSXP, ntime));
    cumhaz = REAL(cumhaz2);
    PROTECT(ievent2 = allocVector(INTSXP, ntime));
    ievent = INTEGER(ivent2);
    PROTECT(icensor2 = allocVector(INTSXP, ntime));
    icensor = INTEGER(icensor2);
    if (error=0) PROTECT(var2 = allocVector(REALSXP, 1));/* no std wanted */
    else PROTECT(var2= allocVector(REALSXP, ntime));
    var = REAL(var2);
    
    /*
    ** first pass, from largest time to smallest, count up
    **  number events, number at risk, number censored
    */
    i = n -1;
    temp =0;  /* accumulates number at risk */
    for (j=ntime-1; j>=0; j--) {
	nevent[j] = 0; ievent[j]=0;
	ncensor[j]= 0; icensor[j]=0;
	ctime = itime[i];  /* current time of interest */
	while(itime[i]== ctime && i>=0) {
	    temp += wt[i];
	    if (status[i]==1){
                nevent[j] += wt[i];
                ievent[j]++;
	    }
	    else{
              ncensor[j] += wt[i];
              icensor[j]++;
	    }
	    i--;
	}
	nrisk[j] = temp;
    }
    
    /*
    ** Second pass, from smallest time to largest, accumulate
    **  the cumulative hazard and survival
    */
    <<km-compute>>
    if (error >0) {
	<<km-var>>
    }

    /*
    ** create the output structure
    */
    PROTECT(rlist = mkNamed(VEXSXP, rnames));
    SET_VECTOR_ELT(rlist, 0, nrisk2);
    SET_VECTOR_ELT(rlist, 1, nevent2);
    SET_VECTOR_ELT(rlist, 2, ncensor2);
    SET_VECTOR_ELT(rlist, 3, surv2);
    SET_VECTOR_ELT(rlist, 4, cumhaz2);
    SET_VECTOR_ELT(rlist, 5, var2);
    SET_VECTOR_ELT(rlist, 1, ievent2);
    SET_VECTOR_ELT(rlist, 2, icensor2);
    UNPROTECT(9); /*once there is NO chance of a memory allocation, we let go*/
    return(rlist);
}
@ 
