\documentclass{article}[11pt]
\usepackage{Sweave}
\usepackage{amsmath}
\addtolength{\textwidth}{1in}
\addtolength{\oddsidemargin}{-.5in}
\setlength{\evensidemargin}{\oddsidemargin}
%\VignetteIndexEntry{Cox models and ``type 3'' Tests}


\SweaveOpts{keep.source=TRUE, fig=FALSE}
% Ross Ihaka suggestions
\DefineVerbatimEnvironment{Sinput}{Verbatim} {xleftmargin=2em}
\DefineVerbatimEnvironment{Soutput}{Verbatim}{xleftmargin=2em}
\DefineVerbatimEnvironment{Scode}{Verbatim}{xleftmargin=2em}
\fvset{listparameters={\setlength{\topsep}{0pt}}}
\renewenvironment{Schunk}{\vspace{\topsep}}{\vspace{\topsep}}

\SweaveOpts{width=6,height=4}
\setkeys{Gin}{width=\textwidth}
<<echo=FALSE>>=
options(continue="  ", width=60)
options(SweaveHooks=list(fig=function() par(mar=c(4.1, 4.1, .3, 1.1))))
pdf.options(pointsize=8) #text in graph about the same as regular text
options(contrasts=c("contr.treatment", "contr.poly")) #reset default
@ 

\title{Cox models and ``type III'' tests}
\author{Terry M Therneau \\ \emph{Mayo Clinic}}
\newcommand{\myfig}[1]{\includegraphics[height=!, width=\textwidth]
                        {tests-#1.pdf}}
\newcommand{\code}[1]{\texttt{#1}}

\newcommand{\ybar}{\overline{y}}

\begin{document}
  \maketitle
\section{Introduction}
This note started with an interchange on the R-help list that 
became a bit adversarial.
A user asked ``how do I do a type III test using the Cox model'', and I
replied that, in essence, this was not a well defined question.
If he/she could define exactly what it was that they were after, then
I would look into it.
The inevitable response was that ``SAS does it''.

So what exactly is it that SAS does? 
A look in the phreg documentation turned up no leads.
Several grant deadlines were looming and I got testy at this point, 
so the discussion didn't get any further.
The present note tries to clarify the issues.
What SAS calls a \emph{type 3} test is rooted in linear models,
and is based on a moderately complex argument.  
How exactly does that argument play out in a Cox model, what
exactly are the calculations used, and
how do they relate to the usual likelihood ratio and score tests?

For the impatient readers among you I'll list the main outline and
conclusions of this report at the start.
\begin{itemize}
  \item SAS type 3 is a particular algorithm for computing a
    linear models estimate known as ``Yates' weighted squares of means'',
    which traces back to a 1934 paper by F. Yates \cite{Yates34}.
    \begin{itemize} 
      \item The utility of the Yates approach for selected designs is
        unquestioned, but whether it is always applicable is 
        far more controversial.
         Scholarly papers discussing fundamental issues with using
        the Yates approach as a default analysis method
        have appeared almost yearly in the statistics
        literature, with no apparent effect on the type 3 superiority
        myth. 
        The paper of Aitkin \cite{Aitkin78} along with its discussion
        is a fundamental resource on the topic.
       \item My own view is that for balanced or near balanced experiments 
         the Yates' hypothesis is very sensible, but for much biological data 
         it corresponds to a valid test of a completely irrelevant question.
    \end{itemize}
  \item I have a natural antipathy for grafting linear models ideas onto
    the Cox model, since I've found that some things do indeed transfer over
    but many (most) do not.  
    \begin{itemize}
      \item For a linear model there are multiple ways to
        compute and/or understand any particular estimate, including the
        Yates approach.  
      \item These different approaches will \emph{not} be identical
        when copied over to the Cox model case, thus it is important to
        carefully understand what computation is being done.
    \end{itemize}
  \item A good portion of this note is focused on linear models and 
    computations, in order to set the necessary groundwork for the Cox
    case.
  \item The SAS type 3 computation for linear models is sophisticated and
    reliable; that used in phreg is neither.  I would in fact describe this 
    recent addition as a serious black eye for the SAS package.
    The calculated results depend on which
    particular coding is chosen for the factors, 
    and will normally lead to an irresponsible result.  Documentation of the
    actual procedure is completely lacking.
    (SAS version 9.3; I will update this note if things change.)
\end{itemize}    
    
\section{Linear approximations and the Cox model}
\label{sect:transfer}
One foundation of my concern has to do with the relationship between
linear models and coxph.
The solution to the Cox model equations
can be represented as an iteratively reweighted least-squares problem, with
an updated weight matrix and adjusted dependent variable at each iteration,
rather like a GLM model.
This fact has been rediscovered multiple times, and leads to the notion
that since the last iteration of the fit \emph{looks} 
just like a set of least-squares
equations, then various least squares ideas could be carried over to the
proportional hazards model by simply writing them out using these final terms.

In practice, sometimes this works and sometimes it doesn't.  
The Wald statistic is one example of the former type, which
is completely reliable as long as the coefficients $\beta$ are not too  
large.\footnote{
  In practice failure only occurs in the rare case that one of the coefficients 
is tending to infinity.  However, in that case the failure is complete: 
the likelihood
ratio and score tests behave perfectly well but the Wald test is worthless.}
A counter example is found in two ideas used to examine model
adequacy: adjusted variable plots and constructed variable plots, each
of which was carried over to the Cox model case by reprising the 
linear-model equations.
After a fair bit of exploring I found neither is worth doing
\cite{Therneau00}.
Copying over a linear models formula simply did not work in this case.

Thus I am naturally suspicious whenever a linear models idea is grafted
onto a Cox model computation.  Has the necessary legwork been done to ensure
that the procedure actually works in the case at hand?  I have not found any
such for the SAS computation of type 3 results in their phreg procedure.
This doesn't mean it won't work, all might well be OK.  
Addition of a new method to a package without reporting such an 
investigation is irresponsible, however.

\begin{figure}
  \myfig{data}
  \caption{Average free light chain for males and females.  The figure
    shows both a smooth and the means within deciles of age.}
  \label{fig:data}
  \end{figure}

\section{Data set}
We will motivate our discussion with the simple case of a two-way
analysis.
The \code{flcdata} data frame contains the results of a small number
of laboratory tests done on a on a large fraction of the 1995
population of Olmsted County, Minnesota aged 50 or older
\cite{Kyle06, Dispenzieri12}.
The R data set contains a 50\% random sample of this larger study
and is included as a part of the survival package.
The primary purpose of the study was to measure the amounts of
plasma immunoglobulins and their components.  
Intact immunoglobulins are composed of a heavy chain and light chain
portion.  In normal subjects there is overproduction of the light chain 
component by the immune cells leading to a small amount of 
\emph{free light chain}  in the circulation.
Excessive amounts of free light chain (FLC) are thought to be a marker of
disregulation in the immune system.
Free light chains have two major forms denoted as kappa and lambda,
we will use the sum of the two.

An important medical question is whether high levels of FLC have an
impact on survival, which will be explored using a Cox model.  
To explore linear models we will compare FLC values between males
and females.
A confounding factor is that free light chain values rise with age, in
part because it is eliminated by the kidneys and renal function
declines with age.
The age distribution of males and females differs, so we
will need to adjust our simple comparison between the sexes
for age effects.
The impact of age on mortality is of course even greater
and so correction for the age imbalance is is critical when exploring
the impact of FLC on survival.

Figure \ref{fig:data} shows the trend in free light chain values
as a function of age.
For illustration of linear models using factors, we have also
created a categorical age value using deciles of age.
The table of counts shows that the sex distribution becomes increasingly
unbalanced at the older ages, from about 1/2 females in the youngest
group to a 4:1 ratio in the oldest.
<<data, fig=TRUE, include=FALSE>>=
library(survival)
age2 <- cut(flcdata$age, c(49, 59, 69, 79, 89, 120),
            labels=c("50-59", "60-69", "70-79", "80-89", "90+"))
counts <- with(flcdata, table(sex, age2))
counts
#
flcdata$flc <- flcdata$kappa + flcdata$lambda
male <- (flcdata$sex=='M')
mlow <- with(flcdata[male,],  smooth.spline(age, flc))
flow <- with(flcdata[!male,], smooth.spline(age, flc))
plot(flow, type='l', ylim=range(flow$y, mlow$y),
     xlab="Age", ylab="FLC")
lines(mlow, col=2)
cellmean <- with(flcdata, tapply(flc, list(sex, age2), mean, na.rm=T))
matpoints(c(55,65,75, 85, 95), t(cellmean), pch='fm', col=1:2)

round(cellmean, 2)
@
Notice that the male/female difference in FLC varies with age, 
\Sexpr{round(cellmean[1,1],1)} versus \Sexpr{round(cellmean[2,1],1)}
at age 50--59 and \Sexpr{round(cellmean[1,5],1)} versus
 \Sexpr{round(cellmean[2,5],1)} at age 90.
The data does not fit a simple additive model; there are ``interactions''
to use statistical parlance.
An excess of free light chain is thought to be at least partly a reflection of 
immune senescence, and due to our hormonal backgrounds men and women simply
do not age in quite the same way.

Real data always has interactions.  The treatment effect of a drug will not be
exactly the same for old and young, thin and obese, physically active and
sedentary, high vs low metabolism, etc.  Explicit recognition of this is
an underlying rationale of the current drive towards ``personalized medicine''
(though that buzzword often focuses only on genetic differences).
Any given data set will often be too small to chart these variations, and
our statistical models will will ignore complex confounding, but interactions
are nevertheless still present.
Any estimate of an overall main effect is in truth an averaged effect over some
population of confounders.

\section{Linear models}
If we ignore the age effect, then everyone agrees on the best estimate
of mean FLC: the simple average of FLC values
within each sex.
The male-female difference is estimated as the difference of these means.
This is what is obtained from a simple linear regression of FLC on sex.
Once we step beyond this and adjust for age, 
the relevant linear models can be looked
at in several ways; we will explore four of them below.
This ``all roads lead to Rome'' property of linear models is one of their
fascinating aspects, at least mathematically.

\subsection{Continuous adjustment}
\begin{figure}
  \myfig{pop}
  \caption{Three possible adjusting populations for the FLC data
    set, a data set based reference in black, least squares based
  one in red, and the US 2000 reference population as `u'.}
  \label{fig:pop}
\end{figure}

How do we form a single number summary of 
``the effect of sex on FLC''?
Here are four common choices.
\begin{enumerate}
  \item Unadjusted.  The mean for males minus the mean for females.
    The major problem with this is that a difference in age distributions
    will bias the result.  Looking at figure \ref{fig:data} imagine that
    this were two treatments A and B rather than male/female, and that
    the upper one had been given to predominantly 50-65 year olds and the 
    lower predominantly to subjects over 80.  An unadjusted difference
    would actually reverse the true ordering of the curves.
  \item Population adjusted.  An average difference between the curves,
    weighted by age. Three common weightings are
    \begin{enumerate}
      \item External reference.  It is common practice in epidemiology
        to use an external population as the reference age distribution,
        for instance the US 2000 census distribution.  This aids in 
        comparing results between studies.
      \item Data reference.  The overall population structure of the data.
      \item Least squares.  The population structure that minimizes the
        variance of the estimated female-male difference.
    \end{enumerate}
\end{enumerate}

Any fitted least squares estimate
can be rewritten as a weighted sum of the data points with weight
matrix $W= (X'X)^{-1}X'$, each row of $W$ is the weight vector for
the corresponding element of $\hat\beta$.  So we can backtrack and
see what population assumption was underneath any given fit
by looking at the weights underlying the sex coefficient.
Consider the two fits below. 
In both the second coefficient is an estimate of the overall 
difference in FLC values between the sexes.  
(The relationship in figure \ref{fig:data} is clearly curved so we have
foregone the use of a simple linear term for age; there is no point
in fitting an obviously incorrect model.)
Since $\beta_2$ is a contrast the underlying weight vectors have
negative values for the females and positive for the males.
<<>>=
us2000 <- rowSums(uspop2[51:101,,'2000'])

fit1 <- lm(flc ~ sex, flcdata, x=TRUE)
fit2 <- lm(flc ~ sex + ns(age,4), flcdata, x=TRUE)
c(fit1$coef[2], fit2$coef[2])

wt1  <- solve(t(fit1$x)%*%fit1$x, t(fit1$x))[2,]
wt2 <-  solve(t(fit2$x)%*%fit2$x, t(fit2$x))[2,]
@ 

To reconstruct the implied population density, one can
use the density function with \code{wt1} or \code{wt2} as
the case weights.
Examination of \code{wt1} immediately shows that the values
are $1/n_f$ for females and $1/n_m$ for males where
$n_f$ and $n_m$ are number of males and females, respectively.
The linear model \code{fit1} is the simple difference in male
and female means; the implied population structure for
males and females is the unweighted density of each.

Because this data set is very large and age is coded in years
we can get a density estimate by simple counting.
The result is coded below and shown in figure \ref{fig:pop}.
The data set reference and least squares reference are nearly
identical.
Least squares fits produce minimum variance unbiased estimates (MVUE),
and the variance of a weighted average is minimized by using weights
proportional to the sample size, thus the MVUE estimate will give highest
weights to those ages with a lot of people.
The weights are not \emph{exactly} which proportional to sample size for
each age.
As we all know, for a given sample size $n$ a study comparing two groups 
will have
the most power with equal allocation between the groups.  
Because the M/F ratio is more unbalanced at the right edge of the age
distribution the MVUE estimate gives just a little less weight there,
but the difference between it and the overall data set population
will be slight for all but those pathological
cases where there is minimal overlap between M/F age distributions.
(And in that case the entire discussion about what ``adjustment'' can or
should mean is much more difficult.)
<<pop, fig=TRUE, include=FALSE>>=
us2000 <- rowSums(uspop2[51:101,,'2000'])
tab0 <- table(flcdata$age)
tab2 <- tapply(abs(wt2), flcdata$age, sum)
matplot(50:100, cbind(tab0/sum(tab0), tab2/sum(tab2)),
        type='l', lty=1,
        xlab="Age", ylab="Density")

us2000 <- rowSums(uspop2[51:101,,'2000'])
matpoints(50:100, us2000/sum(us2000), pch='u')
legend(60, .02, c("Data reference", "LS reference"),
       lty=1, col=1:2, bty='n')
@ 

In practice the data-set reference and population reference methods 
are normally done using a smoothed or grouped density.  
The traditional epidemiology approach groups ages into 5 year intervals
up to age 85 and will set weights within age/sex strata such that the
sum of weights for females = sum of weights for males within each age
group (balance), and the total sum of weights in an age group is 
equal to the reference population.
An increasingly popular approach for data-set reference
is to use inverse probability weights based on logistic regression,
e.g. in the causal models literature.
This approach is illustrated in the vignette on adjusted survival
curves.

\subsection{Categorical predictors and contrasts}
When the adjusting variable is categorical --- a factor in R or a 
class variable in SAS --- then two more aspects come into play.
The first is that any estimate of interest can be written in terms
of the cell means. 
Formally, the cell means are a \emph{sufficient statistic} for the data.
For our data set and using the categorized variable \code{age2}
let $\theta_{ij}$ parametrize these means.
$$
  \begin{tabular}{cccccc}
    &50--59 & 60--69 & 70-79 & 80-89 & 90+ \\ \hline
 Female & $\theta_{11}$ & $\theta_{12}$ & $\theta_{13}$& $\theta_{14}$& 
                    $\theta_{15}$ \\
 Male & $\theta_{21}$ & $\theta_{22}$ & $\theta_{23}$& $\theta_{24}$ &
                    $\theta_{25}$ \\
\end{tabular}
  $$
For a design with three factors we will have $\theta_{ijk}$, etc.
Because it is a sufficient statistic, 
any estimate or contrast of interest can be written as a weighted sum of the
$\theta$s. 
Formulas for the resulting estimates along with their variances and
tests were worked out by Yates in 1934 \cite{Yates34} and are often referred
to as a Yates weighted means estimates.  
For higher order designs the computations can be rearranged in a form that
is manageable on a desk calculator, and this is in fact the primary point of
that paper.  
Interestingly, his computational process turns out to be closely 
related to the fast Fourier transform.

The second facet of categorical variables is that another population 
is added to the list of common estimates:
\begin{enumerate}
  \item Unadjusted 
  \item Population adjusted
    \begin{enumerate}
      \item External reference
      \item Data reference
      \item Least squares
      \item Uniform. A population in which each combination of the
        factors has the same frequency of occurrence.
    \end{enumerate}
\end{enumerate}

The uniform population plays a special role in the case of designed
experiments, where equal allocation corresponds to the optimal study
design.  The Yates estimates are particularly simple in this
case.  For a hypothetical population with equal numbers in each
age category the estimated average FLC for females turns out to be
$\mu_f = \sum_j \theta_{1j} /5$ when written in terms of the cell means
$\theta$,
and the male - female contrast is $\sum_j(\theta_{2j}-\theta_{1j})/5$.
We will refer to these as the ``Yates'' estimates and contrast for
an effect.
Conversely, the estimated age effects, treating sex as a confounding
effect and assuming an equal distribution of females and males as the
reference population, gives 
an estimated average FLC for the 60-69 year olds of
$\mu_{60-69}= (\theta_{12} + \theta_{22})/2$, and etc for the other age
groups.

We can obtain the building blocks for Yates estimates by using the
interaction function and omitting an intercept.
<<yfit>>=
yatesfit <- lm(flc ~ interaction(sex, age2) -1, data=flcdata)
theta <- matrix(coef(yatesfit), nrow=2)
dimnames(theta) <- dimnames(counts)
round(theta,2)
@ 

Any particular weighted average of the coefficients along with its
variance and the corresponding sums of squares can be computed using
the \code{contrast} function given below.
Let $C$ be a contrast matrix with $k$ rows, each containing
one column per coefficient.
Then $C\theta$ is a vector of length $k$ containing the weighted
averages and
$V = \hat\sigma^2 C (X'X)^{-1}C'$ is its variance matrix.
The sums of squares is the increase in the sum of squared residuals if the
fit were restricted to the subspace $C\theta =0$.
Formulas are from chapter 5 of Searle \cite{Searle71}.
Some authors reserve the word \emph{contrast} for the case where each
row of $C$ sums to zero and use \emph{estimate} for all others;
I am being less restrictive since the same computation serves for
both.
<<>>=
qform <- function(beta, var) # quadratic form b' (V-inverse) b
    sum(beta * solve(var, beta))
contrast <- function(cmat, fit) {
    varmat <- vcov(fit)
    if (class(fit) == "lm") sigma2 <- summary(fit)$sigma^2
    else sigma2 <- 1   # for the Cox model case

    beta <- coef(fit)
    if (!is.matrix(cmat)) cmat <- matrix(cmat, nrow=1)
    if (ncol(cmat) != length(beta)) stop("wrong dimension for contrast")

    estimate <- drop(cmat %*% beta)  #vector of contrasts
    ss <- qform(estimate, cmat %*% varmat %*% t(cmat)) *sigma2
    list(estimate=estimate, ss=ss, var=drop(cmat %*% varmat %*% t(cmat)))
    }

yates.sex <- matrix(0, 2, 10)
yates.sex[1, c(1,3,5,7,9)] <-  1/5
yates.sex[2, c(2,4,6,8,10)] <- 1/5

contrast(yates.sex, yatesfit)$estimate  # the estimated "average" FLC for F/M
contrast(yates.sex[2,]-yates.sex[,1], yatesfit) # male - female contrast
@

<<echo=FALSE>>=
# Create the estimates table -- lots of fits
emat <- matrix(0., 6, 3)
dimnames(emat) <- list(c("Unadjusted", "MVUE: continuous age", 
                         "MVUE: categorical age", "Data reference",
                         "US200", "Yates"),
                       c("est", "se", "SS"))

#unadjusted
emat[1,] <- c(summary(fit1)$coef[2,1:2], anova(fit1)["sex", "Sum Sq"])
# MVUE -- do the two fits
fit2 <- lm(flc ~ ns(age,4) + sex, flcdata)
emat[2,] <- c(summary(fit2)$coef[6, 1:2], anova(fit2)["sex", "Sum Sq"])
fit2 <- lm(flc ~ age2 + sex, flcdata)
emat[3,] <- c(summary(fit2)$coef[6, 1:2], anova(fit2)["sex", "Sum Sq"])

#Remainder, use contrasts
tfun <- function(wt) {
    cvec <- c(matrix(c(-wt, wt), nrow=2, byrow=TRUE))
    temp <- contrast(cvec, yatesfit)
    c(temp$est, sqrt(temp$var), temp$ss)
}
emat[4,] <- tfun(colSums(counts)/sum(counts))

usgroup <- tapply(us2000, rep(1:5, c(10,10,10,10,11)), sum)/sum(us2000)
emat[5,]<- tfun(usgroup)
emat[6,] <- tfun(rep(1/5,5))
@ 
\begin{table} \centering
  \begin{tabular}{l|ccc}
    & estimate & sd & SS \\ \hline
<<echo=FALSE, results=tex>>=
temp <- dimnames(emat)[[1]]
for (i in 1:nrow(emat))
   cat(temp[i], sprintf(" &%5.3f", emat[i,1]),sprintf(" &%6.5f", emat[i,2]), 
       sprintf(" & %6.1f", emat[i,3]), "\\\\ \n")
@ 
\end{tabular}
  \caption{Estimates of the male-female difference along with their
    standard errors.  The last 4 rows are based on categorized age.}
  \label{tab:allest}
\end{table}
Table \ref{tab:est} shows all of the estimates of the male/female
difference we have considered so far along with their standard errors.
Because it gives a much larger weight to the 90+ age group than any of
the other estimates, and that group has the largest M-F difference,
the projected difference for a uniform population (Yates estimate)
yields the largest contrast.
It pays a large price for this in terms of standard error, however.
As stated earlier, any least squares parameter estimate can be written
as a weighted sum of the y values.
Weighted averages have 
minimal variance when all of the weights are close to 1.  
The unadjusted estimate adheres to this precisely and the data-reference
and UMVE stay as close as possible to constant weights, subject to balancing the
population.  The Yates estimate, by treating every cell equally,
implicitly gives much larger weights to the oldest ages.
Table \ref{tab:est} shows the effective observation weights used for each of the
age categories.

<<weights>>=
casewt <- array(1, dim=c(2,5,4)) # case weights by sex, age group, estimator
csum <- colSums(counts)
casewt[,,2] <- counts[2:1,] / rep(csum, each=2)
casewt[,,3] <- rep(csum, each=2)/counts
casewt[,,4] <- 1/counts
#renorm each so that the mean weight is 1
for (i in 1:4) {
    for (j in 1:2) {
        meanwt <- sum(casewt[j,,i]*counts[j,])/ sum(counts[j,])
        casewt[j,,i] <- casewt[j,,i]/ meanwt
    }
}
@ 

\begin{table} \centering
  \begin{tabular}{rlrrrrr}
    &&50--59& 60--69 & 70--79 & 80--89 & 90+ \\ \hline
<<echo=FALSE, results=tex>>=
tname <- c("Simple", "Min var", "Population", "Yates")
for (i in 1:2) {
    for (j in 1:4) {
        cat("&",tname[j], " & ",
            paste(sprintf("%4.2f", casewt[i,,j]), collapse= " & "),
             "\\\\\n")
       if (j==1) cat(c("Female", "Male")[i])
    }
    if (i==1) cat("\\hline ")
}
@     
  \end{tabular}
  \caption{Observation weights for each data point
    corresponding to four basic approaches. 
    All weights are normed so as to have an average value of 1.}
  \label{tab:est}
\end{table}
Males have slightly higher average FLC values than females.
The unadjusted estimate underestimates the difference due to
imbalance in age: FLC rises with age and the male population
has a lower average age.  It has the smallest variance but this
is immaterial due to the bias.

Looking at table \ref{tab:est} notice the per observation weights for 
the $\ge 90$ age group, 
which is the one with the greatest female/male imbalance in the
population.  
For all but the unbalanced estimate (which ignores age) the males are
given a weight that is approximately 3 times that for females
in order to rebalance the shortage of males in that category.
However, the
absolute values of the weights differ considerably, especially
for the Yates estimate.

\subsection{Different codings}
Because the cell means are a sufficient statistic, all of the estimates based
on categorical age can be written in terms of the cell means $\hat\theta$.
The Yates contrast is the simplest to write down:
$$ \begin{tabular} {rrrrrr}
  & 50--59 & 60--69 & 70--79 & 80--89 & 90+ \\ \hline
  Female & -1/5 & -1/5 & -1/5 & -1/5 & -1/5 \\
  Male & 1/5 & 1/5 & 1/5 & 1/5 & 1/5
\end{tabular}
$$
%(Note that for calculating a sum of squares we will get the exact same
%result from a matrix using $\pm 1$ rather than $\pm 1/5$;
%the Yates contrast is often written this way.)
For the data set weighting the values of 1/5 are replaced by
$n_{+j}/n_{++}$, the overall frequency of each age group, where a $+$
in the subscript stands for addition over that subscript in the table
of counts.  
The US population weights use the population frequency of each age group.

The MVUE contrast has weights that are somewhat more complicated of
$w_j/\sum w_j$ where $w_j = 1/(1/n_{1j} + 1/n_{2j})$, and are not very
intuitive.
$$
\begin{tabular}{rrrrrr}
  & 50--59 & 60--69 & 70--79 & 80--89 & 90+ \\ \hline
<<echo=FALSE, results=tex>>=
temp <- 1/colSums(1/counts)
temp <- temp/sum(temp)
cat("Female", sprintf(" & %5.3f", -temp), "\\\\ \n")
cat("Male",   sprintf(" & %5.3f", temp), "\\\\ \n")
@ 
\end{tabular}
$$
In the alternate model \code{y \textasciitilde sex + age2} 
the MVUE contrast is much
simpler, namely (0, 1, 0,0,0,0,0), and can be read directly off the
printout as $\beta/se(\beta)$. 
The Yates contrast, however, cannot be created from the coefficients of the
simpler model at all.

%This observation holds in general: a contrast that is simple to write down
%in one coding may appear complicated in another, or not even be possible.
The usual and more familiar coding for a two way model is
\begin{equation}
  y_{ij} = \mu + \alpha_i + \beta_j + \gamma_{ij} \label{std}
\end{equation}
What do the Yates' estimates look like in this form?  Let $e_i$ be the
Yates estimate for row $i$ row $k$ the number of columns in the two way table
of $\theta$ values.  Then
\begin{align*}
  e_i &= (1/k)\sum_{j=1}^k \theta_{ij} \\
      &= \mu + \alpha_i + \sum_j \left(\beta_j + \gamma_{ij}\right)/k 
\end{align*}
and the Yates test for row effect is
\begin{align}
  0 &= e_i - e_{i'} \quad \forall i,i' \nonumber \\
    &= (\alpha_i - \alpha_{i'}) + (1/k)\sum_j(\gamma_{ij} - \gamma_{i'j})
             \label{ycont}
\end{align}

Equation \eqref{std} is overdetermined and constraints must be
added to create a unique solution.
The default in R is treatment contrasts, which use the first level
of any factor as a reference level.
Under this constraint the first level is set to zero, i.e., all
coefficients of equations \eqref{std} and \eqref{ycont} 
above where $i=1$ or $j=1$.
We have been computing the male - female contrast so $i=2$ and $i'=1$
in equation \eqref{ycont}, and the Yates contrast for sex becomes
$\alpha_2 + 1/5(\gamma_{22} +\gamma_{23} +\gamma_{24} +\gamma_{25})$.
<<treatment>>=
fit3 <- lm(flc ~ sex * age2, flcdata)
coef(fit3)
contrast(c(0,1, 0,0,0,0, .2,.2,.2,.2), fit3) #Yates
@ 
The usual constraint is SAS is to use the last level of any class 
variable as the reference group,
i.e., all coefficients with $i=2$ or $j=5$ in equations 
\eqref{std} and \eqref{ycont} are set to zero.
<<sas>>=
options(contrasts=c("contr.SAS", "contr.poly"))
sfit1 <- lm(flc ~ sex, flcdata)
sfit2 <- lm(flc ~ sex + age2, flcdata)
sfit3 <- lm(flc ~ sex * age2, flcdata)
contrast(c(0,-1, 0,0,0,0, -.2,-.2,-.2,-.2), sfit3) # Yates for SAS coding
@
The appendix contains SAS code and output for the three models
\code{sfit1, sfit2} and \code{sfit3} above.
The \code{E3} option was added to the SAS model statements, which causes
a symbolic form of the contrasts that were used for 
``type III'' results to be included in the printout.
Look down the column labeled ``SEX'' and you will see exactly the
coefficients used just above, after a bit of
SAS to English translation.
\begin{itemize}
  \item The SAS printout is labeled per equation \eqref{std}, so
    L1= column 1 of the full $X$ matrix = intercept.  L2 = column 2
    = females, L3 = column 3 = males, L4= column 4 = age 50--59, etc.
   \item In the symbolic printout they act as though sum constraints were
     in force: the last column of age is labeled with a symbolic value that
     would cause the age coefficients to sum to zero.  
     However, in actuality these coefficients are set to zero.  The table of
     parameter estimates at the end of the printout reveals this; forced
     zeros have a blank for their standard error.
  \item When calculating the contrast one can of course skip over the 
    zero coefficients, and the R functions do not include them in the
    coefficient vector.  
    Remove all of these aliased rows from the SAS symbolic printout to get
    the actual contrast that is used; this will agree with my notation.
  \item The SAS printout corresponds to the female-male contrast and I have
    focused on male-female.  This changes the signs of the contrast coefficients
    but not the result.
\end{itemize}
The \code{estimate} statement in the SAS code required that all of
the coefficients be listed, even the aliased ones
(someone more proficient in SAS may
know a way to avoid this and enter only the non-aliased values.)

The common set of constraints found in textbooks is the sum constraints
$\sum_i \alpha_i =0$, $\sum_j \beta_j=0$,
and $\sum_i \gamma_{ij} = \sum_j \gamma_{ij} =0$.
These can be obtained in R by using the contr.sum option and are called
estimate constraints in SAS.
We leave it as an exercise to the reader to verify that under this
set of constraints the Yates contrast for sex reduces to
$\alpha_2 - \alpha_1 =0$, the interaction terms do not appear at all.

A general principle is that a given hypothesis may be represented as
a simple contrast in one coding but be complex in another.  
The unadjusted test is a trivial contrast in the sfit1 coding, but a 
complex one in the sfit3 coding.
The Yates test cannot be expressed as a contrast using the sfit1 or sfit2
coding, is simple and obvious in the cell means coding, and has
simple but non obvious coefficients in the sfit3 coding.
Que sera sera.

\subsection{Sums of squares and projections}
\label{sect:anova}
The most classic exposition of least squares is as a set of
projections, each on to a smaller space.
Computationally we represent this as a series of model fits,
each fit summarized by the change from the prior fit 
in terms of residual sum of squares.
<<anova>>=
options(show.signif.stars = FALSE) #exhibit intelligence
sfit0  <- lm(flc ~ 1, flcdata)
sfit1b <- lm(flc ~ age2, flcdata)
anova(sfit0, sfit1b, sfit2, sfit3)
@ 
The second row is a test for the age effect.
The third row of the above table summarizes the improvement in
fit for the model with sex + age2 over the model with just age2,
a test of ``sex, adjusted for age''.
This test is completely identical to the minimum variance contrast,
and is in fact the way in which that SS is normally obtained.
The test for a sex effect, unadjusted for age, 
is identical to an anova table that compares
the intercept-only fit to one with sex, i.e., the second line from
a call to \code{anova(sfit0, sfit1)}.

The anova table for a nested sequence of models $A$, $A+B$, $A + B +C$, \ldots
has a simple interpretation, outside of contrasts or populations,
as an improvement in fit.  Did the variable(s) $B$ add significantly 
to the goodness of fit for a model with just $A$, was $C$ an important
addition to a model that already includes $A$ and $B$?  
The assessment of improvement is based on the likelihood ratio test (LRT),
and extends naturally to all other models based on likelihoods.
The tests based on a target population (external, data population,
or Yates) do not fit naturally into this approach, however.

Obtaining the Yates contrast using a sequential sums of squares approach
is possible but a bit contrived.
Our final fit in the table will be \code{sfit3}, but
the one prior to it needs to be from a constrained version of \code{sfit3},
whose solution lies in the space spanned by  the Yates contrast 
$\beta_2 + \beta_7/5 + \beta_8/5 + \beta_9/5 + \beta_{10}/5 = 0$. 
There is no simple way to write down an ordinary LS model equation that
will do this, and instead one must use one a program for constrained
linear regression; these are far less familiar.
There are many algorithms to fit a constrained linear regression, one is
to transform the problem as $X\beta = (XQ)(Q'\beta) = Z \phi$
where $Q$ is an orthogonal transformation matrix.
If the first column of $Q$ is chosen as a scaled version of the Yates
contrast, then setting that contrast equal to zero is the same as
the constraint $\phi_1 =0$; it suffices to fit a model using all but the
first column of $Z$. 

\subsection{What is SAS type 3?}
We are now in a position to fully describe the SAS sums of squares.
\begin{itemize}
  \item Type 1 is the output of the ANOVA table, where terms are entered
    in the order specified in the model.
  \item Type 2 is the result of a two stage process
    \begin{enumerate}
      \item Order the terms by level: 0= intercept, 1= main effects, 
        2= 2 way interactions, \ldots.
      \item For terms of level k, print the MVUE contrast from a model
        that includes all terms of levels $0-k$.
        Each of these will be equivalent to the corresponding line of
        a sequential ANOVA table where the term in question was entered
        as the last one of its level.
    \end{enumerate}
  \item Type 3 and 4 are also a 2 stage process
    \begin{enumerate}
      \item Segregate the terms into those for which a Yates contrast
        can be formed versus those for which it can not.  The second group
        includes the intercept, any continuous variables, and any factor
        (class) variables that do not participate in interactions with
        other class variables.
      \item For variables in the first group computed Yates contrasts. For
        those in the second group compute the type 2 results.
    \end{enumerate}
\end{itemize}

In order to replicate this one of the steps needed is a general routine to
form a Yates contrast for any of the possible codings.
This can be done by a summing rows of the $X$ matrix within levels of
the factor of interest.  Since we are aiming at a uniform reference
population, start
with the subset of unique rows of $X$ --- this will have 1 copy for each cell
and thus is by definition balanced --- and take an average within each
level of sex.
<<>>=
xx <- unique(model.matrix(sfit3))
xmean <- rowsum(xx, xx[,2])/5 
t(xmean) #transpose, to fit on the vignette page nicely
yates.sex <- xmean[2,] - xmean[1,]  # Yates contrast for male - female
as.matrix(yates.sex) #another way to force vertical listing
@ 
The rows of \code{xx} above contain the coding for each of the 10 sex by
age cells in the current default (contr.SAS).  Taking averages of the
``female'' and ``male'' rows
reprises the computation of equation \eqref{ycont}. 
The result is equal to the SAS E3 printout.
Here is the same calculation for the age contrast.
<<>>=
agegrp <- xx[,3:6] %*% 1:4  #90+ will be agegrp 0
xmean <- rowsum(xx, agegrp)/2  #two cells per age group
yates.age <- rbind(xmean[2,] - xmean[1,],
                   xmean[3,] - xmean[1,],
                   xmean[4,] - xmean[1,],
                   xmean[5,] - xmean[1,])
t(yates.age) #transpose, to fit nicely on the page
@ 
Again, this again agrees with the SAS type 3 printout.

A routine for the general case would involve first selecting
from the model matrix only those columns for which Yates
contrasts are desired, then selecting the unique subset of rows, 
and finally a small bit of bookkeeping to create the averages.
All of the information required is contained in the model result, however,
within the \code{terms, assign}, and \code{xlevels} components.
The approach extends to higher levels of nesting, e.g., the Yates contrast
for a 2 way interaction in the presence of other 3 way interactions.

SAS describes two contrast algorithms
in their document 
``The four types of estimable functions'' \cite{SASguide}, 
one of which defines type 3 and the other type 4.  
I have read this document multiple times and have not been able to
create code that replicates
their method; several aspects of the description still elude me.
If there are no missing cells, i.e. factor combinations with no observations,
both SAS methods are known to be yield the Yates contrast, however.

When there are missing cells, then it is not possible to compute a 
contrast that corresponds to a uniform
distribution over the cells, and thus the standard Yates contrast is 
also not defined.
The SAS type 3 and 4 algorithms still produce a value, however. 
What exactly this result ``means'' and whether it is a good idea has
been the subject of lengthy debates which I will not explore here. Sometimes
the type 3 and type 4 algorithms will agree but often do not when there
are missing cells, which further muddies the waters.
(My simple Yates algorithm described above will generally agree with 
neither in this case.)

Thus we have 3 different tests: the MVUE comparison which will be close
but not exactly equal to the data set population, Yates comparisons which
correspond to a uniform reference population, and the SAS type 3 (STT) which
prints out a chimeric blend of uniform population weighting for 
those factor variables that are in
interactions and the MVUE weighting for all the other terms.  

We need to mention one other computatation which I will call the non-SAS type 3
(NSTT) method.  If one uses the sum constraints commonly found in textbooks,
which corresponds to the \code{contr.sum} constraint in R and to \code{effect}
constraints in SAS, and there are no missing cells, then the simple and obvious
contrast $\alpha_i =0$ will be equal to the Yates contrast for sex, 
and similarly a test of $\beta_j=0 \quad \forall j$ corresponds to that for age.
I often see this method recommended on R help in response to
the question of ``how to obtain type III'', computed either by use of the 
\code{drop1} command or the \code{Anova} function found within the car
package, but said advice almost never mentions the need for this particular
non-default setting of the contrasts option.\footnote{The Companion to Applied
Regression (car) package is designed to be used with the book of the same
name by John Fox, and the book does clarify the need for sum constraints.}
When applied to other codings the results of this procedure can be 
surprising.
<<nstt>>=
options(contrasts = c("contr.treatment", "contr.poly")) #R default
fit3a <- lm(flc ~ sex * age2, flcdata)
options(contrasts = c("contr.SAS", "contr.poly"))
fit3b <- lm(flc~ sex * age2, flcdata)
options(contrasts=c("contr.sum", "contrl.poly"))
fit3c <- lm(flc ~ sex * age2, flcdata)
#
nstt <- c(0,1, rep(0,8))  #test only the sex coef = the NSTT method
temp <- rbind(unlist(contrast(nstt, fit3a)),
              unlist(contrast(nstt, fit3b)),
              unlist(contrast(nstt, fit3c)))[,1:2]
dimnames(temp) <- list(c("R", "SAS", "sum"), c("effect", "SS"))
print(temp)
#
drop1(fit3a, .~.)
@ 
For the case of a two level effect such as sex, the 
NSTT contrast under the default R coding is a comparison of males to
females in the first age group \textbf{only},
and under the default SAS coding it is a comparison of males to females
within the \textbf{last} age group.
Due to this easy creation of a test statistic which has no relation to
the global comparison one expects from the ``type 3'' label,
a more appropriate acronym for the NSTT might be \emph{not safe type three}
or perhaps even \emph{nonsense type three}.

\subsection{Which estimate is best?}
Deciding which estimate is the best is complicated.
Unfortunately a lot of statistical textbooks emphasize the peculiar
situation of balanced data with exactly the same number of subjects in each cell.
Such data is \emph{extremely} peculiar if you work in medicine;
in 30 years work and several hundred studies I have seen 2 instances.
In this peculiar case the unadjusted, MVUE, data reference and Yates
populations are all correspond to a uniform population and 
so give identical results.
No thinking about which estimate is best is required.
This has led many to avoid the above question, instead pining for that
distant Eden where the meaning of ``row effect'' is perfectly unambiguous.
But we are faced with real data and need to make a choice.

The question has long been debated in depth by wiser heads than mine.
In a companion paper to his presentation at the joint statistical meetings
in 1992, Macnaughton \cite{Macnaughton92} lists 54 references to the topic
between 1952 and 1991.  
Several discussion points recur:
\begin{enumerate}
  \item Many take the sequential ANOVA table as primary, i.e., a set of
    nested models along with likelihood ratio tests (LRT), and decry
    all comparisons of ``main effects in the presence of interaction.''
  \item Others are distressed by the fact that the MVUE adjusting population
    is data dependent, so that one is ``never sure exactly what 
    hypothesis being tested''.
  \item A few look at the contrast coefficients themselves, with a preference
    for simple patterns since they ``are interpretable''.
  \item No one approach works for all problems.  Any author who proposes
    a uniform rule is quickly presented with counterexamples.
\end{enumerate}
Those in group 1 argue strongly against the Yates weighting and those in group 2
argue for the Yates contrast.  
Group 3 is somewhat inexplicable to me since any
change in the choice of constraints will change all the patterns.
I fear that an opening sentence from a 1986 overview/review of Herr
\cite{Herr86}
is still apropos, ``In an attempt to understand how we have arrived at our
present state of ignorance \ldots''.

There are some cases where the Yates approach is clearly sensible, for instance
a designed experiment which has become unbalanced due to a failed
assay or other misadventure that has caused a few data points to be missing.
There are cases such as the FLC data where the Yates contrast 
makes little sense at
all --- the hypothetical population with equal numbers of 50 and 90 year olds is
one that will never be seen--- so it is rather like speculating on the the
potential covariate effect in dryads and centaurs.
The most raucous debate has circled around the case of testing for a treatment
effect in the presence of multiple enrolling centers.  
Do we give each patient equal weight (MVUE) or each center equal weight (Yates).
A tongue-in-cheek but nevertheless excellent commentary on the subject is
given by the old curmudgeon, aka Guernsey McPearson \cite{Senn1, Senn2}.

I find the 1978 paper by Aitkin \cite{Aitkin78} particularly useful.  
This was read before the Royal Statistical
Society and includes remarks by 10 discussants forming a who's who of
statistical theory (F Yates, J Nelder, DR Cox, DF Andrews, KR Gabriel, \ldots).
The summary of the paper states that 
``It is shown that a standard method of analysis used in many ANOVA programs,
equivalent to Yates method of weighted squares of means, may lead to 
inappropriate models''; the paper goes on to carefully show why no one
method can work in all cases.
Despite the long tradition among RSS discussants of first congratulating the 
speaker and then skewering every one their conclusions, 
not one defense of the always-Yates approach is raised!
This includes the discussion by Yates himself, who protests that his original
paper advocated the proposed approach with reservations, it's primary advantage
being that the computations could be performed on a desk calculator.

I have two primary problems with the SAS type 3 approach.
The first and greatest is that their documentation recommends the method
with no reference to this substantial and sophisticated literature
discussing strengths and weaknesses of the approach.  
This represents a level of narcissism which is completely unprofessional.
%Recommending the type III approach as best for all cases, as they do, has
%caused actual harm.
The second is that their documentation explains the method is a way that
is almost impenetrably opaque.  
They start with the columns of X which remain after elimination 
of linearly dependencies (labeled as L1, L2, \ldots), and then cite
an orthogannlity principle which arises from viewing Yates contrast
as a particular linear projection.  
If this is the only documentation one has, there will not be 1 person in 
100 who can
explain the actual biological hypothesis which is being addressed by a type 3
test.

\section{Cox models}
Adapting the Yates test to a Cox model is problematic from the start.
First, what do we mean by a ``balanced population''?
In survival data, the variance of the hazard ratio for each particular
sex/age combination is proportional to the number of deaths in that
cell rather than the number of subjects.
Carrying this forward to the canonical problem of adjusting a treatment
effect for enrolling center, does the rallying cry of
``equal weight for each center'' lead to equal numbers of subjects
or equal numbers of events?
The second issue is that the per-cell hazard ratio estimates are no
longer a minimally sufficient statistic, so underlying arguments
about a reference population no longer directly translate into a contrast
of the parameters.  
A last but more minor issue is that the three common forms of
the test statistic ---  Wald, score, and LRT --- are identical in a linear
model but not for the Cox model, so which should we choose?

To start, take a look at the overall data.
Compute the relative death rates for each age/sex cell.
<<relrate>>=
options(contrasts= c("contr.treatment", "contr.poly")) # R default
cfit0 <- coxph(Surv(futime, death) ~ interaction(sex, age2), flcdata)
cmean <- matrix(c(0, coef(cfit0)), nrow=2)
cmean <- rbind(cmean, cmean[2,] - cmean[1,])
dimnames(cmean) <- list(c("F", "M", "M/F"), dimnames(counts)[[2]])
signif(exp(cmean),3)
@ 
Since the Cox model is a relative risk model all of the
death rates are relative to one of the cells, in this case the
50--59 year old females has been arbitrarily chosen as the reference cell
and so has a defined rate of 1.00.  
Death rates rise dramatically with age for both males and females
(no surprise),
with males always slightly ahead in the race to a coffin.
The size of the disadvantage for males decreases in the last 2
decades, however.

The two standard models are for sex alone, and sex after age.
Likelihood ratio tests for these models are the natural analog to
anova tables for the linear model, and are produced by the
same R command.
<<cox anova>>=
options(contrasts=c("contr.SAS", "contr.poly"))
cfit1 <- coxph(Surv(futime, death) ~ sex, flcdata)
cfit2 <- coxph(Surv(futime, death) ~ age2 + sex, flcdata)
cfit3 <- coxph(Surv(futime, death) ~ sex * age2, flcdata)
#
summary(cfit1)$coef
summary(cfit2)$coef[,1:3]
#
anova(cfit1)
anova(cfit2)
@ 
Without adjustment for age the LRT for sex is only
\Sexpr{round(2*diff(cfit1$loglik),1)}, and after adjustment for %$
age it increases to \Sexpr{round(anova(cfit2)[3,2],2)}.
Since females are older, not adjusting for age almost complete
erases evidence of their actual survival advantage.
The Wald tests for sex are equal to $[\beta/ se(\beta)]^2$ using the
sex coefficient from the fits,
\Sexpr{round(summary(cfit1)$coef[1,4]^2,2)} and 
\Sexpr{round(summary(cfit2)$coef[5,4]^2,2)},
respectively.
Unlike a linear model they are not exactly equal to the 
anova table results, based on log-likelihood, but tell the same story.
A common adjustment for institution in a multi-center study is to
stratify on the enrolling center.
The estimated sex effect for this data set are very similar to the additive model
fit.
<<>>=
cfit4 <- coxph(Surv(futime, death) ~ sex + strata(age2), flcdata)
summary(cfit4)$coef
@ 

The coefficients, variances and log-likelihoods for cfit2
are identical to the phreg output for an additive model as
found in the appendix.
As expected, the ``type III'' results for the additive model
are simply the Wald tests for the fit.
<<>>=
contrast(diag(5)[1:4,], cfit2)$ss #Wald test for age
@ 
Results of the sequential analysis --- which SAS refers to as type 1 ---
are unchanged if we change to any of the other possible codings for
the factor variables (not shown).

Now for the main event: look at the model that contains interactions.
We surmised this model along with contrast calculations to be the most likely
way in which the phreg code would implement type 3, as it is the
easiest to integrate with existing code. 
Results are shown in the last SAS fit of the appendix.
Below we compute the Yates and the NSTT contrasts for both age and sex.
The NSTT contrast is trivial, simply 1's and 0's to pick off the
main effects coefficients for sex and age.
The Yates contrasts were computed above for linear models, we just need
to remove the intercept term since this is absorbed into the baseline
hazard of a Cox model.
<<coxcon1>>=
nstt.sex <- diag(9)[1,]  #contrast for sex
nstt.age <- diag(9)[2:5,]
#
contrast(yates.sex[-1], cfit3)
contrast(nstt.sex, cfit3)$ss
contrast(yates.age[,-1], cfit3)$ss
contrast(nstt.age, cfit3)$ss
@ 
Comparing these results to the SAS printout labeled as ``Type III Wald''
shows that phreg is using the NSTT method.
This is a bit of a shock.
All of the SAS literature on type III emphasizes the care with which 
they form the calculation so as to always produce a Yates contrast
(or in the case of missing cells a Yates-like one),
and there was no hint in the documentation that phreg does anything different.
As a double check direct contrast statements
corresponding to the Yates and NSTT contrasts we added to the SAS code,
and give confirmatory results.  
A further run which forced sum constraints by adding
\code{'/ effect'} to the SAS class statement (not shown) restored the
correct Yates contrast, as expected.
As a final check, look at the NSTT version of the LRT, which corresponds
to simply dropping the sex column from the $X$ matrix.
<<nstt-lrt>>=
xmat3 <- model.matrix(cfit3)
cfit3b <- coxph(Surv(futime, death) ~ xmat3[,-1], flcdata)
anova(cfit3b, cfit3)
@ 
This agrees with the LR ``type 3'' test of the phreg printout.

Since it gives more weight to the larger ages, where the sex effect
is smallest, the Yates estimate of treatment effect is smaller than
the result of cfit2. Nevertheless,
the Yates and the sequential test are more similar for the survival 
outcome than for the linear models.
This is due to the fact that the variances of the individual hazards
for each sex/age combination are proportional to the number of
deaths in that cell rather than the number of subjects per cell.
A table of the number of deaths is not as imbalanced as the
table of subject counts, and so the Yates and MLE ``populations''
are not as far apart as they were for the linear regression.
There are fewer subjects at the higher ages but they die more
frequently.
<<>>=
with(flcdata, table(sex, age2, death))[,,2] #table of deaths per cell
@ 

Last, look at the output of the weighted models, both 
population and Yates.
The case weight argument does translate directly from the
linear model to the Cox case (or any other) and so is of
interest.
The fits require use of a robust variance,
since we are approaching it via a survey sampling computation.
The tapply function creates a per-subject index into the case weight
table created earlier.
<<coxfit>>=
wtindx <- with(flcdata, tapply(death, list(sex, age2)))
cfitpop <- coxph(Surv(futime, death) ~ sex, flcdata,
                robust=TRUE, weight = (casewt[,,3])[wtindx]) 
cfityates <- coxph(Surv(futime, death) ~ sex, flcdata,
                robust=TRUE, weight = (casewt[,,4])[wtindx])
#
# Glue it into a table for viewing
#
tfun <- function(fit, indx=1) {
    c(fit$coef[indx], sqrt(fit$var[indx,indx]))
  }
coxp <- rbind(tfun(cfit1), tfun(cfit2,5), tfun(cfitpop), tfun(cfityates))
dimnames(coxp) <- list(c("Unadjusted", "Sequential", "Balanced Pop", 
                         "Equal Pop"),
                      c("Effect", "se(effect)"))
signif(coxp,3)
@ 
The population estimates based on reweighting lie somewhere between
the unadjusted and the sequential results.
Balancing to the hypothetical population with equal numbers of
subjects per cell yields a smaller estimate because it gives
extreme weights to the oldest age group, where the male/female
difference is smallest.
The argument for balancing an experiment in this way is much
weaker in a Cox model, since it leads to optimal power
only under the null hypothesis of no difference in death rates
between the age groups.

Overall, both rebalanced estimates and coefficient contrasts are 
interesting exercises for the Cox model, but their overall utility
is unclear.  
It is difficult to make any global optimality argument for
either one, particularly in comparison to the sequential tests
which have the entire weight of likelihood theory as a justification.
Case reweighted estimates do play a key role when attempting to 
adjust for non-random treatment assignment, as found in the
literature for causal analysis and marginal structural models;
a topic and literature far too extensive and nuanced for discussion
in this note.

No special role is apparent, at least to this author, for regular or even
sporadic use of a Yates contrast in survival models.  
The addition of such a feature and label to the SAS phreg package is
a statistical disaster, one that knowledgeable and conscientious
statistical practitioners will likely have to fight for the rest of their
careers.
In the common case of a treatment comparison, adjusted for enrolling center,
the default ``type III'' printout from phreg corresponds to a comparison
of treatments within the last center;
the only contribution of the remainder of the data set is to help define
the baseline hazard function + the effect of any continuous adjusters that
happen to be in the model.
The quadruple whammy of a third rate implementation (the NSTT),
defaults that lead to a useless and misleading result,
no documentation of the actual computation that is being done,
and irrational reverence for the type III label conspire
to make this a particularly unfortunate event.


\appendix
\section{SAS computations}
The following code was executed in version 9.3 of SAS.
\begin{verbatim}
options ls=70;
libname save "sasdata";

title "Sex only";
proc glm data=save.flc;
    class sex;
    model flc = sex;
title "Sex only";
    

proc glm data=save.flc;
    class sex age2;
    model flc = age2 sex /solution E1 E2 E3;
title "Second fit, no interaction";


proc glm data=save.flc;
    class sex age2;
    model flc = sex  age2  sex*age2/solution E1 E2 E3;

    estimate 'yates' sex 1 -1  sex*age2 .2 .2 .2 .2 .2 -.2 -.2 -.2 -.2 -.2;
    
title "Third fit, interaction";

proc phreg data=save.flc;
    class sex age2;
    model futime * death(0) = sex age2/ ties=efron;
title "Phreg fit, sex and age, additive";

proc phreg data=save.flc;
    class sex age2;
    model futime * death(0) = sex age2  sex*age2 /
          ties=efron type3(all);
    
    estimate 'Yates sex' sex 1 sex*age2  .2 .2 .2 .2;
    contrast 'NSTT sex ' sex 1 ;
    contrast 'NSTT age' age2 1 0 0 0 ,
                         age2 0 1 0 0 ,
                         age2 0 0 1 0 ,
                         age2 0 0 0 1;
title "Phreg fit, sex and age with interaction";

proc phreg data=save.flc;
    class sex age2/ param=effect;
    model futime * death(0) = sex age2  sex*age2 / ties=efron;
title "Phreg, using effect coding";
\end{verbatim}

The SAS output is voluminous, covering over a dozen pages.  
A subset is extracted below, leaving out portions that are unimportant
to our comparison.
First the GLM model for sex only.  There are no differences
between type 1 and type 3 output for this model.
\small
\begin{verbatim}
...
               Number of Observations Read        7874
               Number of Observations Used        7874
...
Dependent Variable: flc   

                                      Sum of
Source                     DF        Squares    Mean Square   F Value

Model                       1      142.19306      142.19306     42.27

Error                    7872    26481.86345        3.36406          

Corrected Total          7873    26624.05652                         
\end{verbatim}
\normalsize

The second fit with sex and then age.
\small
\begin{verbatim}
                     Type I Estimable Functions
 
                   -----------------Coefficients------------------
   Effect          age2                                        sex

   Intercept       0                                           0  

   age2      1     L2                                          0  
   age2      2     L3                                          0  
   age2      3     L4                                          0  
   age2      4     L5                                          0  
   age2      5     -L2-L3-L4-L5                                0  

   sex       F     -0.2571*L2-0.2576*L3-0.1941*L4-0.0844*L5    L7 
   sex       M     0.2571*L2+0.2576*L3+0.1941*L4+0.0844*L5     -L7



                     Type II Estimable Functions
 
                                 ---Coefficients----
                 Effect          age2            sex

                 Intercept       0               0  

                 age2      1     L2              0  
                 age2      2     L3              0  
                 age2      3     L4              0  
                 age2      4     L5              0  
                 age2      5     -L2-L3-L4-L5    0  

                 sex       F     0               L7 
                 sex       M     0               -L7



                    Type III Estimable Functions
 
                                 ---Coefficients----
                 Effect          age2            sex

                 Intercept       0               0  

                 age2      1     L2              0  
                 age2      2     L3              0  
                 age2      3     L4              0  
                 age2      4     L5              0  
                 age2      5     -L2-L3-L4-L5    0  

                 sex       F     0               L7 
                 sex       M     0               -L7

Dependent Variable: flc   

                                      Sum of
Source                     DF        Squares    Mean Square   F Value

Model                       5     2212.13649      442.42730    142.60

Error                    7868    24411.92003        3.10268          

Corrected Total          7873    26624.05652                         



Source                     DF      Type I SS    Mean Square   F Value

age2                        4    1929.642183     482.410546    155.48
sex                         1     282.494304     282.494304     91.05


Source                     DF     Type II SS    Mean Square   F Value

age2                        4    2069.943424     517.485856    166.79
sex                         1     282.494304     282.494304     91.05


Source                     DF    Type III SS    Mean Square   F Value

age2                        4    2069.943424     517.485856    166.79
sex                         1     282.494304     282.494304     91.05



                                      Standard
Parameter           Estimate             Error    t Value    Pr > |t|

Intercept        5.503757546 B      0.17553667      31.35      <.0001
age2      1     -2.587424744 B      0.17584961     -14.71      <.0001
age2      2     -2.249164537 B      0.17684133     -12.72      <.0001
age2      3     -1.770342603 B      0.17834253      -9.93      <.0001
age2      4     -1.082104827 B      0.18584656      -5.82      <.0001
age2      5      0.000000000 B                                       
sex       F     -0.383454133 B      0.04018624      -9.54      <.0001
sex       M      0.000000000 B                                       
\end{verbatim}
\normalsize

The third linear models fit, containing interactions.
For first portion I have trimmed off long printout on the right, i.e.
the estimable functions for the age2*sex effect since they are not
of interest.
\small
\begin{verbatim}
                      Type I Estimable Functions
 
                 --------------------Coefficients--------
Effect           sex          age2

Intercept        0            0                                       

sex       F      L2           0                                       
sex       M      -L2          0                                       

age2      1      -0.0499*L2   L4                                      
age2      2      -0.0373*L2   L5                                      
age2      3      0.0269*L2    L6                                      
age2      4      0.0482*L2    L7                                      
age2      5      0.0121*L2    -L4-L5-L6-L7                            

sex*age2  F 1    0.3786*L2    0.6271*L4+0.1056*L5+0.0796*L6+0.0346*L7 
sex*age2  F 2    0.2791*L2    0.0778*L4+0.5992*L5+0.0587*L6+0.0255*L7 
sex*age2  F 3    0.2182*L2    0.0527*L4+0.0528*L5+0.6245*L6+0.0173*L7 
sex*age2  F 4    0.1055*L2    0.0188*L4+0.0188*L5+0.0142*L6+0.7006*L7 
sex*age2  F 5    0.0186*L2    -0.7764*L4-0.7764*L5-0.777*L6-0.7781*L7 
sex*age2  M 1    -0.4285*L2   0.3729*L4-0.1056*L5-0.0796*L6-0.0346*L7 
sex*age2  M 2    -0.3164*L2   -0.0778*L4+0.4008*L5-0.0587*L6-0.0255*L7
sex*age2  M 3    -0.1913*L2   -0.0527*L4-0.0528*L5+0.3755*L6-0.0173*L7
sex*age2  M 4    -0.0573*L2   -0.0188*L4-0.0188*L5-0.0142*L6+0.2994*L7
sex*age2  M 5    -0.0065*L2   -0.2236*L4-0.2236*L5-0.223*L6-0.2219*L7 

                     Type II Estimable Functions
 
                 --------------------Coefficients---------------------
Effect           sex          age2

Intercept        0            0                                       

sex       F      L2           0                                       
sex       M      -L2          0                                       

age2      1      0            L4                                      
age2      2      0            L5                                      
age2      3      0            L6                                      
age2      4      0            L7                                      
age2      5      0            -L4-L5-L6-L7                            

sex*age2  F 1    0.41*L2      0.6271*L4+0.1056*L5+0.0796*L6+0.0346*L7 
sex*age2  F 2    0.3025*L2    0.0778*L4+0.5992*L5+0.0587*L6+0.0255*L7 
sex*age2  F 3    0.2051*L2    0.0527*L4+0.0528*L5+0.6245*L6+0.0173*L7 
sex*age2  F 4    0.073*L2     0.0188*L4+0.0188*L5+0.0142*L6+0.7006*L7 
sex*age2  F 5    0.0093*L2    -0.7764*L4-0.7764*L5-0.777*L6-0.7781*L7 
sex*age2  M 1    -0.41*L2     0.3729*L4-0.1056*L5-0.0796*L6-0.0346*L7 
sex*age2  M 2    -0.3025*L2   -0.0778*L4+0.4008*L5-0.0587*L6-0.0255*L7
sex*age2  M 3    -0.2051*L2   -0.0527*L4-0.0528*L5+0.3755*L6-0.0173*L7
sex*age2  M 4    -0.073*L2    -0.0188*L4-0.0188*L5-0.0142*L6+0.2994*L7
sex*age2  M 5    -0.0093*L2   -0.2236*L4-0.2236*L5-0.223*L6-0.2219*L7 


                     Type III Estimable Functions
 
                ---------------------Coefficients---------------------
Effect          sex      age2                          sex*age2

Intercept       0        0                             0              

sex       F     L2       0                             0              
sex       M     -L2      0                             0              

age2      1     0        L4                            0              
age2      2     0        L5                            0              
age2      3     0        L6                            0              
age2      4     0        L7                            0              
age2      5     0        -L4-L5-L6-L7                  0              

sex*age2  F 1   0.2*L2   0.5*L4                        L9             
sex*age2  F 2   0.2*L2   0.5*L5                        L10            
sex*age2  F 3   0.2*L2   0.5*L6                        L11            
sex*age2  F 4   0.2*L2   0.5*L7                        L12            
sex*age2  F 5   0.2*L2   -0.5*L4-0.5*L5-0.5*L6-0.5*L7  -L9-L10-L11-L12
sex*age2  M 1   -0.2*L2  0.5*L4                        -L9            
sex*age2  M 2   -0.2*L2  0.5*L5                        -L10           
sex*age2  M 3   -0.2*L2  0.5*L6                        -L11           
sex*age2  M 4   -0.2*L2  0.5*L7                        -L12           
sex*age2  M 5   -0.2*L2  -0.5*L4-0.5*L5-0.5*L6-0.5*L7  L9+L10+L11+L12 


Source                     DF      Type I SS    Mean Square   F Value

sex                         1     142.193063     142.193063     45.97
age2                        4    2069.943424     517.485856    167.30
sex*age2                    4      87.218363      21.804591      7.05

Source                     DF     Type II SS    Mean Square   F Value

sex                         1     282.494304     282.494304     91.33
age2                        4    2069.943424     517.485856    167.30
sex*age2                    4      87.218363      21.804591      7.05


Source                     DF    Type III SS    Mean Square   F Value

sex                         1     126.961986     126.961986     41.05
age2                        4    1999.446491     499.861623    161.60
sex*age2                    4      87.218363      21.804591      7.05

                                         Standard
 Parameter                 Estimate         Error  t Value  Pr > |t|

 yates                  -0.58972607    0.09204824    -6.41    <.0001

                                       Standard
 Parameter            Estimate            Error   t Value   Pr > |t|

 Intercept         6.003043478 B     0.36672295     16.37     <.0001
 sex       F      -1.024512614 B     0.41553944     -2.47     0.0137
 sex       M       0.000000000 B                                    
 age2      1      -3.176876326 B     0.36950532     -8.60     <.0001
 age2      2      -2.787597918 B     0.37048599     -7.52     <.0001
 age2      3      -2.088127335 B     0.37292760     -5.60     <.0001
 age2      4      -1.353746449 B     0.38703805     -3.50     0.0005
 age2      5       0.000000000 B                                    
 sex*age2  F 1     0.813889663 B     0.42023749      1.94     0.0528
 sex*age2  F 2     0.716160958 B     0.42189464      1.70     0.0896
 sex*age2  F 3     0.330651265 B     0.42487846      0.78     0.4365
 sex*age2  F 4     0.313230835 B     0.44127621      0.71     0.4778
 sex*age2  F 5     0.000000000 B                                    
 sex*age2  M 1     0.000000000 B                                    
 sex*age2  M 2     0.000000000 B                                    
 sex*age2  M 3     0.000000000 B                                    
 sex*age2  M 4     0.000000000 B                                    
 sex*age2  M 5     0.000000000 B                                    
\end{verbatim}
\normalsize

The phreg printout for the additive  model with age and sex.
\small
\begin{verbatim}
                 Testing Global Null Hypothesis: BETA=0

         Test                 Chi-Square       DF     Pr > ChiSq

         Likelihood Ratio      2357.5239        5         <.0001
         Score                 3823.3905        5         <.0001
         Wald                  2374.5250        5         <.0001


                               Type 3 Tests

                                        Wald
                Effect      DF    Chi-Square    Pr > ChiSq

                sex          1       69.9646        <.0001
                age2         4     2374.5211        <.0001


            Analysis of Maximum Likelihood Estimates

                Parameter   Standard                        
Parameter   DF   Estimate      Error Chi-Square Pr > ChiSq  

sex       F  1   -0.36617    0.04378    69.9646     <.0001  
age2      1  1   -4.18209    0.12180  1179.0289     <.0001  
age2      2  1   -3.23859    0.11418   804.5068     <.0001  
age2      3  1   -2.17521    0.10963   393.6524     <.0001  
age2      4  1   -1.15226    0.11072   108.3077     <.0001  
\end{verbatim}
\normalsize

\small
\begin{verbatim}
                        Model Fit Statistics
 
                                Without           With
               Criterion     Covariates     Covariates

               -2 LOG L       37736.900      35374.050
               AIC            37736.900      35392.050
               SBC            37736.900      35443.188


               Testing Global Null Hypothesis: BETA=0
 
       Test                 Chi-Square       DF     Pr > ChiSq

       Likelihood Ratio      2362.8497        9         <.0001
       Score                 3873.5113        9         <.0001
       Wald                  2357.9498        9         <.0001


                             Type 3 Tests
 
                                   LR Statistics
             Effect        DF    Chi-Square    Pr > ChiSq

             sex            1        0.4607        0.4973
             age2           4      932.1371        <.0001
             sex*age2       4        5.3258        0.2555

 
                                 Score Statistics
             Effect        DF    Chi-Square    Pr > ChiSq

             sex            1        0.4757        0.4904
             age2           4     1506.8699        <.0001
             sex*age2       4        5.2516        0.2624

 
                                  Wald Statistics
             Effect        DF    Chi-Square    Pr > ChiSq

             sex            1        0.4833        0.4869
             age2           4      964.6007        <.0001
             sex*age2       4        5.2322        0.2643


               Analysis of Maximum Likelihood Estimates
 
                      Parameter     Standard
Parameter       DF     Estimate        Error   Chi-Square   

sex       F      1     -0.16537      0.23789       0.4833   
age2      1      1     -4.02699      0.22585     317.9171   
age2      2      1     -3.04796      0.21843     194.7187   
age2      3      1     -1.99577      0.21577      85.5504   
age2      4      1     -1.10659      0.22256      24.7216   
sex*age2  F 1    1     -0.21121      0.26896       0.6167   
sex*age2  F 2    1     -0.29334      0.25518       1.3214   
sex*age2  F 3    1     -0.25663      0.24829       1.0684   
sex*age2  F 4    1     -0.04339      0.25527       0.0289   


                 Contrast      DF    Chi-Square    Pr > ChiSq

                 NSTT sex       1        0.4833        0.4869
                 NSTT age       4      964.6007        <.0001

 
          Likelihood Ratio Statistics for Type 1 Analysis
 
                                                      LR
Source                    -2 Log L      DF    Chi-Square    Pr > ChiSq

(Without Covariates)    37736.8997                                    
sex                     37733.0932       1        3.8066        0.0511
age2                    35379.3758       4     2353.7173        <.0001
sex*age2                35374.0501       4        5.3258        0.2555

 
                              Standard
         Label    Estimate       Error    z Value    Pr > |z|

         Yates     -0.3263     0.06149      -5.31      <.0001
\end{verbatim}
\normalsize

\begin{thebibliography}{9}
  \bibitem{Aitkin78} M. Aitkin (1978).
    The analysis of unbalanced cross classifications (with discussion).
    \emph{J Royal Stat Soc A} 141:195-223.

  \bibitem{Dispenzieri12} A. Dispenzieri, J. Katzmann, R. Kyle, 
    D. Larson, T. Therneau, C. Colby,
         R. Clark, .G Mead, S. Kumar, 
         L..J Melton III and  S.V. Rajkumar (2012).
    Use of monoclonal serum immunoglobulin free light chains to predict 
            overall survival in the general population,
    \emph{Mayo Clinic Proc} 87:512--523.
    
  \bibitem{Herr86} D. G. Herr (1986).
   On the History of ANOVA in Unbalanced, Factorial Designs: The First 30 Years.
    \emph{Amer Statistician} 40:265-270. 
  
  \bibitem{Kyle06} R. Kyle, T. Therneau, S.V. Rajkumar,
            D. Larson, M. Plevak, J. Offord,
            A. Dispenzieri, J. Katzmann, and L.J. Melton, III (2006),
	    Prevalence of monoclonal gammopathy of 
              undetermined significance,
	    \emph{New England J Medicine} 354:1362--1369.
            
  \bibitem{Macnaughton92}
    D. B. Macnaughton (1992).  Which sum of squares are best in an
    unbalanced analysis of variance.  www.matstat.com/ss.
                 
  \bibitem{Nelder77} J. Nelder (1977).  A reformulation of linear models 
     (with discussion). 
     \emph{J Royal Stat Soc A} 140:48--76.
           
  \bibitem{SASguide} SAS Institute Inc. (2008), 
  The four types of estimable functions.  SAS/STAT 9.2 User's Guide,
    chapter 15.
   
   \bibitem{Searle71} S. R. Searle, \emph{Linear Models}, Wiley, New York, 1971.
   
   \bibitem{Senn1} S. Senn. Multi-centre trials and the finally decisive 
     argument. www.senns.demon.co.uk/wprose.html\#FDA. 

   \bibitem{Senn2} S. Senn. Good mixed centre practice.
          www.senns.demon.co.uk/wprose.html\#Mixed. 
  
  \bibitem{Therneau00} T. M. Therneau and P. M. Grambsch, \emph{Modeling Survival
    Data: Extending the Cox Model}, Springer-Verlag, New York, 2000.
    
  \bibitem{Yates34} F. Yates (1934). 
    The analysis of multiple classifications with
    unequal numbers in the different classes. \emph{J Am Stat Assoc},
    29:51--66.

\end{thebibliography}
\end{document}
