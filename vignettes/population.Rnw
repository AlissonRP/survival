\documentclass{article}[11pt]
\usepackage{Sweave}
\usepackage{amsmath}
\addtolength{\textwidth}{1in}
\addtolength{\oddsidemargin}{-.5in}
\setlength{\evensidemargin}{\oddsidemargin}
%\VignetteIndexEntry{Population contrasts}

\SweaveOpts{prefix.string=tests,width=6,height=4, keep.source=TRUE, fig=FALSE}
% Ross Ihaka suggestions
\DefineVerbatimEnvironment{Sinput}{Verbatim} {xleftmargin=2em}
\DefineVerbatimEnvironment{Soutput}{Verbatim}{xleftmargin=2em}
\DefineVerbatimEnvironment{Scode}{Verbatim}{xleftmargin=2em}
\fvset{listparameters={\setlength{\topsep}{0pt}}}
\renewenvironment{Schunk}{\vspace{\topsep}}{\vspace{\topsep}}

\SweaveOpts{width=6,height=4}
\setkeys{Gin}{width=\textwidth}

<<echo=FALSE>>=
options(continue="  ", width=60)
options(SweaveHooks=list(fig=function() par(mar=c(4.1, 4.1, .3, 1.1))))
pdf.options(pointsize=8) #text in graph about the same as regular text
options(contrasts=c("contr.treatment", "contr.poly")) #reset default
@ 

\title{Population contrasts}
\author{Terry M Therneau \\ \emph{Mayo Clinic}}
\newcommand{\code}[1]{\texttt{#1}}
\newcommand{\myfig}[1]{\includegraphics[height=!, width=\textwidth]
                        {tests-#1.pdf}}

\newcommand{\ybar}{\overline{y}}

\begin{document}
  \maketitle
  \tableofcontents

\section{Introduction}
The question of how best to summarize the effect of some particular
covariate, within a model that contains multiple others, is an old one.
It becomes particularly acute in the presence of interactions:
consider the hypothetical data shown in the figure below
comparing treatments A and B with age and sex as a confounders.
What value should be used to summarize the treatment effect?
One sensible approach is to select a fixed \emph{population} for the
age/sex distribution, and then compute the
average  effect over that population.

\begin{figure}
<<fig1, fig=TRUE>>=
plot(c(50,85), c(2,4.5), type='n', xlab="Age", ylab="Effect")
abline(.645, .042, lty=1, col=1)
abline(.9, .027, lty=1, col=2)
abline(.35, .045, lty=2, col=1)
abline(1.1, .026, lty=2, col=2)
legend(50, 4.2, c("Treatment A, female", "Treatment B, female", 
                  "Treatment A,  male", "Treatment B, male"),
       col=c(1,2,1,2), lty=c(1,1,2,2), bty='n')
@ 
 \caption{Treatment effects for a hypothetical study.}
 \label{fig1}
\end{figure}

More formally, assume we have some fitted model. Split the model 
predictors into two groups: $U$ and $V$, where $U$ is the covariate of 
interest (treatment in the example above) and $V$ is everything else.
Then a marginal estimate for treatment A is
\begin{equation*}
   m_A = E_F \,\hat y(u=A, V)
\end{equation*}
where $F$ is some population distribution for the covariates $V$.
Important follow-up questions are what population, what statistic,
the computational algorithm, and statistical properties of the resulting
estimate. 

Four common populations are
\begin{itemize}
  \item Empirical: the data set itself.  For the simple example above this
    would be the set of all $n$ age/sex pairs in the data set, 
    irrespective of treatment.
  \item Yates: this is only applicable if the adjusting variables $V$
    are all categorical, and consists of all unique combinations of $V$.
    That is, the data set one would envision for a balanced factorial 
    experiment.
  \item External: an external reference such as the age/sex distribution 
    of the US census.  This is common in epidemiolgy.
  \item SAS: a factorial (Yates) distribution for the categorical predictors 
    and the data distribution for the others. 
\end{itemize}

The \code{yates} function is designed to compute such population averages
from a fitted model, along with desired contrasts on the resultant 
estimates, e.g., that the population average effect for treatment 1 and
treatment 2 are equal. 
It has been tested with the results of lm, glm, and coxph fits, and should
work with any R model that includes a standard set of objects in the result:
(\code{terms}, \code{contrasts}, \code{xlevels}, \code{assign}, and \code{call}).

As the reader might already guess from the labels used just above, the
concept of population averages is a common one in statistics.
Taking an average is, after all, nearly the first thing a statistician
will do.
Yates' weighted means analysis, the g-estimates of causal models,
direct adjusted survival curves, and least squares means (SAS glm
procedure) are a small sample.  
This function's name is a nod to the oldest of these.

\section{Simple examples}
\subsection{Linear model}
The \code{solder} data set, used in the introduction of the
book Statistical Models
in S \ref{Chambers93} provides a simple starting example.
In 1988 an experiment was designed and implemented at one of AT&T's
factories to investigate alternatives in the "wave soldering" procedure
for mounting electronic componentes to printed circuit boards.
The experiment varied a number of factors relevant to the process.
The response, measured by eye, is the number of visible solder skips.

<<solder1, fig=TRUE>>=
summary(solder)
length(unique(solder$PadType))
# reproduce their figure 1.1
temp <- lapply(1:5, function(x) tapply(solder$skips, solder[[x]], mean))
plot(c(0,6), range(unlist(temp)), type='n', xaxt='n',
     xlab="Factors", ylab ="mean number of skips")

axis(1, 1:5, names(solder)[1:5])
for (i in 1:5) {
    y <- temp[[i]]
    x <- rep(i, length(y))
    text(x-.1, y, names(y), adj=1)
    segments(i, min(y), i, max(y))
    segments(x-.05, y, x+.05, y)
}
@ 
A perfectly balanced experiment would have 3*2*10*3 = 180 observations for
each Mask, corresponding to all combinations of opening, solder thickness,
pad type and panel.  
The A3 mask has extra replicates of the Large/Thick, Large/Thin, and Small/Thick
conditions, and A6 has only the Medium/Thick, Medium/Thin, and Small/Thin
conditions.
Essentially, one extra run of 180 was done with a mixture of masks.
Chambers and Hastie focus on the balanced subset that excludes this last
run, so their figure and results are slightly different.

Do a simple fit and then obtain the Yates predictions.
<<solder2>>=
fit1 <- lm(skips ~ Opening + Solder + Mask + PadType + Panel,
           data=solder)
y1 <- yates(fit1, "Opening", population = "factorial")
y1
@ 
The printout has two parts: the left hand colums are mean predicted
values, the right hand are tests on those predicted values.  
The default is a single global test that they are all equal.
(``Population predicted value'' could be an alternate label but the abbreviation
PPV would be confused with the positive predicted value.)
The estimates under a factorial population, \code{y1} above,
are the Yates' weighted
means \cite{Yates} and the corresponding test is the Yates' sum of
squares for that term.  These would be labeled as a 
``least squares mean'' and ``type III SS'', respectively,
by the SAS glm procedure.  
More on this last appears in the section on the SGTT algorithm.

Repeat this using the default population, which is the set of all 900
combinations for solder, mask, pad type and panel found in the data.
The pairwise option gives tests on all pairs of openings, for details
on this and other arguments see the \code{cmatrix} function;
``cm'' is a shorthand for cmatrix allowed within the \code{yates} argument list.
<<solder2b>>=
y2 <- yates(fit1, cm("Opening", test="pairwise"), population = "data") 
y2

temp <- rbind(diff(y1$estimate$mpv), diff(y2$estimate$mpv))
dimnames(temp) <- list(c("factorial", "emprical"), c("2 vs 1", "3 vs 2"))
round(temp,5)
@ 

Although the MPV values shift with the new population the global test is
unchanged, as is the difference in MPV between any two pairs.
This is because we have fit a model with no interactions.
Referring to figure 1 this is a model where all of the predictions are
parallel lines; shifting the population left or
right will change each average but has no effect on the difference between two
lines.
For a linear model with no iteractions the test statistics created by the
\code{yates} function are thus not very interesting, since they will be no 
different than simple tests on the model coefficients.

Here are results from a more interesting fit that includes interactions.
<<solder3>>=
fit2 <- lm(skips ~ Opening + Mask*PadType + Panel, solder)
yates(fit2, "Mask")
@ 
Because of the near perfect balance some of the mean have not moved at
all, however.

\subsection{Missing cells}
Models that involve factors and interactions can have an issue with
missing cells as shown by the example below.
<<solder3>>=
fit3 <- lm(skips ~ Opening * Mask + Solder + PadType + Panel, solder)
yates(fit3, cm("Mask", test="pairwise"))
@ 
The population predictions for each Mask include all combinations of
Opening, Solder, PadType, and Panel that are found in the data.
In the above call the empirical population was used, and the
underlying algorithm amounts to
\begin{enumerate}
  \item Make a copy of the data set (900 obs), and set Mask to A1.5 in
    all observations
  \item Get the 900 resultant predicted values from the model, and take their
    average
  \item Repeat 1 and 2 for each mask type.
\end{itemize}
However, there were no observations in the data set with Mask = A6 and 
opening = Large, and hence relevant coefficients in \code{fit2} are NA.
Formally, any predictions for an A6/Large combination are \emph{not estimable}, 
and as a consequence
neither are any population averages that include them, 
nor any tests that involve those population averages.

If you do the above steps 'by hand' R gives a value for all 900 predictions,
but these are unreliable. 
The presence of a missing value in \code{coef(fit3)} shows that some preditions
will not be estimable, but it is not possible to determine \emph{which}
ones from the
coefficients alone; hence the warning message from \code{predict}.
A formal definition of estimability is that prediction can be written as a 
linear combination of the rows of $X$, the design matrix for the fit.
The \code{yates} function performs necessary calculations to verify formal
estimability of each predicted value.

<<estimable>>=
table(is.na(coef(fit3)))
test <- solder
test$Mask <- "A1.5"
pp <- predict(fit3, newdata=test)
table(is.na(pp))
@ 

\subsection{Cox models}
For a Cox model an immediate question is what to predict.



\section{Population}
The \code{population} parameter parameter of the call can be either a data set
or one of `data', `factorial', `sas', or `none'. 
Alternate labels of `empirical' and `Yates' are allowed for the `data' and
`factorial' options, respectively.
If \code{population} is a data set then it must contain all variables found
in the set of adjusters $V$.  
Using the original data set is equivalent to `data'.

The 'right' data set to use is entirely a function of what question you
want to answer.
Consider the simple example of figure 1.  
If, for instance, one were contemplating effective drugs for a nursing home
population, then a population of ages shifted towards the right hand side of 
the x-axis would be the obvious target, and one would most likely use an
external reference to define the population.  
No population is incorrect in the sense that each addresses a well defined
question; but is is a question that anyone would ask?
The factorial population in particular has been overused.

\section{SAS glim type III (SGTT) algorithm}
``Type 3'' tests have been a statistical bugaboo for decades.  
A major problem is that almost no one knows exactly what they are.  The
generalized linear models (GLM) procedure uses a hybrid algorithm.
For least squares means:
\begin{itemize}
  \item Define a mixture population: factorial for categorical variables;
    those that appear in a class statement, and the data distribution for
    all others.
  \item Assume a data set which had 2 categorical variables x1 and x2 with 3 and 4 
    levels, respectively, 4 other continuous variables, and 47 observations.
    The population data set has 3 * 4 * 47 = 564 observations.
  \item The three least squares means for variable x1 can be obtained by
    taking a copy of the population data set, set x1 to its first level in
    all 564 rows, and compute the mean predicted value, and likewise for the
    second and third levels of x1.  
    This is the same estimate as the \code{yates} function.  
  \item There many ways to shorten and/or speed up this calculation of course,
    but that is not material.
\end{itemize}

The type III SS is \emph{not} a direct test of equality of these mpv values,
and is in fact defined even when some or all of them are not estimable.
\begin{enumerate}
  \item Create a full (overdetermined) design matrix $X$ from left to right:
    the intercept, then main effects, then two variable interactions
    (if any), three variable, etc.  
    \begin{itemize}
      \item A categorical variable with $k$ levels is
        represented by $k$ 0/1 dummy variables 
        which represent the the first, second, etc. levels of the variable.
      \item Likewise the interaction between two categorical variables 
        that have $k$
        and $l$ levels will be represented by $kl$ 0/1 columns, and etc. 
      \item R's \code{model.matrix} function returns far fewer columns, since
        it tries to prospectively remove those that will be redundant.
        Having all the columns is critical to step 3, however.
  \item Create the $p$ by $p$ dependency matrix $D$ from the $n$ by $p$
    matrix $X$, from left to right. 
    \begin{itemize}
      \item If column $k$ of $X$ can be written as a linear combination of
        prior columns, then $D_{1k}$ to $D_{k-1,k}$ contains that combination,
        and $D_{j,k}=0$ for $j \ge k$.
      \item Otherwise set $D_{kk}=1$ and $D_{jk}=0$ for $j ne k$.
      \item Note that if the $i$th column of $X$ is linearly dependent on 
        prior columns, then the $i$th row of $D$ will be zero.
    \end{itemize}
  \item Partially orthagonalize $D'$ from right to left
    \begin{itemize}
      \item For any column of $D'$ that corresponds to an interaction of one
        or more categorical predictors (but only categorical), make any
        columns of $D'$ that correspond to a contained term orthagonal to it.
      \item If x1 and x2 were categorical, for instance, and the model contains
        the x1*x2 interaction, then any columns of $D'$ corresponding to
        x1 only or x2 only will be made orthagonal to the x1*x2 columns of $D'$.
      \item An x1*x2*x3 term, however, where x3 is continuous, would not 
        participate in this process.  
    \end{itemize}
  \item The resulting rows of $D$ form the type III contrasts, i.e.,
    tests of $D \beta =0$ for the corresponding rows of $D$.
\end{enumerate}

If all covariates are categorical and all of the mpv values are estimable 
then the above algorithm will agree with the Yates tests.
This is easy to verify numerically for any given example,
but much more challenging to prove as a general case.  
If not all the mpv values are estimable a Yates SS is not defined and
the SGTT is something quite different.
However, because it is based on subspace and orthagonality arguments, the
SS is unaffected by the choice of the reference level for a class
variable or by the order in which the terms are added to the $X$ matrix.

Another way to derive the Yates estimates is based on
the well known fact that in a balanced factorial design the sums of squares
for main effects and interactions are all orthagonal.  Starting with any
$X$ matrix for the fit, e.g., the full $X$ given above or the R version
under any of the constrast options, first reduce it to a balanced subset
\code{Z =unique(X)} and then find contrast matrices C1, C2, etc. that
are orgthagonal with respect to $(Z'Z)^{-1}$,
the variance matrix of the coefficients.  
The SGTT appears to be based on this idea.

Note that the SGTT algorithm is the SAS \emph{glm} type 3 procedure. 
Several other SAS procedures also create output labeled as ``type 3'' which
is not necessarily the same. 
The phreg procedure uses the NSTT computation for instance, and we have found
others that are not invariant to the choice of the reference level for a 
factor.

