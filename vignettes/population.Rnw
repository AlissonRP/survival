\documentclass{article}[11pt]
\usepackage{Sweave}
\usepackage{amsmath}
\addtolength{\textwidth}{1in}
\addtolength{\oddsidemargin}{-.5in}
\setlength{\evensidemargin}{\oddsidemargin}
%\VignetteIndexEntry{Population contrasts}

\SweaveOpts{prefix.string=tests,width=6,height=4, keep.source=TRUE, fig=FALSE}
% Ross Ihaka suggestions
\DefineVerbatimEnvironment{Sinput}{Verbatim} {xleftmargin=2em}
\DefineVerbatimEnvironment{Soutput}{Verbatim}{xleftmargin=2em}
\DefineVerbatimEnvironment{Scode}{Verbatim}{xleftmargin=2em}
\fvset{listparameters={\setlength{\topsep}{0pt}}}
\renewenvironment{Schunk}{\vspace{\topsep}}{\vspace{\topsep}}

\SweaveOpts{width=6,height=4}
\setkeys{Gin}{width=\textwidth}

<<echo=FALSE>>=
options(continue="  ", width=70)
options(SweaveHooks=list(fig=function() par(mar=c(4.1, 4.1, .3, 1.1))))
pdf.options(pointsize=8) #text in graph about the same as regular text
options(contrasts=c("contr.treatment", "contr.poly")) #reset default
library(survival)
library(splines)
@ 

\title{Population contrasts}
\author{Terry M Therneau \\ \emph{Mayo Clinic}}
\newcommand{\code}[1]{\texttt{#1}}
\newcommand{\myfig}[1]{\includegraphics[height=!, width=\textwidth]
                        {tests-#1.pdf}}

\newcommand{\ybar}{\overline{y}}

\begin{document}
  \maketitle
\  \tableofcontents

\section{Introduction}
Statisticians and their clients have always been fond of finding a single
number summarizing an effect in data (perhaps too much so in fact).
Consider the hypothetical data shown in figure \ref{fig1}
comparing treatments A and B with age as a confounder.
What is a useful single number summary of the difference between 
treatment arms A and B?
One approach is to select a fixed \emph{population} for the
age distribution, and then compute the
mean effect over that population.

\begin{figure}
<<fig1, echo=FALSE, fig=TRUE>>=
plot(c(50,85), c(2,4.5), type='n', xlab="Age", ylab="Effect")
#abline(.645, .042, lty=1, col=1, lwd=2)
#abline(.9, .027, lty=1, col=2, lwd=2)
abline(.35, .045, lty=1, col=1, lwd=2)
abline(1.1, .026, lty=1, col=2, lwd=2)
legend(50, 4.2, c("Treatment A", "Treatment B"), 
        col=c(1,2), lty=1, lwd=2, cex=1.3, bty='n')
@ 
 \caption{Treatment effects for a hypothetical study.}
 \label{fig1}
\end{figure}

More formally, assume we have a fitted model.
We want to compute the conditional expectation
\begin{equation*}
   m_A = E_F\left( \hat y | trt=A \right)
\end{equation*}
where $F$ is some chosen population for the covariates other than treatment.
Important follow-up questions are: what population should be used, 
what statistic should be averaged,
the compuational algorithm, and statistical properties of the resulting
estimate.
Neither the statistic nor population questions should be taken lightly
and need to be closely linked to the scientific question.  
If, for instance, the model of figure 1 were used to inform a nursing home
formulary, then the distribution $F$ might be focused on higher ages.

Four common populations are

\begin{itemize}
  \item Empirical: The data set itself.  For the simple example above, this
    would be the distribution of all $n$ ages in the data set, 
    irrespective of treatment.
  \item Factorial or Yates: This is only applicable if the adjusting 
    variables
    are all categorical, and consists of all unique combinations of them.
    That is, the data set one would envision for a balanced factorial 
    experiment.
  \item External: An external reference such as the age/sex distribution 
    of the US census.  This is common in epidemiology.
  \item SAS type 3: A factorial distribution for the categorical predictors 
    and the data distribution for the others. 
\end{itemize}

The \code{yates} function is designed to compute such population averages
from a fitted model, along with desired contrasts on the resultant 
estimates, e.g., whether the population average effects for treatment A and
treatment B are equal. 
The function has been tested with the results of \code{lm}, \code{glm}, and \code{coxph} fits,
and can easily be extended to any R model that includes a standard set of
objects in the result, i.e., 
\code{terms}, \code{contrasts}, \code{xlevels}, and \code{assign}.

The routine's name is a nod to the 1934 paper by Yates \ref{Yate34} 
\emph{The analysis of multiple classifications with unequal numbers in
  different classes}.
In dealing with an unbalanced 2-way layout he states that 
\begin{quote}\ldots in the absence of any further
assumptions the efficient estimates of the average A effects 
are obtained by taking the means
of the observed sub-class means for like A over all B sub-classes.
\end{quote}
In his example these sub-class means are the predicted values for each
A*B combination, thus his estimate for each level of A is the
mean predicted value over B, i.e., the mean prediction for A over
a factorial population for B.
Yates then develops
formulas for calculating these quantities and testing their
equality that are practical for the manual methods of the time;
these are now easily accomplished directly using matrix computations.
(It is interesting that Yates' paper focuses far more on estimands than
tests, while later references to his work focus almost exclusively
on the latter, e.g., the ``Yates sum of squares''.)

This
concept of population averages is actually a common one in statistics.
Taking an average is, after all, nearly the first thing a statistician
will do.
Yates' weighted means analysis, the g-estimates of causal models,
direct adjusted survival curves, and least squares means are but a 
small sample of the idea's continual rediscovery.  


\section{Solder Example}
\subsection{Data}
In 1988 an experiment was designed and implemented at one of AT\&T's
factories to investigate alternatives in the wave soldering procedure
for mounting electronic components to printed circuit boards.
The experiment varied a number of factors relevant to the process.
The response, measured by eye, is the number of visible solder skips.
The data set was used in the
book Statistical Models in S \ref{Chambers93} and is included in the
survival package.

<<solder1a>>=
summary(solder)
length(unique(solder$PadType))
@ 

A perfectly balanced experiment would have 3*2*10*3 = 180 observations for
each Mask, corresponding to all combinations of Opening, Solder Thickness,
PadType and Panel.  
The A3 Mask has extra replicates for a subset of the Opening*Thickness
combinations, however, while Mask A6 lacks observations for these sets.
Essentially, one extra run of 180 was done with a mixture of Masks.
Figure \ref{fig:solder} gives an overview of univariate results for
each factor.

\begin{figure} 
<<solder1b, fig=TRUE, echo=FALSE>>=
# reproduce their figure 1.1
temp <- lapply(1:5, function(x) tapply(solder$skips, solder[[x]], mean))
plot(c(0.5, 5.5), range(unlist(temp)), type='n', xaxt='n',
     xlab="Factors", ylab ="mean number of skips")

axis(1, 1:5, names(solder)[1:5])
for (i in 1:5) {
    y <- temp[[i]]
    x <- rep(i, length(y))
    text(x-.1, y, names(y), adj=1)
    segments(i, min(y), i, max(y))
    segments(x-.05, y, x+.05, y)
}
@ 
\caption{Overview of the solder data.}
\label{fig:solder}
\end{figure}

\subsection{Linear model}
A subset of the solder data that excludes Mask A6 is exactly the type
of data set considered in Yates' paper: a factorial design whose data set
is not quite balanced. 
Start with a simple fit and then obtain the Yates predictions.

<<solder2>>=
with(solder[solder$Mask!='A6',], ftable(Opening, Solder, PadType))

fit1 <- lm(skips ~ Opening + Solder + Mask + PadType + Panel,
           data=solder, subset= (Mask != 'A6'))
y1 <- yates(fit1, ~Opening, population = "factorial")
print(y1, digits=2)  # (less digits, for vignette page width)
@ 

The printout has two parts: the left hand columns are Mean Population Predicted
Values (MPPV) and the right hand columns are tests on those predicted values.  
The default is a single global test that these MPPV are all equal.
Under a factorial population these are the Yates' weighted
means \cite{Yates} and the corresponding test is the Yates' sum of
squares for that term.  These would be labeled as
``least squares means'' and ``type III SS'', respectively,
by the glm procedure of SAS. 
More on this correspondence appears in the section on the SGTT algorithm.

Repeat this using the default population, which is the set of all 810
combinations for Solder, Mask, PadType and Panel found in the non-A6 data.
The pairwise option requests tests on all pairs of openings.
<<solder2b>>=
y2 <- yates(fit1, "Opening", population = "data", test="pairwise") 
print(y2, digits=2)
#
# compare the two results
temp <- rbind(diff(y1$estimate$mppv), diff(y2$estimate$mppv))
dimnames(temp) <- list(c("factorial", "empirical"), c("2 vs 1", "3 vs 2"))
round(temp,5)
@ 

Although the MPPV values shift with the new population, the difference in
MPPV values between any two pairs is unchanged.
This is because we have fit a model with no interactions.
Referring to figure 1 this is a model where all of the predictions are
parallel lines; shifting the population left or
right will change the MPPV  but has no effect on the difference between two
lines.
For a linear model with no iteractions the test statistics created by the
\code{yates} function are thus not very interesting, since they will be no 
different than simple comparisons of the model coefficients.

Here are results from a more interesting fit that includes interactions.

<<solder3>>=
fit2 <- lm(skips ~ Opening + Mask*PadType + Panel, solder,
           subset= (Mask != "A6"))
temp <- yates(fit2, ~Opening, population="factorial")
print(temp, digits=2)
@ 
This solder data set is close to being balanced,
and the means change only a small amount.
(One hallmark of a completely balanced experiment is that any 
MPPV values are unaffected by the addition of interaction terms.)

\subsection{Missing cells}
Models that involve factors and interactions can have an issue with
missing cells as shown by the example below using the full 
version of the solder data.
<<solder4>>=
fit3 <- lm(skips ~ Opening * Mask + Solder + PadType + Panel, solder)
temp <- yates(fit3, ~Mask, test="pairwise")
print(temp, digits=2)
@ 

The population predictions for each Mask include all combinations of
Opening, Solder, PadType, and Panel that are found in the data.
The call implicity uses the default value of \code{population='data'},
which is the option that users will almost always select, as nearly
balanced experiments like the solder data are rare.
The underlying algorithm amounts to:
\begin{enumerate}
  \item Make a copy of the data set (900 obs), and set Mask to A1.5 for
    all observations
  \item Get the 900 resulting predicted values from the model, and take their
    average
  \item Repeat 1 and 2 for each mask type.
\end{enumerate}

There is a problem, however, it that there were no observations in the data 
set with Mask = A6 and Opening = Large.
Formally, predictions for the A6/Large combination are \emph{not estimable}, 
and as a consequence
neither are any population averages that include those predicted values, 
nor any tests that involve those population averages.
This lack of estimability is entirely due to the inclusion of a Mask by Opening
interaction term in the model, which states that each Mask/Opening combination
has a unique effect, which in turn implies that we need an estimate for all
Mask*Opening pairs to compute a population prediction.

If you do the above steps `by hand' using the R \code{predict} function, 
it will return a value for all 900 observations
along with a warning message that the results may not be reliable,
and the warning is correct in this case.
The result of \code{coef(fit2)} reveals that the fit generated an NA 
for the coefficient.
The presence of a missing value shows that some preditions
will not be estimable, but it is not possible to determine \emph{which}
ones are estimable from the coefficients alone.
The predict function knows that some predictions will be wrong but not
which ones.
A formal definition of estimability for a given prediction is that it 
can be written 
as a linear combination of the rows of $X$, the design matrix for the fit.
The \code{yates} function performs the necessary calculations to verify formal
estimability of each predicted value and thus is able to correctly identify the 
deficient terms.

\section{Generalized linear models}
\label{sect:glm}
Since the solder response is a count of the number of skips, Poisson
regression is the more natural modeling approach than linear regression.
In a glm model we need to consider more carefully both the population
and the statistic to be averaged.

<<glm>>=
gfit2 <- glm(skips ~ Opening * Mask + PadType + Solder, data=solder,
             family=poisson)
yates(gfit2, ~ Mask, predict = "link") 
yates(gfit2, ~ Mask, predict = "response")  
@ 

Mean predicted values for the number of skips using
\code{type='response'} in \code{predict.glm} are similar to estimates when the linear regression model was used.
Prediction of type 'link' yields a population average of the
linear predictor $X\beta$.
Though perfectly legal (you can take the mean of anything you want)
these MPPV values can more difficult to interpret.
The default link function for poisson regression is log(), so the MPPV 
is a mean of the log predicted value.
Since exp(mean(log(x)) defines the geometric mean of $x$, 
one can view the exponentiated version of the link estimate as 
a geometric mean of predicted values over the population.
Given the skewness of the observed skip counts, an argument can
be made for using the geometric mean. 
It's value and variance variance can be approximated
using the usual delta method per the code below.
Tests of the difference between Masks would remain unchanged.

<<glm2>>=
# post-process the link
ytemp <- yates(gfit2, ~ Mask, predict="link")
ytemp$estimate$mppv <- exp(ytemp$estimate$mppv)
ytemp$estimate$std <- ytemp$estimate$std * ytemp$estimate$mppv
ytemp
@ 

For nonlinear predictors such as the response, the population choice
matters even for an additive model.  The two results
below have different estimates for the "between MPPV differences"
and tests. 

<<glm3>>=
gfit1 <- glm(skips ~ Opening + Mask +PadType + Solder, data=solder,
             family=poisson)
yates(gfit1, ~ Opening, test="pairwise", predict = "response", population='data')
yates(gfit1, ~ Opening, test="pairwise", predict = "response", 
      population="yates")
@ 


\section{Free Light Chain}
In 2012, Dispenzieri and colleages  examined the
distribution and consequences of a the free light chain value,
a laboratory test, on a large fraction of the 1995
population of Olmsted County, Minnesota aged 50 or older
\cite{Kyle06, Dispenzieri12}.
The R data set \code{flchain} contains a 50\% random sample of this larger study
and is included as a part of the survival package.
The primary purpose of the study was to measure the amount of
plasma immunoglobulin and its components.  
Intact immunoglobulins are composed of a heavy chain and light chain
portion.  In normal subjects there is overproduction of the light chain 
component by the immune cells leading to a small amount of 
\emph{free light chain}  in the circulation.
Excessive amounts of free light chain (FLC) are thought to be a marker of
disregulation in the immune system.
An important medical question is whether high levels of FLC have an
impact on survival, which will be explored using a Cox model.  
Free light chains have two major forms denoted as kappa and lambda;
we will use the sum of the two.

A confounding factor is that free light chain values rise with age, in
part because it is eliminated by the kidneys and renal function
declines with age.
The age distribution of males and females differs, so we
will adjust any comparisons for both age and sex.
The impact of age on mortality is dominatingly large
and so correction for the age imbalance is critical when exploring
the impact of FLC on survival.

Figure \ref{fig:data} shows the trend in free light chain values
as a function of age.
For illustration of linear models using factors, we have also
created a categorical age value using deciles of age.

\begin{figure}
<<data, fig=TRUE, echo=FALSE>>=
male <- (flchain$sex=='M')
flchain$flc <- flchain$kappa + flchain$lambda
mlow <- with(flchain[male,],  smooth.spline(age, flc))
flow <- with(flchain[!male,], smooth.spline(age, flc))
plot(flow, type='l', ylim=range(flow$y, mlow$y),
     xlab="Age", ylab="FLC")
lines(mlow, col=2)
@
\caption{Free light chain values as a function of age.}
\label{fig:flc}
\end{figure}

The table of counts shows that the sex distribution becomes increasingly
unbalanced at the older ages, from about 1/2 females in the youngest
group to a 4:1 ratio in the oldest.
<<counts>>=
flchain$flc <- flchain$kappa + flchain$lambda                    
age2 <- cut(flchain$age, c(49, 59, 69, 79, 89, 120),                   
            labels=c("50-59", "60-69", "70-79", "80-89", "90+"))
fgroup <- cut(flchain$flc, quantile(flchain$flc, c(0, .5, .75, .9, 1)),
              include.lowest=TRUE, labels=c("<50", "50-75", "75-90", ">90"))
counts <- with(flchain, table(sex, age2))
counts
#
# Mean counts in each age/sex group
cellmean <- with(flchain, tapply(flc, list(sex, age2), mean))
round(cellmean,1)                 
@ 

Notice that the male/female difference in FLC varies with age, 
\Sexpr{round(cellmean[1,1],1)} versus \Sexpr{round(cellmean[2,1],1)}
at age 50--59 and \Sexpr{round(cellmean[1,5],1)} versus
 \Sexpr{round(cellmean[2,5],1)} at age 90,
and as shown in figure \ref{fig:flc}.
The data does not fit a simple additive model; there are ``interactions''
to use statistical parlance.
Men and women simply do not age in quite the same way.

\subsection{Linear models}
Compare the mean FLC for males to females, with and without adjusting for
age.
<<flc1>>=
library(splines)
flc1 <- lm(flc ~ sex, flchain)
flc2a <- lm(flc ~ sex + ns(age, 3), flchain)
flc2b <- lm(flc ~ sex + age2, flchain)
flc3a <- lm(flc ~ sex * ns(age, 3), flchain)
flc3b <- lm(flc ~ sex * age2, flchain)
# predict near to the mean age
tdata <- data.frame(sex=c("F", "M"), age=65, age2="60-69")
temp <- rbind("unadjusted" = predict(flc1, tdata),
              "additive, continuous age" = predict(flc2a, tdata),
              "additive, discrete age"   = predict(flc2b, tdata),
              "interaction, cont age"    = predict(flc3a, tdata),
              "interaction, discrete"    = predict(flc3b, tdata))
colnames(temp) <- c("Female", "Male")
round(temp,2)
@ 

The between gender difference is underestimated without adjustment for
age.  The females are over-represented at the high ages, which inflates
their naive average.  Continuous and categorical age adjustment is similar in
this particular data set.  
Now look at population adjustment.

<<flc2>>=
yates(flc3a, ~sex)  # population = data is the default
yates(flc3b, ~sex) 
yates(flc3b, ~sex, population="factorial")
@ 

The population average values for the empirical distribution are just a
bit higher then the prediction at the mean due to the upward curvature
of the age vs FLC curve. 
The average for a factorial population jumps up even more.
This is because it is the average for an unusual population which has
as many 90+ year old subjects as 50--59 year old; i.e., 
it is the correct answer to a rather odd question.

We can also look at the age effect after adjusting for sex.  For the
continuous model the age values of interest for the MPPV need to be added using
the \code{levels} argument.
With a factor the routine can guess that you want all the levels.
<<flc3>>=
yates(flc3a, ~ age, levels=c(65, 75, 85))  
yates(flc3b, ~ age2)
@ 

\section{Cox Models}
Finally we come to Cox models which are, after all, the point of this
vignette.
Here the question of what to predict is more serious.
To get a feel for the data look at three simple models.

<<cox1>>=
options(show.signif.stars=FALSE)  # show statistical intelligence
coxfit1 <- coxph(Surv(futime, death) ~ sex, flchain)
coxfit2 <- coxph(Surv(futime, death) ~ sex + age, flchain)
coxfit3 <- coxph(Surv(futime, death) ~ sex * age, flchain)
anova(coxfit1, coxfit2, coxfit3)
#
exp(c(coef(coxfit1), coef(coxfit2)[1]))  # sex estimate without and with age
@ 

The model with an age*sex interaction does not fit substantially better
than the additive model.  
This is not a surprise as the US and Minnesota death rate curves for males and
females are nearly parallel after age 50.  
The sex coefficients for models 1 and 2 differ substantially.  
Males in this data set have almost 1.5 the death rate of females at any
given age, but when age is ignored the fact that females dominate the
oldest ages almost completely cancels this out.
Adjustment for both age and sex is critical for understanding the potential
effect of FLC on survival.

Dispenzieri \cite{Dispenzierixx} looked at the impact of FLC by dividing the
sample into those above and below the 90th percentile of FLC;
for illustration we will use 4 groups consisting of the lowest 50\%,
50 to 75th percentile, 75 to 90th and above 90th.

<<coxfit2>>=
coxfit4 <- coxph(Surv(futime, death) ~ fgroup*age + sex, flchain)
yates(coxfit4, ~ fgroup, predict="linear")
yates(coxfit4, ~ fgroup, predict="risk")
@ 
We see that after adjustment for age and sex, FLC is a strong
predictor of survival.
Since the Cox model is a model of relative risk any constant term is
arbitrary: one could add 100 to all of the log rates (type `linear' above)
and have as valid an answer.  To keep the coefficients on a sensible scale,
the \code{yates} function centers the mean linear predictor original data
at zero.  This centers the linear predictor at 0 but 
does not precisely center the risks at exp(0) = 1 due to
Jensen's inequality, but suffices to keep the values in a sensible range
for display.
 
A similar argument to that found in section \ref{sect:glm} about the arithmetic
versus geometric mean can be made here, but a more fundamental issue is that
the overall hazard function for a population is not the average of the
hazards for each of its members, and in fact will change over time as the
higher risk members of the population die.  
Though computable there is no natural feel for what either a mean r
hazard ratio or a mean log hazard ratio actually represent.

Survival curves, however, do lead to a proper average: 
the survival curve of a population is the
mean of the individual survival curves of its member.
Functions computed from the survival curve such as a mean or median time
until event will also be proper and interpretable.
Here is the overall survival for the group.  
The longest death time is  at
\Sexpr{round(with(flchain, max(futime[death==1])/365.25),1)} years, we
will use a restricted mean survival at a threshold of 13 years.
<<surv>>=
# longest time to death
round(max(flchain$futime[flchain$death==1]) / 365.25, 1)
#compute naive survival curve
flkm <- survfit(Surv(futime, death) ~ fgroup, data=flchain)
print(flkm, rmean= 13*365.25)
@ 

Straightforward survival prediction takes longer than recommended for a CRAN
vignette: there are \Sexpr{nrow(flchain)} subjects in the study and
4 FLC groups, which leads to just over 30 thousand predicted survival curves
when using the default \code{population='data'}, and each curve
has over 2000 time points (the number of unique death times).
To compute a variance
this is then repeated the default \code{nsim = 1000} times.  
We can use this as an opportunity to demonstrate a user supplied population,
however, which is any data set containing a population of 
values for the control variables.
We'll use every 20th observation in the flchain data as the population
and also reduce the number of simulations to limit the run time.
<<surv2>>=
popdata <- flchain[seq(1, nrow(flchain), by=20), c("age", "sex")]
ysurv <- yates(coxfit4, ~fgroup, predict="survival", nsim=50,
               population = popdata, 
               options=list(rmean=365.25*13))
ysurv

# display side by side
temp <- rbind("simple KM" = summary(flkm, rmean=13*365.25)$table[, "*rmean"],
              "population adjusted" = ysurv$estimate$mppv)
round(temp/365.25, 2)

@ 
The spread in restricted mean values is considerably less in the marginal
 (MPPV) survival curves from the yates function,
with the biggest change for those above the 90th percentile of FLC.
The \code{ysurv} object can also contain an optional summary component, which
in this case is the set of 4 MPPV survival curves.
Plot these along with the unadjusted curves, with solid lines for the
MPPV estimates and dashed for the unadjusted curves.
This shows the difference between adjusted and unadjusted even more clearly.
(But is not a 1 number summary with a simple p value ;-).
<<surv3, fig=TRUE>>=
plot(flkm, xscale=365.25, fun="event", col=1:4, lty=2,
     xlab="Years", ylab="Death")
lines(ysurv$summary, fun="event", col=1:4, lty=1, lwd=2)
legend(0, .65, levels(fgroup), lty=1, lwd=2, col=1:4, bty='n')
@ 


\section{Mathematics}
The underlying code uses a simple brute force algorithm.  It first builds a
population data set for the control variables that includes a placeholder
for the variable of interest.
Then for each level of variable(s) of interest: place that value in all rows
of the data, call the model.matrix routine to compute an $X$ matrix, compute
the prediction for all rows, and take the mean prediction.

When the prediction is the simple linear predictor $X\beta$ we take advantage
of the fact that $\mbox{rm mean}(X \beta) = [\mbox{column means}(X)] \beta = c\beta$.
If $C$ is the matrix of said column means, one row for each of the groups
of interest, the $C\beta$ is the vector of MPPV values and $CVC'$ is the
variance covariance matrix of those values, where $V$ is the variance matrix
of $\hat \beta$. 
The lion's share of the work is building the individual $X$ matrices and that
is unchanged.

For other than the linear case the variance is obtained by a short simulation.
Assuming that $\beta \sim N(\hat\beta, V)$, \code{nsim} independent $\hat \beta$
vectors are created, the MPPV values are computed for each, and an empirical
variance matrix for the MPPV values is then computed.

\section{SAS glim type III (SGTT) algorithm}
Earlier in this document reference was made to the SAS ``type 3'' estimates,
and we now delve into that topic.
It is placed at the end because it is in many ways a side issue with
respect to population averages. 
However,  whatever ones opinion on the wisdom
or folly of the SAS estimator, one cannot ignore its ubiquity, and showing how
it fits into this framework is an important part of the picture.

\subsection{SGTT}
The SAS GLM procedure prints out two statistics that are relevant to the
discussion: least squares means (LSM) and type 3 tests.
The LSM values are identical to the \code{yates} MPPV estimates, computed
for a mixture population that uses a factorial for categorical variables, i.e.,
those that appear in a class statement, and the data distribution for
all others.   
For instance if the data set had $n$ rows, some continuous
predictors, and two categorical adjusters with 3 and 4 levels, respectively,
then the population data set would have $12 n$ rows.
If the LSM involves non-estimable terms the result will be missing,
just as with the \code{yates} function.

The Yates sums of squares associated with tests for equality of the MPPV
values has many computational variants.
One very interesting form is based on the fact that in a balanced factorial
linear model the tests for all the effects are orthagonal;
a dataset and model where all predictors are categorical and all 
combinations appear the same number of times.
In the model \code{lm(y ~ x1 + x2 + x1:x2)}, for instance, the sequential 
SS for x1, as obtained from an R \code{anova} command,
will be the same whether x1 preceeds or follows after x2 in the
formula. 
Assume that $C$, $D$ and $E$ are the the relevant contrast matrices
for the three effect, i.e., such that the test for $C\beta=0$ corresponds
to a test for x1, $D$ for x2, and $E$ for x1:x2.
Then orthagonality implies that the correlation between effects is zero,
i.e, $CVD' =0$, $CVE'=0$ and $DVE' =0$ where $V= \sigma^2(X'X)^-$ is the
variance-covariance of the coefficients.

Some statisticians see this orthagonality of the tests an interesting aside 
while others view it as a central aspect of the estimation,
one which should be emulated in other models whenever possible.
The SAS type 3 computation is based on this orthagonality principle, namely
choose contrasts that are orthagonal with respect to the $(X'X)$ matrix of
a \emph{balanced subset} of the data.
In one of the early SAS technical report Goodnight writes 
\begin{quote} For most unbalanced designs it is usually possible to test the 
  same set of hypotheses (estimable functions) that would have been tested if
  the design had been balanced.  For those designs which started out balanced,
  but for which observations were lost due to external forces, there is no
  reason to alter the hypothesis \cite{Goodnight78}. 
\end{quote}

This leads to the following simple algorithm for computing Yates contrasts in an
unbalanced design.
\begin{enumerate}
  \item Let $Z$ be the design matrix for a balanced subset of the the data; 
    i.e., the factorial population of the \code{yates} command.
    One simple way to construct $Z$, if there are no missing cells, as the set
    of unique rows of $X$.  
  \item Choose contrasts that are orthagonal with respect to $(Z'Z)^-$.
    \begin{itemize}
      \item One set comes from the generalized Cholesky decomposition
        $LDL' = Z'Z$ where $L$ is lower triangular with diagonal elements of 1,
        $D$ is diagonal, and $D_{ii}=0$ if the $i$th column of $Z$ is a linear
        combination of prior columns and positive otherwise.  Use $L'$ as the
        contrast matrix.
      \item $L'(Z'Z)^-L = D^-$, a diagonal matrix, so $L$ clearly fulfills the
        orthagonality requirement.  
      \item It is also the case that off diagonal elements of $L$ will be 0
        unless sharing a common term, e.g., $L_{ij}=0$ if row $i$ is associated
        with $x_1$ and row $j$ with $x_2$, but not if row $j$ were associated
        with the $x_1:x_2$ interaction.
    \end{itemize}
\end{enumerate}

If there are both continuous and categorical variables in the $X$ matrix
then step 1 above assigns uniqueness based only on categorical columns.
(The continuous portion of $Z'Z$ need not even be computed since it is never
used.)
The interaction of a continuous and a categorical covariate is considered 
to be in the set of continuous variables.

What if not all of the population values are estimable, such as the A6/Large
combination in the solder data?  
Then the ``keep unique rows'' step of the above does not lead to the balanced
population $Z$. 
The SGTT algorithm tries to find a contrast matrix $C$ that looks like $L'$:
a diagonal of 1, zeros off the diagonal for any pairs of terms where one is
not contained in the other, orthagonal to $(Z'Z)^-$. 
Any columns of $C$ corresponding the missing coefficients in the fit will be
set to zero.
 
The SAS glm procedure uses a different algorithm, which is described indirectly
in \ref{Sas2013}.
\begin{enumerate}
  \item Create a design matrix $X$ of the form used by the GLM procedure,
    containing from left to right
    the intercept, then main effects, then two variable interactions
    (if any), three variable interactions, etc.  Within each level
    variables are in the order of the model statement.
    \begin{itemize}
      \item A categorical variable with $k$ levels is
        represented by $k$ 0/1 dummy variables, 
        which represent the first, second, etc. levels of the variable.
      \item The interaction between two categorical variables 
        that have $k$
        and $m$ levels will be represented by $km$ 0/1 columns, and 
        likewise for higher level interactions.
    \end{itemize}
  \item Create the $p$ by $p$ dependency matrix $D=(X'X)^-(X'X)$ from the 
    $n$ by $p$ matrix $X$, from left to right.  It has a special form:
    \begin{itemize}
      \item If column $k$ of $X$ can be written as a linear combination of
        prior columns, then $D_{1k}$ to $D_{k-1,k}$ contains that combination,
        and $D_{j,k}=0$ for $j \ge k$.
      \item Otherwise set $D_{kk}=1$ and $D_{jk}=0$ for $j ne k$.
      \item Note that $D$ is upper triangular.  If the 
        $i$th column of $X$ is linearly dependent on 
        prior columns, then the $i$th row of $D$ will be zero.
    \end{itemize}
  \item Let $C=D$, then partially orthogonalize the rows of $C$.
    \begin{itemize}
      \item For any row of $C$ that corresponds to a categorical term,
        and is contained in some categorical term further to the right in
        the matrix, make this row orthogonal to any rows of 
        those containing terms.
      \item The term x1 is contained in x1*x2 for instance.
    \end{itemize}
  \item The resulting rows of $C$ form the type III contrasts, i.e.,
    tests of $C_{j.} \beta =0$ where $j$ is the set of rows for a particular 
    term in the model.
\end{enumerate}

An advantage of the first algorithm is that it works for any coding of the 
categorical variables; one can even have different codings in the same
model, say with x1 using contr.treatment and x2 contr.helmert.  Changing
the coding will not change the final result. 
The SAS glm algorithm on the other
hand appears to be totally dependent on using this exact form of the
$X$ matrix before proceeding to steps 2 and 3;
the model matrices produced by \code{model.matrix} in R have far fewer
columns and simply don't work.
Neither approach produces the MPPV values themselves, only a global contrast 
$C$ for the test $C\beta =0$.

If all covariates are categorical and all of the MPPV values are estimable 
then both algorithms will agree with the Yates tests.
The ``keep unique rows'' operation of algorithm 1 is simply a clever way
to generate all of the population matrices at once, 
e.g., if \code{treatment} were the variable of interest $Z$ will contain
rows for treatment A + all combinations of the other factors, rows
for treatment B + all combinations of the others, etc.
If not all the MPV values are estimable then the Yates SS is not defined, 
the keep-unique-rows operation does not produce a population matrix, algorithms
1 and 2 do not always agree, and the result from either of them is something
quite different.

The tests are still orthogonal, but orthogonal to what?

For continuous variables SAS computes type 2 tests and labels them as 
type 3.
Type 2 tests are a sensible but completely different idea that is not
related to populations, and the \code{yates} function does not address
them.  

Note that the SGTT algorithm is the SAS \emph{glm} type 3 procedure. 
Several other SAS procedures also create output labeled as ``type 3'' which
is not necessarily the same. 
The SAS phreg procedure appears to use the NSTT computation, for instance,
and we have found
others that are not invariant to the choice of the reference level for a 
factor.

\subsection{NSTT}
A major problem with SAS type III computations is that almost no one knows
exactly what is being computed.  
The documentation is deficient for the glm procedure ---
\cite{SAS2013} describes the
algorithm but not what that algorithm computes --- and is largely non-existent 
for others.
This has led to propogation of a false ``type 3'' algorithm in multiple packages
which I call the NSTT.
\begin{enumerate}
  \item Build an $X$ matrix from left to right in the 
   standard order of intercept, then main effects, then 2 way interactions, etc.
  \item From left to right, remove any columns of $X$ that are redundant,
    i.e., can be written as a linear combination of prior columns.
    They will not be used again.
  \item Fit the model using the revised $X$ matrix.
  \item For any given term $k$ do simple test that the \emph{remaining}
    coefficients corresponding to term $k$ are zero.  
    In a model \code{y ~ a * b} the test for \code{b} will not involve any
    coefficient labeled as part of the \code{a:b} interaction.
\end{enumerate}

Here is an example using the solder data:
<<nstt>>=
options(contrasts = c("contr.treatment", "contr.poly"))  # default
nfit1 <- lm(skips ~ Solder*Opening + PadType, solder)
drop1(nfit1, ~Solder)
@ 

This shows a type III sum of squares of 389.88.
However, if a different coding is used then the SS increases over 25 fold:

<<nstt2>>=
options(contrasts = c("contr.SAS", "contr.poly"))
nfit2 <- lm(skips ~ Solder*Opening + PadType, solder)
drop1(nfit2, ~Solder)

# Simple means
with(solder, tapply(skips, list(Solder, Opening), mean))
@ 

The example shows a primary problem with the NSTT: the answer that you get
depends on how the contrasts were coded.  
For a simple 2 way interaction like the above, it turns out that the
NSTT actually tests the effect of Solder within the reference cell
for Opening, it is not a global test at all. 
Looking at the simple cell means it is no surprise that the \code{contr.SAS}
fit, which uses Opening=S as the reference will yield a large NSTT
SS for its comparison of 17.4 and 5.5, while the \code{contr.treatment}
version using Opening=L as reference has a much smaller NSTT.
In fact, re-running a particular analysis with different reference levels for
one or more more of the adjusting variables is a quick way to diagnose
probable use of the NSTT algorithm by a program.  

The biggest problem with the NSTT is that it sometimes gives 
the correct answer.
If one uses summation constraints, a form of the model 
that most of us have not seen since graduate school:
\begin{align*}
  y_{ijk} &= \mu + \alpha_i + \beta_j + \gamma_{ij} + \epsilon \\
  \sum_i \alpha_i & = 0 \\
  \sum_j \beta_j  & = 0 \\
  \sum_{ij} \gamma_{ij} &=0
\end{align*}
then the `reference cell' for Opening is the mean Opening effect,
and the NSTT for Solder will correspond to an MPV using the factorial 
population.

<<nstt3>>=
options(contrasts = c("contr.sum", "contr.poly"))
nfit3 <- lm(skips ~ Solder*Opening + PadType, solder)

drop1(nfit3, ~Solder )  
yates(nfit1, ~Solder, population='factorial')   
@ 

Thus our acronym for this method of not-safe-type-three (NSTT), since 
the method does work if one is particularly careful.
Given the number of incorrect analyses that have arisen from 
this approach
`nonsense type 3' would also be valid interpretation, however.

<<nstt4, echo=FALSE>>=
options(contrasts = c("contr.treatment", "contr.poly"))  # restore
@ 

\section{Conclusion}
The population average predicted value or marginal estimate is a 
very useful statistical concept.
One simple advantage of the approach is that because predicted values do not
depend on how a model is parameterized, the results are naturally invariant
to how categorical variables are represented.
But like many good ideas in statistics, proper application of the idea requires
some thought. 
In terms of what to predict, for linear models the simple linear predictor
$X\beta$ is the obvious choice, but for others is more nuanced.
The choice of which predicted value should be used for the summary of a
Cox model is, for instance ,an area that requires further research.

In terms of the population choice, it should match the research question
at hand.
The factorial population in particular has been overused.
This population is entirely appropriate for the use cases
described in Yates \cite{Yates34} or Goodnight \cite{Goodnight78} that 
relate to designed experiments 
such as the solder data, but in observational or clinical data
it will too often be the answer to a question that nobody asked.
In the FLC data set for instance, what is the use of a factorial prediction
that has equal numbers of subjects at each age?
This is a quantity that no one will ever observe, and its continued use
begins to resemble
medieval debates of angels dancing on the head of a pin.
\end{document}
