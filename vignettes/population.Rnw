\documentclass{article}[11pt]
\usepackage{Sweave}
\usepackage{amsmath}
\addtolength{\textwidth}{1in}
\addtolength{\oddsidemargin}{-.5in}
\setlength{\evensidemargin}{\oddsidemargin}
%\VignetteIndexEntry{Population contrasts}

\SweaveOpts{prefix.string=tests,width=6,height=4, keep.source=TRUE, fig=FALSE}
% Ross Ihaka suggestions
\DefineVerbatimEnvironment{Sinput}{Verbatim} {xleftmargin=2em}
\DefineVerbatimEnvironment{Soutput}{Verbatim}{xleftmargin=2em}
\DefineVerbatimEnvironment{Scode}{Verbatim}{xleftmargin=2em}
\fvset{listparameters={\setlength{\topsep}{0pt}}}
\renewenvironment{Schunk}{\vspace{\topsep}}{\vspace{\topsep}}

\SweaveOpts{width=6,height=4}
\setkeys{Gin}{width=\textwidth}

<<echo=FALSE>>=
options(continue="  ", width=60)
options(SweaveHooks=list(fig=function() par(mar=c(4.1, 4.1, .3, 1.1))))
pdf.options(pointsize=8) #text in graph about the same as regular text
options(contrasts=c("contr.treatment", "contr.poly")) #reset default
@ 

\title{Population contrasts}
\author{Terry M Therneau \\ \emph{Mayo Clinic}}
\newcommand{\code}[1]{\texttt{#1}}
\newcommand{\myfig}[1]{\includegraphics[height=!, width=\textwidth]
                        {tests-#1.pdf}}

\newcommand{\ybar}{\overline{y}}

\begin{document}
  \maketitle
  \tableofcontents

\section{Introduction}
The question of how best to summarize the effect of some particular
covariate, within a model that contains multiple others, is an old one.
It becomes particularly acute in the presence of interactions:
consider the hypothetical data shown in the figure below
comparing treatments A and B with age and sex as a confounders.
What value should be used to summarize the treatment effect?
One sensible approach is to select a fixed \emph{population} for the
age/sex distribution, and then compute the
average  effect over that population.

<<fig1, fig=TRUE>>=
plot(c(50,85), c(2,4.5), type='n', xlab="Age", ylab="Effect")
abline(.645, .042, lty=1, col=1)
abline(.9, .027, lty=1, col=2)
abline(.35, .045, lty=2, col=1)
abline(1.1, .026, lty=2, col=2)
legend(50, 4.2, c("Treatment A, female", "Treatment B, female", 
                  "Treatment A,  male", "Treatment B, male"),
       col=c(1,2,1,2), lty=c(1,1,2,2), bty='n')
@ 

More formally, assume we have some fitted model. Split the model 
predictors into two groups: $U$ and $V$, where $U$ is the covariate of 
interest (treatment in the example above) and $V$ is everything else.
Then a marginal estimate for treatment A is
\begin{equation*}
   m_A = E_F \,\hat y(u=A, V)
\end{equation*}
where $F$ is some population distribution for the covariates $V$.
Important follow-up questions are what population, what statistic,
the computational algorithm, and statistical properties of the resulting
estimate. 

Four common populations are
\begin{itemize}
  \item Empirical: the data set itself.  For the simple example above this
    would be the set of all $n$ age/sex pairs in the data set, 
    irrespective of treatment.
  \item Yates: this is only applicable if the adjusting variables $V$
    are all categorical, and consists of all unique combinations of $V$.
    That is, the data set one would envision for a balanced factorial 
    experiment.
  \item External: an external reference such as the age/sex distribution 
    of the US census.
  \item SAS: a factorial (Yates) distribution for the categorical predictors 
    and the data distribution for the others. 
\end{itemize}

The \code{yates} function is designed to compute such population averages
from a fitted model, along with desired contrasts on the resultant 
estimates, e.g., that the population average effect for treatment 1 and
treatment 2 are equal. 
It has been tested with the results of lm, glm, and coxph fits, and should
work with any R model that includes a standard set of objects in the result
(\code{terms}, \code{contrasts}, \code{xlevels}, \code{assign}, and \code{call}).

As the reader might already guess from the labels used just above, the
concept of population averages is a common one in statistics.
(Taking an average is, after all, nearly the first thing a statistician
will do.)
Yates' weighted means analysis, the g-estimates of causal models,
direct adjusted survival curves, and least squares means (SAS glm
procedure) are a small sample.  
The function's name is a nod to the oldest of these.

\section{Simple examples}
\subsection{Linear model}
The \code{solder} data set, used in the introduction to Statistical Models
in S \ref{Chambers93} provides a simple starting example.
In 1988 an experiment was designed and implemented at one of AT&T's
factories to investigate alternatives in the "wave soldering" procedure
for mounting electronic componentes to printed circuit boards.
The experiment varied a number of factors relevant to the process.
The response, measured by eye, is the number of visible solder skips.

<<solder1, fig=TRUE>>=
summary(solder)
length(unique(solder$PadType))
# reproduce their figure 1.1
temp <- lapply(1:5, function(x) tapply(solder$skips, solder[[x]], mean))
plot(c(0,6), range(unlist(temp)), type='n', xaxt='n',
     xlab="Factors", ylab ="mean number of skips")

axis(1, 1:5, names(solder)[1:5])
for (i in 1:5) {
    y <- temp[[i]]
    x <- rep(i, length(y))
    text(x-.1, y, names(y), adj=1)
    segments(i, min(y), i, max(y))
    segments(x-.05, y, x+.05, y)
}
@ 
A perfectly balanced experiment would have 3*2*10*3 = 180 observations for
each Mask, corresponding to all combinations of opening, solder thickness,
pad type and panel.  
The A3 mask has extra replicates of the Large/Thick, Large/Thin, and Small/Thick
conditions, and A6 has only the Medium/Thick, Medium/Thin, and Small/Thin
conditions.
Essentially, one extra run of 180 was done with a mixture of masks.
Chambers and Hastie focus on the balanced subset so their figure and results
are slightly different.

Do a simple fit and then obtain the Yates predictions.
<<solder2>>=
fit1 <- lm(skips ~ Opening + Solder + Mask + PadType + Panel,
           data=solder)
y1 <- yates(fit1, "Opening", population = "factorial")
y1
@ 
The printout has two parts: the left hand colums are mean predicted
values, the right hand are tests on those predicted values.  The default is
a global test that they are all equal.
(``Population predicted value'' would be a better label but the abbreviation
PPV would be confused with the positive predicted value.)
The estimates under a factorial population (\code{y1}) are the Yates' weighted
means \cite{Yates} and the corresponding test is the Yates' sum of
squares for that term.  These would be labeled as a 
``least squares mean'' and ``type III SS'', respectively,
by the SAS glm procedure.  
More on this topic appears in the section on the SGTT algorithm.

Repeat this using the default population, which is the set of all 900
combinations for solder, mask, pad type and panel found in the data.
The pairwise option gives tests on all pairs of openings, for details
on this and other arguments see the \code{cmatrix} function;
``cm'' is a shorthand allowed within the \code{yates} argument list.
<<solder2b>>=
y2 <- yates(fit1, cm("Opening", test="pairwise"), population = "data") 
y2

temp <- rbind(diff(y1$estimate$mpv), diff(y2$estimate$mpv))
dimnames(temp) <- list(c("factorial", "emprical"), c("2 vs 1", "3 vs 2"))
round(temp,5)
@ 

Although the MPV values shift with the new population, the difference
between any two pairs and the global test are unchanged.
This is because we have fit a model with no interactions.
In figure 1 this is a model with parallel prediction lines; shifting left or
right may change an average but has no effect on the difference.
For a model with no iteractions the test statistics created by the
\code{yates} function will be no different than simple tests on the model
coefficients.

More interesting are models with an interaction.
We'll drop the A6 group to leave an ``expanded'' experiment, an overall
balanced design that has some extra observations in some of the cells.

<<solder3>>=
solder2 <- droplevels(subset(solder, Mask != "A6"))
fit2a <- lm(skips ~ Mask + Opening + Solder, solder2)
fit2b <- lm(skips ~ Mask * Opening * Solder, solder2)
anova(fit2a, fit2b

yates(fit2a, "Mask", population="Yates")
yates(fit2b, "Mask", population="yates")
@ 

The added interactions significantly improve the model fit.
Interestingly, in this case they hardly change the population averages,
the A3 group shifts slightly;
the major change is a reduction in standar error.


\subsection{Missing cells}
Models that involve factors and interactions can have an issue with
missing cells as shown by the example below.
<<solder4>>=
fit3 <- lm(skips ~ Opening * Mask + Solder + PadType + Panel, solder)
yates(fit3, cm("Mask", test="pairwise"))
@ 
The set of population predictions includes Opening*Mask combinations of
Large/A6.  There are no observations in the data with this combination and
hence relevant coefficient(s) in \code{fit2} are NA.
Formally, those predictions are \emph{not estimable}, and as a consequence
neither are any population averages that include them, nor any tests that involve
those averages.

\subsection{Cox models}


\subsection{Population}
The \code{population} parameter parameter of the call can be either a data set
or one of `data', `factorial', `sas', or `none'. 
Alternate labels of `empirical' and `Yates' are allowed for the `data' and
`factorial' options, respectively.
If \code{population} is a data set then it must contain all variables found
in the set of adjusters $V$.  
Using the original data set is equivalent to `data'.


