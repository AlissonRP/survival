\documentclass{article}[11pt]
\usepackage{Sweave}
\usepackage{amsmath}
\addtolength{\textwidth}{1in}
\addtolength{\oddsidemargin}{-.5in}
\setlength{\evensidemargin}{\oddsidemargin}
%\VignetteIndexEntry{Population contrasts}

\SweaveOpts{prefix.string=tests,width=6,height=4, keep.source=TRUE, fig=FALSE}
% Ross Ihaka suggestions
\DefineVerbatimEnvironment{Sinput}{Verbatim} {xleftmargin=2em}
\DefineVerbatimEnvironment{Soutput}{Verbatim}{xleftmargin=2em}
\DefineVerbatimEnvironment{Scode}{Verbatim}{xleftmargin=2em}
\fvset{listparameters={\setlength{\topsep}{0pt}}}
\renewenvironment{Schunk}{\vspace{\topsep}}{\vspace{\topsep}}

\SweaveOpts{width=6,height=4}
\setkeys{Gin}{width=\textwidth}

<<echo=FALSE>>=
options(continue="  ", width=60)
options(SweaveHooks=list(fig=function() par(mar=c(4.1, 4.1, .3, 1.1))))
pdf.options(pointsize=8) #text in graph about the same as regular text
options(contrasts=c("contr.treatment", "contr.poly")) #reset default
#library(survival)
library(splines)
@ 

\title{Population contrasts}
\author{Terry M Therneau \\ \emph{Mayo Clinic}}
\newcommand{\code}[1]{\texttt{#1}}
\newcommand{\myfig}[1]{\includegraphics[height=!, width=\textwidth]
                        {tests-#1.pdf}}

\newcommand{\ybar}{\overline{y}}

\begin{document}
  \maketitle
\  \tableofcontents

\section{Introduction}
Statistician and their clients have always been fond of finding a single
number summary of an effect in data (and perhaps too much so).
Consider the hypothetical data shown in figure \ref{fig1}
comparing treatments A and B with age as a confounder.
What is a useful single number summary of the difference between 
treatment arms A and B?
One approach is to select a fixed \emph{population} for the
age distribution, and then compute the
mean effect over that population.

\begin{figure}
<<fig1, echo=FALSE, fig=TRUE>>=
plot(c(50,85), c(2,4.5), type='n', xlab="Age", ylab="Effect")
#abline(.645, .042, lty=1, col=1, lwd=2)
#abline(.9, .027, lty=1, col=2, lwd=2)
abline(.35, .045, lty=1, col=1, lwd=2)
abline(1.1, .026, lty=1, col=2, lwd=2)
legend(50, 4.2, c("Treatment A", "Treatment B"), 
        col=c(1,2), lty=1, lwd=2, cex=1.3, bty='n')
@ 
 \caption{Treatment effects for a hypothetical study.}
 \label{fig1}
\end{figure}

More formally, assume we have a fitted model. Split the model 
predictors into two groups $U$ and $V$, where $U$ is the covariate of 
interest (treatment in the example above) and $V$ is everything else.
Then a marginal estimate for treatment A is
\begin{equation*}
   m_A = E_F \,\hat y(A, V)
\end{equation*}
where $F$ is some chosen population distribution for the covariates $V$.
Important follow-up questions are what population, what statistic,
the computational algorithm, and statistical properties of the resulting
estimate.
Neither the statistic nor population questions should be taken lightly
and need to be closely linked to the scientific question.  
If for instance the model of figure 1 were used to inform a nursing home
formulary then the distribution $F$ might be shifted toward the right.

Four common populations are
\begin{itemize}
  \item Empirical: the data set itself.  For the simple example above this
    would be the distribution of all $n$ ages in the data set, 
    irrespective of treatment.
  \item Factorial or Yates: this is only applicable if the adjusting 
    variables $V$
    are all categorical, and consists of all unique combinations of $V$.
    That is, the data set one would envision for a balanced factorial 
    experiment.
  \item External: an external reference such as the age/sex distribution 
    of the US census.  This is common in epidemiolgy.
  \item SAS type 3: a factorial distribution for the categorical predictors 
    and the data distribution for the others. 
\end{itemize}

The \code{yates} function is designed to compute such population averages
from a fitted model, along with desired contrasts on the resultant 
estimates, e.g., whethter the population average effects for treatment A and
treatment B are equal. 
The function has been tested with the results of lm, glm, and coxph fits,
and can easily be extended to any R model that includes a standard set of
objects in the result:
(\code{terms}, \code{contrasts}, \code{xlevels}, and \code{assign}.

The routine's name is a nod to the 1936 paper by Yates \ref{Yatesxx} 
which contains an early explicit use of the notion.  In dealing with
unbalanced factorial data he states that ``ababc'',
which is the mean predicted value of model predictions for a balanced 
population.
He then develops
formulas for calculation of these quantities and tests of their
equality that were practical for manual calculations of the time.
This
concept of population averages is actually a common one in statistics.
Taking an average is, after all, nearly the first thing a statistician
will do.
Yates' weighted means analysis, the g-estimates of causal models,
direct adjusted survival curves, and least squares means are but a 
small sample.  


\section{Solder Example}
\subsection{Data}
In 1988 an experiment was designed and implemented at one of AT\&T's
factories to investigate alternatives in the "wave soldering" procedure
for mounting electronic componentes to printed circuit boards.
The experiment varied a number of factors relevant to the process.
The response, measured by eye, is the number of visible solder skips.
The \code{solder} data set is used in the introduction of the
book Statistical Models
in S \ref{Chambers93}; it is also included in the survival package to
illustrate the \code{yates} function for \code{lm} and \code{glm} models.

<<solder1, fig=TRUE>>=
summary(solder)
length(unique(solder$PadType))
# reproduce their figure 1.1
temp <- lapply(1:5, function(x) tapply(solder$skips, solder[[x]], mean))
plot(c(0,6), range(unlist(temp)), type='n', xaxt='n',
     xlab="Factors", ylab ="mean number of skips")

axis(1, 1:5, names(solder)[1:5])
for (i in 1:5) {
    y <- temp[[i]]
    x <- rep(i, length(y))
    text(x-.1, y, names(y), adj=1)
    segments(i, min(y), i, max(y))
    segments(x-.05, y, x+.05, y)
}
@ 
A perfectly balanced experiment would have 3*2*10*3 = 180 observations for
each Mask, corresponding to all combinations of opening, solder thickness,
pad type and panel.  
The A3 mask has extra replicates for a subset of the opening*thickness
combinations, however, while mask A6 is lacks observations for these sets.
Essentially, one extra run of 180 was done with a mixture of masks.

\subsection{linear model}
A subset of the solder data that excludes mask A6 is exactly the type
of data set considered in Yates' paper: a balanced design whose data set
is not quite balanced.
Start with a simple fit and then obtain the Yates predictions.
<<solder2>>=
fit1 <- lm(skips ~ Opening + Solder + Mask + PadType + Panel,
           data=solder, subset= (Mask != 'A6'))
y1 <- yates(fit1, ~Opening, population = "factorial")
y1
@ 
The printout has two parts: the left hand colums are mean population predicted
values (mppv), the right hand are tests on those predicted values.  
The default is a single global test that they are all equal.
Under a factorial population these are the Yates' weighted
means \cite{Yates} and the corresponding test is the Yates' sum of
squares for that term.  These would be labeled as
``least squares means'' and ``type III SS'', respectively,
by the glm procedure of SAS. 
More on this correspondence appears in the section on the SGTT algorithm.

Repeat this using the default population, which is the set of all 810
combinations for solder, mask, pad type and panel found in the (non A6) data.
The pairwise option requests tests on all pairs of openings.
<<solder2b>>=
y2 <- yates(fit1, "Opening", population = "data", test="pairwise") 
y2

temp <- rbind(diff(y1$estimate$mpv), diff(y2$estimate$mpv))
dimnames(temp) <- list(c("factorial", "emprical"), c("2 vs 1", "3 vs 2"))
round(temp,5)
@ 

Although the MPV values shift with the new population the global test is
unchanged, nor is the difference in MMPV between any two pairs.
This is because we have fit a model with no interactions.
Referring to figure 1 this is a model where all of the predictions are
parallel lines; shifting the population left or
right will change the MPPV  but has no effect on the difference between two
lines.
For a linear model with no iteractions the test statistics created by the
\code{yates} function are thus not very interesting, since they will be no 
different than simple comparisons of the model coefficients.

Here are results from a more interesting fit that includes interactions.
<<solder3>>=
fit2 <- lm(skips ~ Opening + Mask*PadType + Panel, solder,
           subset= (Mask != "A6"))
yates(fit2, ~Opening, population="factorial")
@ 
This data set is close to balanced and the means change only a small amount.

\subsection{Missing cells}
Models that involve factors and interactions can have an issue with
missing cells as shown by the example below.
<<solder4>>=
fit3 <- lm(skips ~ Opening * Mask + Solder + PadType + Panel, solder)
yates(fit3, ~Mask, test="pairwise")
@ 
The population predictions for each Mask include all combinations of
Opening, Solder, PadType, and Panel that are found in the data.
In the above call the empirical population was used, and the
underlying algorithm amounts to
\begin{enumerate}
  \item Make a copy of the data set (900 obs), and set Mask to A1.5 in
    all observations
  \item Get the 900 resultant predicted values from the model, and take their
    average
  \item Repeat 1 and 2 for each mask type.
\end{enumerate}
However, there were no observations in the data set with Mask = A6 and 
opening = Large.
Formally, predictions for the A6/Large combination are \emph{not estimable}, 
and as a consequence
neither are any population averages that include those predicted values, 
nor any tests that involve those population averages.
This lack of estimability is entirely due to the inclusion of a mask by opening
interaction term in the model, which states that each Mask/Opening combination
has a unique effect, which in turn implies that we need an estimate for all
Mask*Opening pairs to compute a population prediction.

If you do the above steps 'by hand' the R \code{predict} function will
return a value for all 900 predictions
along with a warning message that the results may not be reliable,
and the warning is correct.
The result of \code{coef(fit2)} reveals an NA coefficient.
The presence of a missing value shows that some preditions
will not be estimable, but it is not possible to determine \emph{which}
ones are estimable from the coefficients alone.
The predict function knows that some predictions will be wrong but not
which ones.
A formal definition of estimability for a given prediction is that it 
can be written 
as a linear combination of the rows of $X$, the design matrix for the fit.
The \code{yates} function performs the necessary calculations to verify formal
estimability of each predicted value and thus is able to correctly identify the 
deficiency.

\section{Generalized linear models}
\label{sect:glm}
Since the solder response is a count of the number of skips a Poisson
model is the more natural approach than lm().
In a glm model we need to consider more carefully both the population
and \emph{what} statistic should be averaged, however.

<<glm>>=
gfit1 <- glm(skips ~ Opening + Mask +PadType + Solder, data=solder,
             family=poisson)
gfit2 <- glm(skips ~ Opening + Mask +PadType + Solder, data=solder,
             family=poisson)
yates(gfit2, ~ Mask, predict = "link") 
yates(gfit2, ~ Mask, predict = "response")  
@ 

Predicted values for the average number of skips are similar to the
results that were obtained from the linear model; they parallel 
\code{type='response'} in \code{predict.glm}.
Prediction of type 'link' correspond to the linear predictor $X\beta$,
averaged over the population.
Though perfectly legal (you can take the mean of anything you want)
these MMPV values are more difficult to interpret.  
Since exp(mean(log(x)) defines the geometric mean of $x$, and log() is the
default link function for poisson regression, one can view the 
exponentiated MMPV values as a geometric mean of predicted values over
the population.
Given the skewness of the observed skip counts an argument can clearly
be made for using the geometric mean, and its variance can be approximated
using the usual delta method.  Tests between Masks would remain unchanged.

<<glm2>>=
# post-process the link
ytemp <- yates(gfit2, ~ Mask, predict="link")
ytemp$estimate$mmpv <- exp(ytemp$estimate$mmpv)
ytemp$estimate$std <- ytemp$estimate$std * ytemp$estimate$mpv
ytemp
@ 

For nonlinear predictors such as the response, the population matters even
for an additive model.  The two below have different between MPPV differences
and different tests.

<<glm3>>=
yates(gfit1, ~ Opening, test="pairwise", predict = "link")
yates(gfit1, ~ Opening, test="pairwise", predict = "response")
@ 


\section{Free Light Chain}
The \code{flchain} data frame contains the results of a small number
of laboratory tests done on a large fraction of the 1995
population of Olmsted County, Minnesota aged 50 or older
\cite{Kyle06, Dispenzieri12}.
The R data set contains a 50\% random sample of this larger study
and is included as a part of the survival package.
The primary purpose of the study was to measure the amount of
plasma immunoglobulins and its components.  
Intact immunoglobulins are composed of a heavy chain and light chain
portion.  In normal subjects there is overproduction of the light chain 
component by the immune cells leading to a small amount of 
\emph{free light chain}  in the circulation.
Excessive amounts of free light chain (FLC) are thought to be a marker of
disregulation in the immune system.
An important medical question is whether high levels of FLC have an
impact on survival, which will be explored using a Cox model.  
Free light chains have two major forms denoted as kappa and lambda,
we will use the sum of the two.

A confounding factor is that free light chain values rise with age, in
part because it is eliminated by the kidneys and renal function
declines with age.
The age distribution of males and females differs, so we
will need to adjust our simple comparison between the sexes
for age effects.
The impact of age on mortality is of course even greater
and so correction for the age imbalance is is critical when exploring
the impact of FLC on survival.

Figure \ref{fig:data} shows the trend in free light chain values
as a function of age.
For illustration of linear models using factors, we have also
created a categorical age value using deciles of age.
The table of counts shows that the sex distribution becomes increasingly
unbalanced at the older ages, from about 1/2 females in the youngest
group to a 4:1 ratio in the oldest.
<<counts>>=
flchain$flc <- flchain$kappa + flchain$lambda                    
age2 <- cut(flchain$age, c(49, 59, 69, 79, 89, 120),                   
            labels=c("50-59", "60-69", "70-79", "80-89", "90+"))
fgroup <- cut(flchain$flc, quantile(flchain$flc, c(0, .5, .75, .9, 1)),
              include.lowest=TRUE, labels=c("<50", "50-75", "75-90", ">90"))

counts <- with(flchain, table(sex, age2))
counts
@ 
\begin{figure}
<<data, fig=TRUE, include=FALSE, echo=FALSE>>=
male <- (flchain$sex=='M')
mlow <- with(flchain[male,],  smooth.spline(age, flc))
flow <- with(flchain[!male,], smooth.spline(age, flc))
plot(flow, type='l', ylim=range(flow$y, mlow$y),
     xlab="Age", ylab="FLC")
lines(mlow, col=2)
@
\caption{Free light chain values as a function of age.}
\label{fig:data}

Notice that the male/female difference in FLC varies with age, 
\Sexpr{round(cellmean[1,1],1)} versus \Sexpr{round(cellmean[2,1],1)}
at age 50--59 and \Sexpr{round(cellmean[1,5],1)} versus
 \Sexpr{round(cellmean[2,5],1)} at age 90.
The data does not fit a simple additive model; there are ``interactions''
to use statistical parlance.
Men and women simply do not age in quite the same way.

\subsection{Linear models}
Compare the mean FLC for males to females, with and without adjusting for
age.
<<flc1>>=
library(splines)
flc1 <- lm(flc ~ sex, flchain)
flc2a <- lm(flc ~ sex + ns(age, 3), flchain)
flc2b <- lm(flc ~ sex + age2, flchain)
flc3a <- lm(flc ~ sex * ns(age, 3), flchain)
flc3b <- lm(flc ~ sex * age2, flchain)
# predict near to the mean age
tdata <- data.frame(sex=c("F", "M"), age=65, age2="60-69")
temp <- rbind("unadjusted" = predict(flc1, tdata),
              "additive, continuous age" = predict(flc2a, tdata),
              "additive, discrete age"   = predict(flc2b, tdata),
              "interaction, cont age"    = predict(flc3a, tdata),
              "interaction, discrete"    = predict(flc3b, tdata))
colnames(temp) <- c("Female", "Male")
round(temp,2)
@ 

The between gender difference is underestimated without adjustment for
age.  The females are overrepresented at the high ages, which inflates
their naive average.  Continous and categorical age adjustment is similar in
this particular data set.  
Now look at population adjustment.

<<flc2>>=
yates(flc3a, ~sex)  # population = data is the default
yates(flc3b, ~sex) 
yates(flc3b, ~sex, population="factorial")
@ 

The population average values for the empirical distribution are just a
bit higher then the prediction at the mean due to the upward curvature
of the age vs FLC curve. 
The average for a factorial population, however, jumps up much more.
This is because it is the average for an unusual population which has
as many 90+ year old subjects as 50--59 year old; i.e., 
it is the correct answer to a rather odd question.

We can also look at the age effect after adjusting for sex.  For the
continuous model the age values of interest for the MPPV need to be added using
the \code{levels} argument.

<<flc3>>=
#yates(flc3a, ~ age, levels=c(65, 75, 85))  # currently fails
yates(flc3b, ~age)
@ 

\section{Cox Models}
Finally we come to Cox models which are, after all, the point of this
vignette.
Here the question of what to predict is more serious.
Start by looking at three simple models.

<<cox1>>=
options(show.signif.stars=FALSE)  # show statistical intelligence
coxfit1 <- coxph(Surv(futime, death) ~ sex, flchain)
coxfit2 <- coxph(Surv(futime, death) ~ sex + age, flchain)
coxfit3 <- coxph(Surv(futime, death) ~ sex * age, flchain)
anova(coxfit1, coxfit2, coxfit3)
#
exp(c(coef(coxfit1), coef(coxfit2)[1]))  # sex without and with age
@ 

The model with an age*sex interaction does not fit substantially better
than the additive model.  
This is not a surprise as the US and Minnesota death rate curves for males and
females are nearly parallel after age 50.  
The sex coefficients for models 1 and 2 differ substantially.  
Males in this data set have almost 1.5 the death rate of females at any
given age, but when age is ignored the fact that females dominate the
oldest ages almost completely cancels this out.
Adjustment for both age and sex is critical for understanding the potential
effect of FLC on survival.

Dispenzieri \cite{Dispenzierixx} looked at the impact of FLC by dividing the
sample into those above and below the 90th percentile of FLC;
for illustration we will use 4 groups defined above of the lowest 50\%,
50 to 75th percentile, 75 to 90th and above 90.

<<coxfit2>>=
coxfit4 <- coxph(Surv(futime, death) ~ fgroup*age + sex, flchain)
yates(coxfit4, ~ fgroup, predict="linear")
yates(coxfit4, ~ fgroup, predict="risk")
@ 
We see that after adjustment for age and sex, the FLC is a strong
predictor of survival.
Since the Cox model is a model of relative risk any constant term is
arbitrary: one could add 100 to all of the log rates (type 'linear' above)
and have as valid an answer.  To keep the coefficients on a sensible scale
the \code{yates} function centers the mean linear predictor for the 
underlying model at zero.  
This does not precisely center the risks at 1 due to
Jensen's inequality, but suffices to keep the values in a sensible range. 
 
A similar argument to that found in section \ref{sect:glm} about the arithmetic
versus geometric mean can be made here, but a more fundamental issue is that
the overall hazard function for a population is not the average of the
hazards for each of its members, and in fact will change over time as the
higher risk members of the population die.  
Though computable there is no natural feel for what either the mean risk
or mean log risk actually are.

Survival curves however to average; the survival curve of a population is the
average of the individual survival curves of its members.
Functions of the survival curve such as the median survival = time until the
curve reaches .5 or the mean survival = area under the curve will then
also be well defined quanties.



\section{Population}
The \code{population} parameter parameter of the call can be either a data set
or one of `data', `factorial', `sas', or `none'. 
Alternate labels of `empirical' and `Yates' are allowed for the `data' and
`factorial' options, respectively.
If \code{population} is a data set then it must contain all variables found
in the set of adjusters $V$.  
Using the original data set as the population is equivalent to `data'.

The 'right' data set to use is entirely a function of what question you
want to answer.
Consider the simple example of figure 1.  
If, for instance, one were contemplating effective drugs for a nursing home
population, then a population of ages shifted towards the right hand side of 
the x-axis would be the obvious target, and one would most likely use an
external reference to define that population.  
Both the \code{data} and \code{factorial} populations are also valid
choices, in the senses that each of them addresses a well defined
question; but is is a question that anyone would ask?
The factorial population in particular has been overused.
In figure 1 it corresponds to a population with the same number
of 100 year olds as 50 year olds: a population distribution that will never be
observed.

\section{SAS glim type III (SGTT) algorithm}
A major problem with SAS type III computations is that almost no one knows what
is being computed.  The documentation is sparse for the GLM procedure and
nearly non-existent for others.
``Least squares means'' (LSM) roughly correspond to MPV values, but are defined
only for categorical predictors.
\begin{itemize}
  \item Build an $X$ matrix from left to right in the 
   standard order of intercept, then main effects, then 2 way interactions, etc.
  \item Again from left to right, remove any columns of $X$ that are redundant,
    i.e., can be written as a linear combination of prior columns.
  \item Fit the linear model.
  \item For any given term $k$ do a second fit the removes those columns
    of $X$ corresponding to term $k$, but retaining all others.  This includes
    ``interaction'' columns.  Compare the two fits.
\end{itemize}

Here is an example:
<<nstt>>=
options(contrasts = c("contr.treatment", "contr.poly"))  # default
fit1 <- lm(skips ~ Solder*Opening + PadType, solder)
drop1(fit1, ~Solder)

options(contrasts = c("contr.SAS", "contr.poly"))
fit2 <- lm(skips ~ Solder*Opening + PadType, solder)
drop1(fit2, ~Solder)

with(solder, tapply(skips, list(Solder, Opening), mean))
@ 

The example shows a primary problem with the NSTT: the answer that you get
depends on how the contrasts were coded.  
For a simple 2 way interaction like the above, it turns out that the
NSTT actually tests the effect of Solder within the reference cell
for Opening; it is not a global test at all. 
Looking at the simple cell means it is no surprise that the \code{contr.SAS}
fit, which uses Opening=S as the reference, will have a much larger
estimated effect than the fit that uses Opening=L as reference.
Re-running a particular analysis, but with different reference levels for
one or more more of the adjusting variables, is a quick way to diagnose
probable use of the NSTT algorithm by a program.  
The \code{yates} function with \code{population='none'} also reduces to the
NSTT.

The biggest problem with the NSTT is that it sometimes gives a sensible answer.
If one uses the cell means coding, i.e., the model form that most of us have not
seen since graduate school of
\begin{align*}
  y_{ijk} &= \mu + \alpha_i + \beta_j + \gamma_{ij} + \epsilon \\
  \sum_i \alpha_i & = 0 \\
  \sum_j \beta_j  & = 0 \\
  \sum_{ij} \gamma_{ij}=0
\end{align*}
then the `reference cell' for Solder will be the average Opening effect,
and the NSTT then corresponds to an MPV using the factorial population.

<<nstt2>>=
options(contrasts = c("contr.sum", "contr.poly"))
fit3 <- lm(skips ~ Solder*Opening + PadType, solder)
drop1(fit3, ~Solder)   #something's not right here
yates(fit1, ~Solder, population='factorial')   
@ 

Below is the detailed algorithm used by the SAS GLM procedure.
The least squares means are MPV values as above, using a mixure population:
factorial for categorical variables, i.e.,
    those that appear in a class statement, and the data distribution for
    all others.   For instance if the data set had $n$ rows, some continuous
    predictors, and two categorical adjusters with 3 and 4 levels, respectively,
    then the population data set would have $12 n$ rows.
If the MPV involves non-estimable terms the result will be missing,
    just as with the \code{yates} function.

The type III SS is \emph{not} a direct test of equality of these MPV values,
and is in fact defined even when some or all of them are not estimable.
The root algorithm is based on the fact that in a balanced factorial
model (all predictors categorical, all combinations appear the same number
of times) the SS for all the effects are orthagonal.  
In the model \code{y ~ x1 + x2 + x1:x2} for instance the sequential SS for x1, 
as obtained from an R \code{anova} command,
will be the same whether x1 preceeds or follows after x2 in the
formula.  
Let $V = (X'X)^-$ be the variance-covariance of the coefficients and let
$C$ and $D$ be contrast matrices for two of the 3 terms above.
Orthagonality implies that the covariance between the two effects
is zero, i.e. $CVD' =0$.

This leads to the following algorithm for computing Yates contrasts in an
unbalanced design. 
\begin{itemize}
  \item Let $Z$ be the design matrix for a balanced subset of the the data; 
    i.e., the factorial population of the \code{yates} command.
    One simple way to construct $Z$, if there are no missing cells, is the set
    of unique rows of $X$.  
  \item Choose contrasts that are orthagonal with respect to $(Z'Z)^-$.
  \item Assume an $X$ matrix in the standard column order of intercept, then
    main effects, then 2 way interactions, etc.  One way to create the
    contrasts is
    \begin{enumerate}
      \item Since the columns of $X$ that correspond to any partitular term are
        adjacent, we can think of $U = (Z'Z)^-$ as a partitioned matrix where
        $U_{kk}$ is the block corresponding to the $k$th term.
      \item Start with $C$ the identity matrix, then for any two terms $j<k$ 
         where $j$ is contained in $k$  set $C_{jk} = U_{jk}U_{kk}^{-1}$.  
         The term \code{x1} is for instance contained in \code{x1:x2}.
      \item This results in an upper triangular matrix $C$ whose $k$th set
        of rows is a Yates contrast matrix for term $k$ and which satisfies
        the orthagonality condition.  
    \end{enumerate}
\end{itemize}
The above is the algorithm used by the \code{yates} function for the SGTT
calculation. 
If there are both continuous and categorical variables in the $X$ matrix
then step 2 above is only applied if both $j$ and $k$ are categorical, and
the unique rows operation applies only to the categorical predictors.
(The continuous portion of $Z'Z$ need not even be computed since it is never
used.)
The interaction of a continous and a cateogorical covariate is considered 
to be continous for this purpose.
        
The SAS glm procedure uses a differnt algorithm, which is desribed indirectly
in SASxxx.
\begin{enumerate}
  \item Create design matrix $X$ of the form used by the GLM procedure,
    containing from left to right
    the intercept, then main effects, then two variable interactions
    (if any), three variable interactions, etc.  Within each level
    variables are in the order of the model statement.
    \begin{itemize}
      \item A categorical variable with $k$ levels is
        represented by $k$ 0/1 dummy variables, 
        which represent the first, second, etc. levels of the variable.
      \item The interaction between two categorical variables 
        that have $k$
        and $m$ levels will be represented by $km$ 0/1 columns, and 
        likewise for higher level interactions.
      \item R's \code{model.matrix} function returns far fewer columns, since
        it tries to prospectively remove ones that will be redundant.
        Having all these columns is critical to steps 2 and 3, however.
    \end{itemize}
  \item Create the $p$ by $p$ dependency matrix $D$ from the $n$ by $p$
    matrix $X$, from left to right. 
    \begin{itemize}
      \item If column $k$ of $X$ can be written as a linear combination of
        prior columns, then $D_{1k}$ to $D_{k-1,k}$ contains that combination,
        and $D_{j,k}=0$ for $j \ge k$.
      \item Otherwise set $D_{kk}=1$ and $D_{jk}=0$ for $j ne k$.
      \item Note that $D$ is upper triangular.  If the 
        $i$th column of $X$ is linearly dependent on 
        prior columns, then the $i$th row of $D$ will be zero.
    \end{itemize}
  \item Partially orthagonalize the transpose of $D$ from right to left
    \begin{itemize}
      \item For any column of $D'$ that corresponds to an interaction of one
        or more categorical predictors (but not mixed interactions of
        categorical and continuous variables), make any
        columns of $D'$ that correspond to a contained term orthagonal to it.
      \item If x1 and x2 were categorical, for instance, and the model contains
        the x1*x2 interaction, then the columns of $D'$ corresponding to
        x1 will be made orthagonal to the x1*x2 columns of $D'$, and likewise
        for x2.
    \end{itemize}
  \item The resulting rows of $D$ form the type III contrasts, i.e.,
    tests of $D \beta =0$ for the corresponding rows of $D$.
\end{enumerate}


An advantage of the first algorithm is that it works for any coding of the 
categorical variables; one can even have different codings in the same
model, say with x1 using contr.treatment and x2 contr.helmert.  Changing
the coding will not change the result. 
The SAS glm algorithm on the other
hand appears to be totally dependent on using this exact form of the
$X$ matrix.  One can change the order of the columns within a term,
i.e. change which level is the reference, however,
without effecting the final result.
Neither approach produces the MPV values themselves, only a global contrast 
matrix for testing that they all are equal.
Proving that algorithm 1 gives the same result as the straightforward 
MPV approach is challenging but doable, as is a proof that algorithms 1 and 2
give the same result.

If all covariates are categorical and all of the mpv values are estimable 
then both algorithms will agree with the Yates tests.
The ``keep unique rows'' operation of algorithm 1 is simply clever way
to generate all of the population matrices at once, 
e.g., if \code{treatment} were the variable of interest $Z$ will contain
rows for treatment A + all combinations of the other factors and rows
for treatment B + all combinations of the others.
If not all the MPV values are estimable then a Yates SS is not defined and
the SGTT is something quite different.
The result does not normally correspond to any population at all,
and this author is 
unsure what exactly the SGTT contrast tests in this situation.

Note that the SGTT algorithm is the SAS \emph{glm} type 3 procedure. 
Several other SAS procedures also create output labeled as ``type 3'' which
is not necessarily the same. 
The SAS phreg procedure uses the NSTT computation for instance,
and we have found
others that are not invariant to the choice of the reference level for a 
factor.

For continuous variables SAS computes type 2 tests and labels them as type 3.
Type 2 tests are a sensible but completely different idea that is not
related to populations, and the \code{yates} function does not address
them.  
\end{document}
