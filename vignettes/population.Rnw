\documentclass{article}[11pt]
\usepackage{Sweave}
\usepackage{amsmath}
\addtolength{\textwidth}{1in}
\addtolength{\oddsidemargin}{-.5in}
\setlength{\evensidemargin}{\oddsidemargin}
%\VignetteIndexEntry{Population contrasts}

\SweaveOpts{prefix.string=tests,width=6,height=4, keep.source=TRUE, fig=FALSE}
% Ross Ihaka suggestions
\DefineVerbatimEnvironment{Sinput}{Verbatim} {xleftmargin=2em}
\DefineVerbatimEnvironment{Soutput}{Verbatim}{xleftmargin=2em}
\DefineVerbatimEnvironment{Scode}{Verbatim}{xleftmargin=2em}
\fvset{listparameters={\setlength{\topsep}{0pt}}}
\renewenvironment{Schunk}{\vspace{\topsep}}{\vspace{\topsep}}

\SweaveOpts{width=6,height=4}
\setkeys{Gin}{width=\textwidth}

<<echo=FALSE>>=
options(continue="  ", width=60)
options(SweaveHooks=list(fig=function() par(mar=c(4.1, 4.1, .3, 1.1))))
pdf.options(pointsize=8) #text in graph about the same as regular text
options(contrasts=c("contr.treatment", "contr.poly")) #reset default
library(survival)
@ 

\title{Population contrasts}
\author{Terry M Therneau \\ \emph{Mayo Clinic}}
\newcommand{\code}[1]{\texttt{#1}}
\newcommand{\myfig}[1]{\includegraphics[height=!, width=\textwidth]
                        {tests-#1.pdf}}

\newcommand{\ybar}{\overline{y}}

\begin{document}
  \maketitle
\  \tableofcontents

\section{Introduction}
The question of how best to summarize the effect of some particular
covariate, within a model that contains multiple others, is an old one.
It becomes particularly acute in the presence of interactions:
consider the hypothetical data shown in figure \ref{fig1}
comparing treatments A and B with age and sex as a confounders.
What is the best single number summary of the difference between 
treatment arms A and B?
One sensible approach is to select a fixed \emph{population} for the
age/sex distribution, and then compute the
mean effect over that population.

\begin{figure}
<<fig1, fig=TRUE>>=
plot(c(50,85), c(2,4.5), type='n', xlab="Age", ylab="Effect")
abline(.645, .042, lty=1, col=1)
abline(.9, .027, lty=1, col=2)
abline(.35, .045, lty=2, col=1)
abline(1.1, .026, lty=2, col=2)
legend(50, 4.2, c("Treatment A, female", "Treatment B, female", 
                  "Treatment A,  male", "Treatment B, male"),
       col=c(1,2,1,2), lty=c(1,1,2,2), bty='n')
@ 
 \caption{Treatment effects for a hypothetical study.}
 \label{fig1}
\end{figure}

More formally, assume we have some fitted model. Split the model 
predictors into two groups $U$ and $V$, where $U$ is the covariate of 
interest (treatment in the example above) and $V$ is everything else.
Then a marginal estimate for treatment A is
\begin{equation*}
   m_A = E_F \,\hat y(A, V)
\end{equation*}
where $F$ is some population distribution for the covariates $V$.
Important follow-up questions are what population, what statistic,
the computational algorithm, and statistical properties of the resulting
estimate. 

Four common populations are
\begin{itemize}
  \item Empirical: the data set itself.  For the simple example above this
    would be the set of all $n$ age/sex pairs in the data set, 
    irrespective of treatment.
  \item Yates: this is only applicable if the adjusting variables $V$
    are all categorical, and consists of all unique combinations of $V$.
    That is, the data set one would envision for a balanced factorial 
    experiment.
  \item External: an external reference such as the age/sex distribution 
    of the US census.  This is common in epidemiolgy.
  \item SAS: a factorial (Yates) distribution for the categorical predictors 
    and the data distribution for the others. 
\end{itemize}

The \code{yates} function is designed to compute such population averages
from a fitted model, along with desired contrasts on the resultant 
estimates, e.g., that the population average effect for treatment A and
treatment B are equal. 
The function has been tested with the results of lm, glm, and coxph fits, 
and should
work with any R model that includes a standard set of objects in the result:
(\code{terms}, \code{contrasts}, \code{xlevels}, \code{assign}, and \code{call}).

As the reader might already guess from the labels used just above, the
concept of population averages is a common one in statistics.
Taking an average is, after all, nearly the first thing a statistician
will do.
Yates' weighted means analysis, the g-estimates of causal models,
direct adjusted survival curves, and least squares means (SAS glm
procedure) are a small sample.  
This function's name is a nod to the oldest of these.

\section{Data}
\subsection{Solder}
The \code{solder} data set, used in the introduction of the
book Statistical Models
in S \ref{Chambers93} provides a simple starting example.
In 1988 an experiment was designed and implemented at one of AT\&T's
factories to investigate alternatives in the "wave soldering" procedure
for mounting electronic componentes to printed circuit boards.
The experiment varied a number of factors relevant to the process.
The response, measured by eye, is the number of visible solder skips.

<<solder1, fig=TRUE>>=
summary(solder)
length(unique(solder$PadType))
# reproduce their figure 1.1
temp <- lapply(1:5, function(x) tapply(solder$skips, solder[[x]], mean))
plot(c(0,6), range(unlist(temp)), type='n', xaxt='n',
     xlab="Factors", ylab ="mean number of skips")

axis(1, 1:5, names(solder)[1:5])
for (i in 1:5) {
    y <- temp[[i]]
    x <- rep(i, length(y))
    text(x-.1, y, names(y), adj=1)
    segments(i, min(y), i, max(y))
    segments(x-.05, y, x+.05, y)
}
@ 
A perfectly balanced experiment would have 3*2*10*3 = 180 observations for
each Mask, corresponding to all combinations of opening, solder thickness,
pad type and panel.  
The A3 mask has extra replicates of the Large/Thick, Large/Thin, and Small/Thick
conditions, and A6 has only the Medium/Thick, Medium/Thin, and Small/Thin
conditions.
Essentially, one extra run of 180 was done with a mixture of masks.
Chambers and Hastie focus on the balanced subset that excludes this last
run, so their figure and results are slightly different.

\subsection{Free Light Chain}
The \code{flchain} data frame contains the results of a small number
of laboratory tests done on a large fraction of the 1995
population of Olmsted County, Minnesota aged 50 or older
\cite{Kyle06, Dispenzieri12}.
The R data set contains a 50\% random sample of this larger study
and is included as a part of the survival package.
The primary purpose of the study was to measure the amount of
plasma immunoglobulins and its components.  
Intact immunoglobulins are composed of a heavy chain and light chain
portion.  In normal subjects there is overproduction of the light chain 
component by the immune cells leading to a small amount of 
\emph{free light chain}  in the circulation.
Excessive amounts of free light chain (FLC) are thought to be a marker of
disregulation in the immune system.
Free light chains have two major forms denoted as kappa and lambda,
we will use the sum of the two.

An important medical question is whether high levels of FLC have an
impact on survival, which will be explored using a Cox model.  
To explore linear models we will compare FLC values between males
and females.
A confounding factor is that free light chain values rise with age, in
part because it is eliminated by the kidneys and renal function
declines with age.
The age distribution of males and females differs, so we
will need to adjust our simple comparison between the sexes
for age effects.
The impact of age on mortality is of course even greater
and so correction for the age imbalance is is critical when exploring
the impact of FLC on survival.

Figure \ref{fig:data} shows the trend in free light chain values
as a function of age.
For illustration of linear models using factors, we have also
created a categorical age value using deciles of age.
The table of counts shows that the sex distribution becomes increasingly
unbalanced at the older ages, from about 1/2 females in the youngest
group to a 4:1 ratio in the oldest.
<<data, fig=TRUE, include=FALSE>>=
library(survival)
library(splines)
age2 <- cut(flchain$age, c(49, 59, 69, 79, 89, 120),                   
            labels=c("50-59", "60-69", "70-79", "80-89", "90+"))
counts <- with(flchain, table(sex, age2))
counts
#
flchain$flc <- flchain$kappa + flchain$lambda                    
male <- (flchain$sex=='M')
mlow <- with(flchain[male,],  smooth.spline(age, flc))
flow <- with(flchain[!male,], smooth.spline(age, flc))
plot(flow, type='l', ylim=range(flow$y, mlow$y),
     xlab="Age", ylab="FLC")
lines(mlow, col=2)
cellmean <- with(flchain, tapply(flc, list(sex, age2), mean, na.rm=T))
matpoints(c(55,65,75, 85, 95), t(cellmean), pch='fm', col=1:2)

round(cellmean, 2)
@
Notice that the male/female difference in FLC varies with age, 
\Sexpr{round(cellmean[1,1],1)} versus \Sexpr{round(cellmean[2,1],1)}
at age 50--59 and \Sexpr{round(cellmean[1,5],1)} versus
 \Sexpr{round(cellmean[2,5],1)} at age 90.
The data does not fit a simple additive model; there are ``interactions''
to use statistical parlance.
Men and women simply do not age in quite the same way.

\section{Linear model examples}
Start with a simple fit and then obtain the Yates predictions.
<<solder2>>=
fit1 <- lm(skips ~ Opening + Solder + Mask + PadType + Panel,
           data=solder)
y1 <- yates(fit1, ~Opening, population = "factorial")
y1
@ 
The printout has two parts: the left hand colums are mean predicted
values, the right hand are tests on those predicted values.  
The default is a single global test that they are all equal.
(``Population predicted value'' could be an alternate label but the abbreviation
PPV would be confused with the positive predicted value.)
The estimates under a factorial population, \code{y1} above,
are the Yates' weighted
means \cite{Yates} and the corresponding test is the Yates' sum of
squares for that term.  These would be labeled as a 
``least squares means'' and ``type III SS'', respectively,
by the glm procedure of SAS. 
More on this correspondence appears in the section on the SGTT algorithm.

Repeat this using the default population, which is the set of all 900
combinations for solder, mask, pad type and panel found in the data.
The pairwise option gives tests on all pairs of openings, for details
on this and other arguments see the \code{cmatrix} function;
``cm'' is a shorthand for cmatrix allowed within the \code{yates} argument list.
<<solder2b>>=
y2 <- yates(fit1, cm("Opening", test="pairwise"), population = "data") 
y2

temp <- rbind(diff(y1$estimate$mpv), diff(y2$estimate$mpv))
dimnames(temp) <- list(c("factorial", "emprical"), c("2 vs 1", "3 vs 2"))
round(temp,5)
@ 

Although the MPV values shift with the new population the global test is
unchanged, as is the difference in MPV between any two pairs.
This is because we have fit a model with no interactions.
Referring to figure 1 this is a model where all of the predictions are
parallel lines; shifting the population left or
right will change each average but has no effect on the difference between two
lines.
For a linear model with no iteractions the test statistics created by the
\code{yates} function are thus not very interesting, since they will be no 
different than simple comparisons of the model coefficients.

Here are results from a more interesting fit that includes interactions.
<<solder3>>=
fit2 <- lm(skips ~ Opening + Mask*PadType + Panel, solder)
yates(fit2, ~Opening, population="factorial")
@ 
Because of the near perfect balance the means change only a small amount.

\subsection{Missing cells}
Models that involve factors and interactions can have an issue with
missing cells as shown by the example below.
<<solder4>>=
fit3 <- lm(skips ~ Opening * Mask + Solder + PadType + Panel, solder)
yates(fit3, cm(~Mask, test="pairwise"))
@ 
The population predictions for each Mask include all combinations of
Opening, Solder, PadType, and Panel that are found in the data.
In the above call the empirical population was used, and the
underlying algorithm amounts to
\begin{enumerate}
  \item Make a copy of the data set (900 obs), and set Mask to A1.5 in
    all observations
  \item Get the 900 resultant predicted values from the model, and take their
    average
  \item Repeat 1 and 2 for each mask type.
\end{enumerate}
However, there were no observations in the data set with Mask = A6 and 
opening = Large.
Formally, predictions for the A6/Large combination are \emph{not estimable}, 
and as a consequence
neither are any population averages that include those predicted values, 
nor any tests that involve those population averages.
This lack of estimability is entirely due to the inclusion of a mask by opening
interaction term in the model, which states that each Mask/Opening combination
has a unique effect, which in turn implies that we need an estimate for all
Mask/Opening pairs to compute a population prediction.

If you do the above steps 'by hand' R gives a value for all 900 predictions,
along with a warning message that the results may not be reliable,
and they are not.
In this
particular case all the A6/Large predictions are deficient; they have in
effect assumed a value of 0 for the missing coefficient.
The presence of a missing value in \code{coef(fit3)} shows that some preditions
will not be estimable, but it is not possible to determine \emph{which}
ones are estimable from the coefficients alone. 
For this data set the A6:Small coefficient is NA, because the mask='Small'
coefficients are ordered last in the X matrix and that is when the
singularity is noticed.
A formal definition of estimability for a given prediction is that it 
can be written 
as a linear combination of the rows of $X$, the design matrix for the fit.
The \code{yates} function performs the necessary calculations to verify formal
estimability of each predicted value and thus is able to correctly identify the 
deficiency.

\subsection{Continuous variables}
Since the free light chain data set is a population sample there are far fewer
males than females at the oldest ages.  
The \code{yates} function can compute population averages over the sex
distribution for selected ages as shown below.
<<cont1>>=
flcfit <- lm(flc ~ age * sex, flchain)
yf1 <- yates(flcfit, cm(~age, levels=c(60, 70, 80, 90)))
@ 
This gives the results for a synthetic population that consists of
\code{table(flchain$sex)/nrow(flchain)} = 55\% females at all ages.
Setting \code{population='factorial'} will give one that has equal numbers
of males and females.

\section{Generalized linear models}
In a glm model we need to consider more carefully \emph{what} to target
as the population average.
<<glm>>=
gfit1 <- glm(skips ~ Opening + Mask*PadType + Panel, data=solder,
             family=poisson)
yates(gfit1, ~ Mask, type="linear")    # identical to type="link"
yates(gfit1, ~ Mask, type="response")
@ 
\section{Population}
The \code{population} parameter parameter of the call can be either a data set
or one of `data', `factorial', `sas', or `none'. 
Alternate labels of `empirical' and `Yates' are allowed for the `data' and
`factorial' options, respectively.
If \code{population} is a data set then it must contain all variables found
in the set of adjusters $V$.  
Using the original data set as the population is equivalent to `data'.

The 'right' data set to use is entirely a function of what question you
want to answer.
Consider the simple example of figure 1.  
If, for instance, one were contemplating effective drugs for a nursing home
population, then a population of ages shifted towards the right hand side of 
the x-axis would be the obvious target, and one would most likely use an
external reference to define that population.  
Both the \code{data} and \code{factorial} populations are also valid
choices, in the senses that each of them addresses a well defined
question; but is is a question that anyone would ask?
The factorial population in particular has been overused.
In figure 1 it corresponds to a population with the same number
of 100 year olds as 50 year olds: a population distribution that will never be
observed.


\section{SAS glim type III (SGTT) algorithm}
A major problem with SAS type III computations is that almost no one knows what
is being computed.  The documentation is sparse for the GLM procedure and
nearly non-existent for others.
``Least squares means'' (LSM) roughly correspond to MPV values, but are defined
only for categorical predictors.
\begin{itemize}
  \item Define a mixture population: factorial for categorical variables, i.e.,
    those that appear in a class statement, and the data distribution for
    all others.   For instance if the data set had $n$ rows, some continuous
    predictors, and two categorical adjusters with 3 and 4 levels, respectively,
    then the population data set would have $12 n$ rows.
  \item Compute MPV values.
  \item If the MPV involves non-estimable terms the result will be missing,
    just as with the \code{yates} function.
  \item There many ways to shorten and/or speed up this calculation of course,
    but that is not material to this discussion.
\end{itemize}

The type III SS is \emph{not} a direct test of equality of these MPV values,
and is in fact defined even when some or all of them are not estimable.
The root algorithm is based on the fact that in a balanced factorial
model (all predictors categorical, all combinations appear the same number
of times) the SS for all the effects are orthagonal.  
In the model \code{y ~ x1 + x2 + x1:x2} for instance the sequential SS for x1, 
from an R \code{anova} command,
will be the same whether x1 preceeds x2 or follows after it in the
formula.  
Let $V = (X'X)^-$ be the variance-covariance of the coefficients and let
$C$ and $D$ be contrast matrices for two of the 3 terms above.
Orthagonality implies that $CVD' =0$ = the covariance
between the two effect effects.

This leads to the following algorithm for computing Yates contrasts in an
unbalanced design. 
\begin{itemize}
  \item Let $Z$ be the design matrix for a balanced subset of the the data; 
    i.e., the factorial population of the \code{yates} command.
    One simple way to construct $Z$, if there are no missing cells, is the set
    of unique rows of $X$.  
  \item Choose contrasts that are orthagonal with respect to $(Z'Z)^-$.
  \item Assume an $X$ matrix in the standard column order of intercept, then
    main effects, then 2 way interactions, etc.  One way to create the
    contrasts is
    \begin{enumerate}
      \item Since the columns of $X$ that correspond to any partitular term are
        adjacent, we can think of $U = (Z'Z)^-$ as a partitioned matrix where
        $U_{kk}$ is the block corresponding to the $k$th term.
      \item Start with $C$ as a diagonal matrix, then for any two terms $j<k$ 
         where $j$ is contained in $k$  set $C_{jk}$ = U_{jk}U_{kk}^{-1}$.  The term
         \code{x1} is for instance contained in \code{x1:x2}.
      \item This results in an upper triangular matrix $C$ whose $k$th set
        of rows is a Yates contrast matrix for term $k$ and which satisfies
        the orthagonality condition.  
    \end{enumerate}
\end{itemize}
This is the algorithm used by the \code{yates} function for the SGTT
calculation. 
If there are both continuous and categorical variables in the $X$ matrix
then step 2 above is only applied if both $j$ and $k$ are categorical, and
the unique rows operation applies only to the categorical predictors.
(The continuous portion of $Z'Z$ need not even be computed since it is never
used.)
The interaction of a continous and a cateogorical covariate is considered 
to be continous for this purpose.
        
The SAS glm procedure uses a differnt algorithm, which is desribed indirectly
in SASxxx.
\begin{enumerate}
  \item Create design matrix $X$ of the form used by the GLM procedure,
    containing from left to right
    the intercept, then main effects, then two variable interactions
    (if any), three variable interactions, etc.  Within the set of main
    effects variables are in the order of the model statement.
    \begin{itemize}
      \item A categorical variable with $k$ levels is
        represented by $k$ 0/1 dummy variables, 
        which represent the first, second, etc. levels of the variable.
      \item The interaction between two categorical variables 
        that have $k$
        and $l$ levels will be represented by $kl$ 0/1 columns, and 
        likewise for higher level interactions.
      \item R's \code{model.matrix} function returns far fewer columns, since
        it tries to prospectively remove those that will be redundant.
        Having all these columns is critical to steps 2 and 3, however.
    \end{itemize}
  \item Create the $p$ by $p$ dependency matrix $D$ from the $n$ by $p$
    matrix $X$, from left to right. 
    \begin{itemize}
      \item If column $k$ of $X$ can be written as a linear combination of
        prior columns, then $D_{1k}$ to $D_{k-1,k}$ contains that combination,
        and $D_{j,k}=0$ for $j \ge k$.
      \item Otherwise set $D_{kk}=1$ and $D_{jk}=0$ for $j ne k$.
      \item Note that $D$ is upper triangular, and also that if the 
        $i$th column of $X$ is linearly dependent on 
        prior columns, then the $i$th row of $D$ will be zero.
    \end{itemize}
  \item Partially orthagonalize the transpose of $D$ from right to left
    \begin{itemize}
      \item For any column of $D'$ that corresponds to an interaction of one
        or more categorical predictors (but not mixed interactions of
        categorical and continuous variables), make any
        columns of $D'$ that correspond to a contained term orthagonal to it.
      \item If x1 and x2 were categorical, for instance, and the model contains
        the x1*x2 interaction, then any columns of $D'$ corresponding to
        x1 only or x2 only will be made orthagonal to the x1*x2 columns of $D'$.
    \end{itemize}
  \item The resulting rows of $D$ form the type III contrasts, i.e.,
    tests of $D \beta =0$ for the corresponding rows of $D$.
\end{enumerate}

An advantage of the first algorithm is that it works for any coding of the 
categorical variables; one can even have different codings in the same
model, say with x1 using contr.treatment and x2 contr.helmert, and changing
the coding will not change the result. The SAS glm algorithm on the other
hand appears to be totally dependent on using this exact coding of the
$X$ matrix.  One can change the order of the columns within a term,
i.e. change which level is the reference, without effecting the final result
however.
Neither approach produces the MPV values themselves, only a global contrast 
matrix for testing that they all are equal.
Proving that algorithm 1 gives the same result as the straightforward 
MPV approach is challenging but doable, as is a proof that algorithms 1 and 2
give the same result.

If all covariates are categorical and all of the mpv values are estimable 
then the above will agree with the Yates tests.
If not all the MPV values are estimable a Yates SS is not defined and
the SGTT is something quite different.
The ``keep unique rows'' step generates a $Z$ matrix that does not
correspond to a population, nor do the resulting tests.

Another way to derive the Yates estimates is based on
the well known fact that in a balanced factorial design the sums of squares
for main effects and interactions are all orthagonal;
any two contrast matrices $B$ and $C$ correspoding to different effects will
satisfy $B(X'X)^{-}C' =0$.
For an unbalanced data set, create contrasts that are orthagonal with 
respect to the variance matrix of the balanced design.
Assume for the moment that all predictors are categorical.
\begin{enumerate}
  \item Let $X$ be the design matrix for the full study.  Reduce this to 
    a balanced subset using only the unique rows \code{Z =unique(X)}.
  \item Find contrast matrices $B$, $C$, \ldots for the effects that are
    orthagonal with respect to $(Z'Z)^{-}$.  For instance start with naive
    contrast matrices for each effect which utilizes the colums for that
    effect, and then make each orthagonal to all those terms which contain it.
  \item Form sums of squares for each contrast using the original variance,
    e.g. $B (X'X)^{-} B'$.
\end{enumerate}

under any of the constrast options, first reduce it to a balanced subset
\code{Z =unique(X)} and then find contrast matrices B, C, etc. that
are orgthagonal with respect to $(Z'Z)^{-1}$. 
For a design with no missing cells this leads to the Yates sum of squares.
If there are missing cells the $Z$ matrix does not correspond to a balanced
design and the equivalence to a population average is lost.
However, it leads to a simple R algorithm that agrees with SAS type III for
a set of test cases. 
If the data contains continuous variables then $Z'Z$ uses only the categorical
columns of $X$, and is used to create the tests only for those columns.

Note that the SGTT algorithm is the SAS \emph{glm} type 3 procedure. 
Several other SAS procedures also create output labeled as ``type 3'' which
is not necessarily the same. 
The SAS phreg procedure uses the NSTT computation for instance,
and we have found
others that are not invariant to the choice of the reference level for a 
factor.
\end{document}
