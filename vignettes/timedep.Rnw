\documentclass{article}
\usepackage{amsmath}
\usepackage{Sweave}
\addtolength{\textwidth}{1in}
\addtolength{\oddsidemargin}{-.5in}
\setlength{\evensidemargin}{\oddsidemargin}

%\VignetteIndexEntry{Using Time Dependent Covariates}

\title{Using Time Dependent Covariates and Time Dependent Coefficients 
  in the Cox Model}
\author{Terry Therneau \and Cindy Crowson\\ Mayo Clinic}

\begin{document}
\maketitle

\SweaveOpts{keep.source=TRUE}
<<preamble, echo=F>>=
options(width=60, continue=" ")
makefig <- function(file, top=1, right=1, left=4) {
    pdf(file, width=9.5, height=7, pointsize=18)
    par(mar=c(4, left, top, right) +.1)
    }
library(survival)
@ 

\section{Introduction}
This vignette covers 3  different but interrelated concepts:
\begin{itemize}
  \item An introduction to time dependent covariates, along with some
    of the most common mistakes.
  \item Tools for creating time-dependent covariates, or rather the
    data sets used to encode them.
  \item Time dependent coefficients.
    
\section{Time dependent covariates}
One of the strengths of the Cox model is its ability to encompass covariates
that change over time.  
The practical reason that time-dependent covariates work
is based on the underlying way in which the Cox
model works: at each event time the program compares the current 
covariate values of the subject
who had the event to the current values of all others who were at risk
at that time. 
One can think of it as a lottery model, where at 
each death time there is a drawing to decide which subject ``wins'' the
event.  Each subject's risk score $\exp(X\beta)$
determines how likely they are to win,
e.g., how many ``tickets'' they have purchased for the drawing.
The model tries to assign a risk score to each subject that best predicts
the outcome of each drawing based on 
\begin{itemize}
  \item The risk set: which subjects are present for each event; the set of
    those able to ``win the prize''.
  \item The covariate values of each subject just prior to the event time.
\end{itemize}

The model has a theoretical foundation in martigale theory,
a mathematical construct which arose out of the study of games of chance.
A key underlying condition for a martingale like game is that present
actions depend only on the past. The decision of whether to play
(is one in the risk set or not) and the
size of a bet (covariates) can depend in any way on
prior bets and patterns of won/lost, but cannot look into the future.
If this holds then multiple properties can be proven about the
resulting process.

The key rule for time dependent covariates in a Cox model is simple
and essentially the same as that for gambling: \emph{you cannot look into
the future}.
A covariate may change in any way based on past data or outcomes, but it
may not reach forward in time.
One of the more well known examples of this error is analysis by 
treatment response:
at the end of a trial a survival curve is made comparing those who had an
early response to treatment (shrinkage of tumor, lowering of cholesterol,
or whatever) to those who did not, and it discovered that responders have
a better curve.  
A Cox model fit to the same data will demonstrate a strong 
``significant'' effect.
The problem arises because any early deaths, those that occur before response 
can be assessed,
will all be assigned to the non-responder group, even deaths that have
nothing to do with the condition under study.
The alarm about this incorrect approach has been sounded often
\cite{xxx} but is routinely re-discovered.
A slightly subtler form of the error is found in Bonadonna and Valagussa
\cite{Benadona81, Bonadonna85}.  Breast cancer chemotherapy patients were
divided into three groups based on whether the patient eventually
received $>85$\%, 65--85\% or $<65$\% of the dose planned at the start of
their treatment.  The chemotherapy regiment spans 12 weeks of treatment
and the early deaths, not surprisingly, do not get all their dose.  

If response is instead coded as a time-dependent covariate whose values 
depend only on the past, then the problem disappears.
For treatment response this will be a variable that starts at
0 for all subjects and is recoded to 1 only when the response occurs.
For dose it would measure cumulative dose to date.

There are many variations on the error: interpolation of the values of a
laboratory test linearly between observation times, removing subjects who do
not finish the treatment plan, imputing the date of an adverse event as midway
between observation times, etc. 
Using future data will often generate large positive or negative bias in the 
coefficients, sometimes it generates little bias ata all.
It is nearly impossible to predict a priori which of these will occur in any
given data set.
Using such a covariate is similar to jogging across a Los Angeles freeway:
disaster is not guarranteed --- but it is likely.

The most common way to encode time-dependent covariates is to use the
(start, stop] form of the model.
<<echo=TRUE, eval=FALSE>>=
fit <- coxph(Surv(time1, time2, status) ~ age + creatinine, 
             data=mydata)
@ 
In data set \code{mydata} a patient might have the following observations
\begin{center}
  \begin{tabular}{ccccccc}
    subject  & time1 & time2 & status & age & creatinine & \ldots \\ \hline
    1 & 0 & 15 & 0 & 25 & 1.3 \\
    1 & 15& 46 & 0 & 25 & 1.5 \\
    1 & 46& 73 & 0 & 25 & 1.4 \\
    1 & 73& 100& 1 & 25 & 1.6 \\
\end{tabular}
\end{center}
In this case the variable \emph{age} = age at entry to the study stays the
same from line to line, while the value of creatinine varies and is treated as
1.3 over the interval $(0, 15]$, 1.5 over $(15, 46]$, etc.  The intervals are
open on the left and closed on the right, which means that the creatinine
is taken to be 1.3 on day 15.
The status variable describes whether or not each interval ended in an event.

One commmon question with this data setup is whether we need to worry about
correlated data, since a given subject has multiple observations.
The answer is no, we do not.  The reason is that this representation is
simply a programming trick.
The likelihood equations at any time point
use only one copy of any subject, the program picks out the correct
row of data at any given time.
There are two exceptions to this rule, in which case the cluster variance
is necessary:
\begin{itemize}
  \item When subjects have multiple events.
  \item When a subject appears in overlapping intervals.  This however is 
    almost always a data error, since it corresponds to two copies of the
    subject being present at the same time, e.g., they could meet themselves
    on the sidewalk.
\end{itemize}


\section{Building time-dependent sets with tmerge}
\subsection{The function}
A useful function for building data sets is \code{tmerge}, which is
part of the survival library. 
The motivating case for \code{tmerge} came from a particular problem.
The Rochester Epidemiology Project has tracked all subjects living in
Olmsted County, Minnesota, from 1965 to the present.  
For an investigation of cumulative comorbidity we had three data sets
\begin{itemize}
  \item base: demographic data such as subject identifier, sex, 
    and birth date
  \item timeline: one or more rows for each subject containing 
    age intervals during which they were a resident of the county.  
    The important variables are id, age1 (start) and age2 (end).  
  \item outcome: one row for each age/outcome pair of interest.  The
    outcomes were 20 comorbid conditions as defined by NIH.
\end{itemize}

The structure of building the data is shown below.
(The data for this example unfortunately cannot be included in the survival
library so the code is shown but not executed.)

<<rep, eval=FALSE, echo=TRUE>>=
newd <- tmerge(data1=base, data2=timeline, id=repid, tstart=age1, 
               tstop=age2, options(id="repid"))
newd <- tmerge(newd, outcome, id=repid, mtype = cumevent(age))
newd <- with(subset(outcome, event='diabetes'), 
             tmerge(newd, id=repid, diabetes= tdc(age)))
newd <- with(subset(outcome, event='arthritis'),
             tmerge(newd, id=repid, event =tdc(age)))
@ 

The first call to tmerge adds the timeline for each observation to the
baseline data.  
The \code{tstart} and \code{tstop} arguments refer to the starting and
ending times for each subject and are taken from data set 2 (\code{data2}).
The \code{options} argument tells the routine that the identifier variable
in data set 1 is called `repid', and will cause the identifier variable
in the ouput data set \code{newd} to have that name.  
By default, the names of the three key variables are ``id'', ``tstart'', 
and ``tstop''.

Each subsequent call adds a new variable to the data set.
The second line creates an event variable which is a cumulative count of
the number of outcomes seen so far for each subject.  
The third and fourh lines create a time dependent covariate (tdc) which will
be 0 until the age of diabetes or arthritis and 1 thereafter.  
This is the basic working approach for all uses of tmerge: first establish
the range over which the subject is at risk, and then add events and/or
time dependent covariates to the data set one by one.  
These additions will often increase the number of rows in the data
set.
Say at some stage subject ``Smith'' has time intervals of
(0,15), (15,40) and (40,100) and we add a time dependent covariate \code{sbp}
(systolic blood pressure)
which is evaluated at months 6, 12, and 24 with values of
134, 126, and 140, respectively.  In the resulting
data set Smith will have intervals of 
\begin{center}
  \begin{tabular}{rrrrrr}
    (0,6)  &(6,12) & (12,15) & (15,24) & (24,40) & (40,100)\\
    prior  &  134  &  126    &  126    &  140    & 140
  \end{tabular}
\end{center}
The value over the interval (0,6) will be the value of the variable
\code{sbp} in data set 1, if the variable existed there, or NA otherwise.

\subsubsection{CGD data set}
Chronic granulomatous disease (CGD) is a heterogenous group of uncommon
inherited disorders characterized by recurrent pyogenic infections that
usually begin early in life and may lead to death in childhood. Interferon
gamma is a macrophage-activating factor shown to partially correct
the metabolic defect in phagocytes, and it
was hypothesized that treatment with interferon might reduce
the frequency of serious infections in patients with CGD.
In 1986, Genentech, Inc. conducted a
randomized, double-blind, placebo-controlled trial in 128 CGD patients who
received Genentech's humanized interferon gamma (rIFN-g) or placebo three %'
times daily for a year. Data were collected on all serious
infections until the end of followup,
which occurred before day 400 for most patients.
  One patient was taken off on the day of his last infection; all others
have some followup after their last episode.

Below are the first 10 observations, see the help page for \texttt{cgd0}
for the full list of variable names. The last few columns contain the 
duration of follow-up for the subject followed by infection times.
Subject 1 was followed for 414 days and had 2 infections during the study, 
subject 2 had 7 infections, and subject 3 had none.
\small
\begin{verbatim}
  1 204 082888 1 2 12 147.0  62.0 2 2 2 2 414 219 373
  2 204 082888 0 1 15 159.0  47.5 2 2 1 2 439   8  26 152 241 249 322 350
  3 204 082988 1 1 19 171.0  72.7 1 2 1 2 382
  4 204 091388 1 1 12 142.0  34.0 1 2 1 2 388
  5 238 092888 0 1 17 162.5  52.7 1 2 1 1 383 246 253
  6 245 093088 1 2 44 153.3  45.0 2 2 2 2 364
  7 245 093088 0 1 22 175.0  59.7 1 2 1 2 364 292
  8 245 093088 1 1  7 111.0  17.4 1 2 1 2 363
  9 238 100488 0 1 27 176.0  82.8 2 2 1 1 349 294
 10 238 100488 1 1  5 113.0  19.5 1 2 1 1 371
\end{verbatim}
\normalsize

The data set above is included as \code{cgd0} in the survival
library, and is a simple read of the above data set.
We want to turn this into a data set that has survival in a counting
process form.
\begin{itemize}
  \item Each row of the resulting data set represents a time interval
    (time1, time2] which is open on the left and closed on the right.
    Covariate values for that row are the covariate values that apply
    over that interval.
  \item The event variable for each row $i$ is 1 if that time interval ends
    with an event and 0 otherwise.
\end{itemize}

We don't want the variables etime1--etime7 in the final data set, so they are
left out of the data1 argument in the first call.
<<cgd1>>=
newcgd <- tmerge(cgd0[, 1:13], cgd0, id=id, tstop=futime)
newcgd <- tmerge(newcgd, cgd0, id=id, infect = event(etime1))
attr(newcgd, "tcount")
newcgd <- with(cgd0, tmerge(newcgd, id=id, infect = event(etime2))
newcgd <- tmerge(newcgd, cgd0, id=id, infect = event(etime3)) 
newcgd <- tmerge(newcgd, cgd0, id=id, infect = event(etime4), 
                 infect= event(etime5), infect=event(etime6),
                 infect= event(etime7))
attr(newcgd, "tcount")
all.equal(newcgd[, c("id", "tstart", "tstop", "infect")], 
          cgd   [, c("id", "tstart", "tstop", "status")], 
          check.attributes=FALSE)
@ 

\begin{itemize}
  \item A standard way to build data sets is one addition at a time,
    as shown by the etime1, etime2, and etime3 lines.  When multiple
    additions come from the same data set, however, one can do multiples
    at a time as shown in the last addition.
    They are processed one at a time within the code so the
    outcome of the two approaches is identical.
    Additions with a missing time value are skipped.
  \item The result of \code{tmerge} is a data frame with a few extra
    attributes. One of these, tcount, is designed to help visualize
    the process.
    Assume that a subject already had 3 intervals of (2,5), (5,10)
    and (14,40).
    A new event at time 1 would be ``early'' while one at time 50
    is after any interval and would be recorded as ``late''.  An event at
    time 3 would is within an interval, one at 5 is on the border of two
    intervals, one at 14 is at the leading edge of an interval and one at
    time 10 in on the trailing edge.
    In this data set all new additions fell stricly within prior intervals.
    We also see that etime6 and etime7 each added only a single event to 
    the data.
  \item If two observations in data2 for a single person share exactly
    the same time, the new value with be the sum of the contributions.
    The ``tied times'' column tells how often this happened; in some
    data sets this behaviour might not be desired and one would need to
    break the ties.
  \item The data2 argument is not required.  Sometimes the ``where'' form
    will be more convenient, as shown in the addition of etime2.
  \item These extra attributes of the data frame are ephemeral: they will be
    lost as soon as any further manipulation is done.  This was part of
    the design.
\end{itemize}

The last line above shows that the created data set is identical to
\code{cgd}, a (start, stop] version of the CGD data, also part of the
survival library; this had been created by hand several years earlier.

\subsection{Stanford heart transplant}
The \code{jasa} data set contains information from the Stanford heart
transplant study, in the form that it appeared in the J. Amer Statistical
Assoc paper of Crowley and Hu \cite{Crowley77}.
Each row also contain also contains the calculated age, time to transplant
and time to last follow-up calculated from these values.
Patients were on medical treatmant from their entry to the study until
a matching heart became available, at which time they transferred to 
surgical treatment.  
As is often the case with real data, this data set contains a few anomalies 
that need to be dealt with when setting up an analysis data set.
\begin{enumerate}
  \item The coefficients in table 5.2 of the definitive analysis found in
    Kalbfliesch and Prentice will only be obtained if covariates are defined
    in precisely the same way.  For age this is (age in days)/ 365.25 - 40 
    years, and for year of enrollment it is the number of years since the
    start of the study: (entry date - 1967/10/1)/365.25.
  \item One subject died on the day of entry.  However (0,0) is an illegal
    time interval for the program.  It suffices to have them die on day
    0.5.
  \item A subject transplanted on day 10 is considered to have been on medical
    treatment for days 1--10 and as transplanted starting on day 11.
    That is, except for patient 38 who died during the procedure on day 5.
    They should be treated as a transplant death; the problem is resolved by
    moving the transplant day to 4.5.
\end{enumerate}
Since time is in days the fractional time of 0.5 could be any chosen value
$t$ with $0 < t < 1$, it will not affect the results.
<<stanford>>=
tdata <- jasa[, -(1:4)]  #leave off the dates, temporary data set
tdata$futime <- pmax(.5, tdata$futime)  # the death on day 0
indx <- with(tdata, which(wait.time == futime))
tdata$wait.time[indx] <- tdata$wait.time[indx] - .5  #the tied transplant
sdata <- tmerge(tdata, tdata, id=1:nrow(tdata), 
                death = event(futime, fustat), 
                transplant = tdc(wait.time))
attr(sdata, "tcount")
coxph(Surv(tstart, tstop, death) ~ age + transplant, sdata)
@ 

This example shows one special case for the \code{tmerge} function that 
is moderately
common: when the data1 and data2 arguments are the same, and the first
created variable is an event code, then the range for each subject is
inferred to be from 0 to that event time: an explicit \code{tstop} argument
is not required.  It also makes use of a two argument form of \code{event}.
Each of the \code{event}, \code{cumevent}, \code{tdc} and \code{cumtdc}
may have a second argument, which will be used as the value or increment
to the event code or time dependent covariate.
If not present a value of 1 is assumed.

As a further example of time dependent covariates consider the PBC data.
The \code{pbc} data set contains baseline data and follow-up status
for a set of subjects with primary biliary cirrhosis, while the
\code{pbcseq} data set contains repeated laboratory values.
<<pbc>>=
temp <- subset(pbc, select=c(id:sex, stage))
pbc2 <- tmerge(temp, temp, id=id, status = event(time, status))
pbc2 <- tmerge(pbc2, pbcseq, id=id, ascites = tdc(day, ascites),
               bili = tdc(day, bili), albumin = tdc(day, albumin),
               protime = tdc(day, protime), alkphos = tdc(day, alk.phos))
@ 
 
\subsection{Predictable time-dependent covariates}
Occasionaly one has a time-dependent covariate whose values in the future
are predictable.
The most obvious of these is patient age, occasionally this may also be
true for the cumulative dose of a drug.
If age is entered as a linear term in the model, then the effect of changing
age can be ignored in a Cox model, due to the structure of the partial
likelihood.  Assume that subject $i$ has an event at time $t_i$, with other
subject $j \in R_i$ at risk at that time, with $a$ denoting age.
The partial likelihood term is
\begin{equation*}
  \frac{e^{\beta * a_i}}{\sum_{j \in R_i} e^{\beta* a_j}} =
  \frac{e^{\beta * (a_i + t_i)}}{\sum_{j \in R_i} e^{\beta* (a_j + t_i)}} 
\end{equation*}
We see that using time-dependent age (the right hand version) or age at
baseline (left hand), the partial likelihood term is identical since
$\exp(\beta t_i)$ cancels out of the fraction.
Howevever, if the effect of age on risk is \emph{non-linear}, this
cancellation does not occur.

Since age changes continuously, we would in theory need a very large
(start, stop] data set to completely capture the effect --- an interval
per day to match the usual resolution for death times.
In practice this level of resolution is not necessary; though we all
grow older, risk does not increase so rapidly that we need to know
our age to the day!  
For most medical applications year is sufficient, but this still leads
to a large data set.
One useful way to generate this data set is through use of the \texttt{pyears}
function.
The following example uses data on rehospitalization for cohort of 
rheumatoid arthritis
patients who also have conjestive heart failure (CHF)\cite{Nicola05}.
The variables are
\begin{itemize}
  \item patid: patient identifier
  \item agechf: age at onset of CHF 
  \item yearchf calendar year of CHF, relative to the start of the study
  \item startday, stopday: an interval of risk
  \item hospevt: =1 if the interval ends with a hospitalization
  \item prevhosp: number of prior hosptializations
  \item duration: duration of RA prior to CHF
  \item male sex: 1 for male, 0 for female
\end{itemize}
The age and duration variables have been rounded to .1 year to 
maintain patient privacy.

<<>>=
load('raheart.rda')
age2 <- tcut(raheart$agechf*365.25, 0:110* 365.25, labels=0:109)
rowid <- 1:nrow(raheart)
pfit <- pyears(Surv(startday, stopday, hospevt) ~ age2 + rowid,
               data=raheart, data.frame=TRUE, scale=1)
print(pfit$offtable)
pdata <- pfit$data
print(pdata[1:6,])
@
The \texttt{tcut} function attaches a set of cutpoints to the starting age
for each subject, it's primary job is to mark the variable as time-increasing%'
for the \texttt{pyears} function.
In the \texttt{pyears} call we set scale=1 to prevent the age intervals from
being rescaled to years, this is not critical.
Printing out the value of offtable is important, however.  One of them
most common mistakes in using \texttt{pyears} is mismatched scales, for
instance if the age were in years and the follow-up time in days, and
a common result of that error is to have follow-up time that fits into
none of the categories.
This will give a large amount of time that is outside the boundaries of the
table.
In the resulting data frame the first observation has 7 days of follow-up,
exactly as in the starting data.
The second observation has been broken into 4 rows, 109.6 days at age 92,
then a year each at age 93 and 94, and a final 234.9 days at age 95
ending with a hospitalization.
(The odd fractions of a day like .575 are a consequence of rounding the
age values to 1 digit.)

Now we combine this with the original data set using the same indexing
trick found in the first example.
We also need a variable containing the end time for the prior row of
each subject and zero for the first row of each subject, which is
\texttt{lagtime} below.
We then fit two models.  The first looks at the effect of age at
diagnosis of CHF, the second at the effect of current age.
<<>>=
index <- as.integer(pdata$rowid)
lagtime <- c(0, pdata$pyears[-nrow(pdata)])
lagtime[1+ which(diff(index)==0)] <- 0 #starts at 0 for each subject
temp <- raheart$startday[index] + lagtime  #start of each new interval
data2 <- data.frame(raheart[index,], 
                    time1= temp,
                    time2= temp + pdata$pyears,
                    event= pdata$event,
                    age2=  1+ as.numeric(pdata$age2) )

afit1 <- coxph(Surv(startday, stopday, hospevt) ~ male + pspline(agechf), 
               data=raheart)
afit2 <- coxph(Surv(time1, time2, event) ~ male + pspline(age2), data2)
#termplot(afit1, terms=2, se=TRUE, xlab="Age at Diagnosis of CHF")
#termplot(afit2, terms=2, se=TRUE, xlab="Current Age")

table(with(raheart, tapply(hospevt, patid, sum)))
@ 
In this particular case the two fits are quite similar.  
In retrospect this perhaps should have been expected: the mean age
for onset of CHF in this group is 75 years, which does leave a lot
of time for aging.
(This analysis is very preliminary, however. As shown by the last line
above there are a few patients with a very large number of admissions,
sometimes referred to as ``entering a revolving door'' near the end of
their disease.  These have an untoward influence on the fit.)


\subsection{Predictable covariates, method 2}

Another method to create a time-changing covariate is to use the
\emph{time-transform} feature of coxph.
<<>>=
afit2b <- coxph(Surv(startday, stopday, hospevt) ~ male + tt(agechf),
                data=raheart, 
                tt=function(x, t, ...) pspline(x + t/365.25))
afit2b
@ 

If there are one or more terms on the right hand side of the equation
marked with the tt() operator, the program will pre-compute the values
of that variable for each unique event time and strata combination.
A user-defined function is called with arguments of
\begin{itemize}
  \item the covariate: whatever is inside the tt() call
  \item the event time
  \item the event number: if there are multiple strata and the same event
    time occurs in two of them, they are treated separately
  \item the weight for the observation, if the call used weights
\end{itemize}
There is a single call to the function with a very large $x$ vector, 
it contains an element for each subject at risk at each event time.
If there are multiple tt() terms in the formula, then the tt argument
should be a list of functions with the requisite number of elements.

There are other interesting uses for the time-transform capability.
One example is O'Brien's logit-rank test procedure \cite{obrien78}.
He proposed replacing the covariate at each event time with a logit
transform of its ranks.  
This removes the influence of any outliers in the predictor $x$.
For this case we ingore the event time argument and concentrate on the
groupings.
<<>>=
function(x, t, riskset, weights){ 
    obrien <- function(x) {
        r <- rank(x)
        (r-.5)/(.5+length(r)-r)
    }
    unlist(tapply(x, riskset, obrien))
}
@ 
This relies on the fact that the input argments to tt() are ordered by the 
event number or riskset.
This function is used as a default if no tt argument is present in the
coxph call, but there are tt terms in the model formula.
(Doing so allowed me to depreciate the survobrien function).

Another interesting useage is to replace the data by simple ranks, not
rescaled to 0--1.
<<>>=
function(x, t, riskset, weights) 
    unlist(tapply(x, riskset, rank))
@ 
The score statistic for this model is $(C-D)/2$, where $C$ and $D$ are the
number of concordant and discordant pairs, see the
survConcordance function.  
The score statistic from this fit is then a test for significance of the
concordance statistics, and is in fact the basis for the standard error
reported by survConcordance.  
The O'Brien test can be viewed as concordance statistic that gives equal  %'
weight to each event time, whereas the standard concordance weights each
event proportionally to the size of the risk set.
(The Cox score statistic depends on the mean $x$ at each event time;
since ranks go from 1 to number at risk the mean also scales.)

Although handy, the computational impact of the tt argument should be
considered before using it.
The Cox model requires computation of a weighted mean and variance of
the covariates at each event time, a process that is inherently 
$O(ndp^2)$ where $n$ = the sample size, $d$ = the number of events and $p$=
the number of covariates.  
Much of the algorithmic effort in coxph() is to use updating methods for
the mean and variance matrices, reducing the compute time to $O((n+d) p^2)$.
When a tt term appears updating is not possible; for even moderate size
data sets the impact of $nd$ versus $n+d$ can be surprising.


The time-transform is a new addition and still has some rough edges.
At this moment the $x=T$ argment is needed to get proper residuals and
predicted values, and termplot is unable to properly reconstruct the
data to plot the spline fit.
Please communicate any concerns or interesting examples to the author.

\begin{thebibliography}{9}
  \bibitem{Nicola05} Nicola PJ, Maradit-Kremers H, Roger VL, Jacobsen SJ, 
    Crowson CS, Ballman KV, Gabriel SE. ``The risk of Congestive Heart Failure
    in Rheumatoid Arthritis: a Population-Based Study Over 46 Years.''
    \emph{Arthritis Rheum} 52: 412--20, 2005.
  \bibitem{obrien78} O'Brien, Peter, ``A non-parametric test for           %'
    association with censored data'', \emph{Biometrics} 34:243--250, 1978.
\end{thebibliography}
\end{document}
the covariates at each event time, a process that is inherently 
$O(ndp^2)$ where $n$ = the sample size, $d$ = the number of events and $p$=
the number of covariates.  
Much of the algorithmic effort in coxph() is to use updating methods for
the mean and variance matrices, reducing the compute time to $O((n+d) p^2)$.
When a tt term appears updating is not possible; for even moderate size
data sets the impact of $nd$ versus $n+d$ can be surprising.


The time-transform is a new addition and still has some rough edges.
At this moment the $x=T$ argment is needed to get proper residuals and
predicted values, and termplot is unable to properly reconstruct the
data to plot the spline fit.
Please communicate any concerns or interesting examples to the author.

\begin{thebibliography}{9}
  \bibitem{Nicola05} Nicola PJ, Maradit-Kremers H, Roger VL, Jacobsen SJ, 
    Crowson CS, Ballman KV, Gabriel SE. ``The risk of Congestive Heart Failure
    in Rheumatoid Arthritis: a Population-Based Study Over 46 Years.''
    \emph{Arthritis Rheum} 52: 412--20, 2005.
  \bibitem{obrien78} O'Brien, Peter, ``A non-parametric test for           %'
    association with censored data'', \emph{Biometrics} 34:243--250, 1978.
\end{thebibliography}
\end{document}
