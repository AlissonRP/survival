\documentclass{article}[11pt]
\usepackage{Sweave}
\usepackage{amsmath}
\addtolength{\textwidth}{1in}
\addtolength{\oddsidemargin}{-.5in}
\setlength{\evensidemargin}{\oddsidemargin}
%\VignetteIndexEntry{Concordance}

\SweaveOpts{keep.source=TRUE, fig=FALSE}
% Ross Ihaka suggestions
\DefineVerbatimEnvironment{Sinput}{Verbatim} {xleftmargin=2em}
\DefineVerbatimEnvironment{Soutput}{Verbatim}{xleftmargin=2em}
\DefineVerbatimEnvironment{Scode}{Verbatim}{xleftmargin=2em}
\fvset{listparameters={\setlength{\topsep}{0pt}}}
\renewenvironment{Schunk}{\vspace{\topsep}}{\vspace{\topsep}}

% I had been putting figures in the figures/ directory, but the standard
%  R build script does not copy it and then R CMD check fails
\SweaveOpts{prefix.string=compete,width=6,height=4}
\newcommand{\myfig}[1]{\includegraphics[height=!, width=\textwidth]
                        {compete-#1.pdf}}
\setkeys{Gin}{width=\textwidth}
<<echo=FALSE>>=
options(continue="  ", width=60)
options(SweaveHooks=list(fig=function() par(mar=c(4.1, 4.1, .3, 1.1))))
pdf.options(pointsize=10) #text in graph about the same as regular text
options(contrasts=c("contr.treatment", "contr.poly")) #ensure default

require("survival")
@

\title{Concordance}
\author{Terry Therneau}
\newcommand{\code}[1]{\texttt{#1}}

\begin{document}
\maketitle

\section{The concordance statistic}
 Use of the concordance statistic for Cox models was popularized by
Harrell \cite{Harrell96}, and it is now
the most used measure of goodness-of-fit in survival models.  
In general let $y_i$ and $x_i$ be observed and predicted data values,
in the most common case $x = \hat\eta$, the linear predictor from a fitted
model.
A pair of obervations $i$, $j$ is considered concordant if the prediction
and the data go in the same direction, i.e., 
$(y_i > y_j, x_i > x_j)$ or $(y_i < y_j, x_i < x_j)$.
The concordance is the fraction of concordant pairs.
For a Cox model remember that the predicted survival $\hat y$ is longer if
the risk score $X\beta$ is lower, so we have to flip the definition and
count ``discordant'' pairs.  For now ignore this detail and use the usual
definition.

One wrinkle is what to do with ties in either $y$ or $x$.  Such pairs
can be ignored in the count (treated as incomparable), treated as discordant,
or given a score of 1/2.  
Let $c, d, t_x, t_y$ and $t_{xy}$ be a count of the pairs that are concordant,
discordant, and tied on $x$ (but not $y$), tied on $y$ but not $x$, and
tied on both. Then
\begin{align}
  \tau_a &= \frac{c-d}{c+d + t_x + t_y + t_{xy}} \label{tau.a} \\
  \tau_b &= \frac{c-d}{\sqrt{(c+d+t_x)(c + d + t_y)}} \label{tau.b} \\
  \gamma &= \frac{c-d}{c+d} \label{gamma} \\
  d   &=    \frac{c-d}{c + d + t_x} \label{somer}
\end{align}

\begin{itemize}
  \item Kendall's tau-a \eqref{tau.a} is the most conservative; essentially
    treating ties as failures
  \item The Goodman-Kruskal $\gamma$ statistic \eqref{gamma} ignores ties in 
    either $y$ or $x$.
  \item Somers' $d$ treats ties in $y$ as incomparable, pairs that are tied
    in $x$ (but not $y$) score as 1/2.  The AUC from logistic regression is
    equal to Somers' $d$.
\end{itemize}
All three of the above range from -1 to 1.  The concordance is $(d+1)/2$.

The concordance has a natural interpretation as an experiment: present pairs
of subjects one at a time to the physician, statistical rule or some other
oracle, and count the number of correct
predictions.  Pairs that have the same outcome are not put forward (that would
be unfair); and if the oracle cannot decide it makes a random choice.
This leads to $c + t_x/2$ correct selections out of $c + d + t_x$ choices,
which is easily seen to be equal to $(d+1)/2$. 
Kendall's tau-b has a denominator of the same type, but treating $x$ and
$y$ symmetrically.

This hypothetical experiment gives a baseline insight into the concordance.
A value of 1/2 corresponds to using a random guess for each subject, and 
values of $<.55$ are not very impressive.
The ordering for some pairs of subjects will be obvious, and someone with
almost no medical knowledge (even a statistician like me) could nearly
that well by marking those pairs and using a coin flip for the rest.
Values of less than 1/2 are possible; some stock market analysts come to 
mind.
   
For survival data any pairs which cannot be ranked with certainty are
also considered incomparable.
For instance $y_i$ is censored at time 10 and $y_j$ is an event (or censor) 
at time 20.  Subject $i$ may or may not survive longer than subject $j$,
and so it is not possible to tell if the rule has ranked them correctly
or not. 
Note that if $y_i$ is censored at time
10 and $y_j$ is an event at time 10 then $y_i > y_j$.  
For stratified models, observations that are in different strata are 
also considered to be incomparable. 

\section{Examples}

<<examples>>=
fit1 <- glm(status ~ age + sex + ph.ecog, data=lung) # logistic regression
concordance(fit1)  # this give the AUC 

fit2 <- coxph(Surv(time, status) ~ age + sex + ph.ecog, data=lung)
concordance(fit2)

#fit3 <- survreg(Surv(time, status) ~ age + sex + ph.ecog, data=lung)
#concordance(fit3)
@ 

The concordance can also be visualized using its contribution at each
event time as below.
<<rplot, fig=TRUE>>=
par(mfrow=c(1,2))
c2 <- concordance(fit2, ranks=TRUE)
plot(rank ~ time , c2$ranks)
with(c2$ranks, lines(lowess(time, rank), col=2, lwd=2))
abline(0,0, lty=2)

plot(variance ~ time, c2$ranks, ylim=c(0, .34))
abline(h=1/3, col=2)
@ 

\section{Weighted concordance}
Watson and Therneau \cite{Watson15} show that the numerator of Somers' $d$
for a response $y$ and predictor $x$ can be re-written as
\begin{equation}
  c-d = 2 \sum \delta_i n(t_i) \left[ r_i(t_i) - 1/2 \right] \label{cscore}
\end{equation}
where $n(t)$ is the number of subjects still at risk at time $t$, and
$r_i(t)$ is the rank of $x_i$ among all those still at risk at time $t$,
where the rank is defined with $0 \le r \le 1$.
It turns out that 
equation \eqref{cscore} is exactly the score statistic for a Cox model
with $n(t) r(t)$ as the time-dependent covariate.

One immediate consequence of this connection is a straightforward definition
of concordance for a risk score containing time dependent covariates. Since
the Cox model score statistic is well defined for time dependent terms this
justifies calculation of the values $c$, $d$, etc in the same way: at each
event time the current risk score of the subject who failed is compared to the
current scores of all those still at risk.


A more interesting consequence is the quesion of alternate weightings of the
risk scores.  Define ``Somers' d'' ranks as $2 (r_i(t) -1/2)$; they range from
-1 to 1 and their weighted sum is both the Cox score statistic and the numerator
of Somers' $d$.
Under H0, these residuals will be approximately uniformly distributed from
-1 to 1 with a mean of 0 and variance of 1/3. 
The right hand panel of figure xx
above shows the increment to the Cox model information matrix at each
death time for the lung data set, that increment being the estimated variance
of $r$ at that time point, and we see that the variance is essentially constant
until the very end, where the number at risk is less than 10.  


If the original Cox model has a single 0/1 treatment covariate then $d$
is exactly the Gehan-Wilcoxon statistic, using weights of 1 instead of $n(t)$
it will be the log-rank statistic.  
We can apply some of the same historical arguments used in survival tests
to the concordance as well.
Peto and Peto \cite{Peto72} point out that 
    $n(t) \approx n(0)S(t-)G(t-)$, where $S$
    is the survival distribution and $G$ the censoring distribution.
    They argue that $S(t)$ would be a better weight since $G$ may have
    features that are irrelevant to the question being tested.  
    For a particular data set Prentice \cite{Prentice79} later showed 
    that these concerns were indeed justified, and
    almost all software now uses the Peto-Wilcoxon variant. 
If proportional hazards does not hold, then Schemper 
    \cite{Schemper09} argues for
    a weight of $S(t)/G(t)$ in the Cox model.  The basic argument is \ldots.
    The same argument would hold more strongly for the concordance, since the 
    target is an ``assumption free'' assessment of association.

A similar argument to Schemper is argued by Umo \cite{Umo09},
who recommend the use
of $n/G^2$ as a weight.  The Peto and Peto argument would suggest $S/G$ as
an equivalent but more stable choice.

So, what weight should we use?  One very important issue to think about
what the target of our measure \emph{should} be. 
Korn and Simon \cite{Korn90} for instance point out the importance of
restricting the range. 
Though a risk model can be used for long-range prediction, in actual patient
practice this will not be the case.  
A physician is quite unlikely to look up my lab tests from 5 years ago,
and then compute a 6 year model based survival from that point, in order
to predict my outcome one year from today. 
The \code{concordance} routine allows for \code{ymin} and \code{ymax} options
to restrict the range of comparison;
one could even argue for a sliding scale that weighted recent time more
heavily, if that is the milleau in which the model will be used.
In addition to careful thought about what the assessment is for,
there are three intersecting and more technical issues.
\begin{itemize}
  \item Equality vs. efficiency.  On one hand we would like to treat each
    data pair equally, but in our quest for ever sharper p-values we want to
    be efficient.  The first argues for $n(t)$ as the weight and the
    second for using equal weights, since the variances of each term are
    the same.  This is exactly the argument between the Gehan-Wilcoxon and
    the log-rank test.
  \item Safety.  If using the Gehan-Wilcoxon, the Peto-Wilcoxon variant 
    would appear to be more stable.
  \item Current data versus future invariance.  On the one hand assessment
    of a model should use of 
    the data in hand, ``make the most with what you have'', but on the other
    we would like an estimated concordance to stay stable as a study's
    follow-up matures.
\end{itemize}
Our current opinion is that since the point of the concordance is to evaluate
the model in a more non-parametric way, that the log-rank type of focus on
ideal p-values is misplaced.  This suggests using either $S$ or $S/G$ as
weights.  Careful thought about the application of the model will often
lead to a restricted upper time \code{ymax}.
This can be particularly important for the $S/G$ weight, since $1/G$ can induce
very large weights near to the end of follow-up.  
A weight of $n/G^2$ will be even more susectible to this issue.
Weights of $S/G$ will give more prominence to the later time points and $S$
somewhat more to the earlier ones. 
The choice between them is an argument about whether actual usage of the score
will be weighted to earlier times versus the benefit of consistency over time.
If time limits have been thought through carefully the difference will often
be minor. 
If proportional hazards approximately holds over the time window, as in the
example above, then all weighted averages over the range will be close to
identical.


\section{Variance}
The variance of the statistic is estimated in two ways. The first is to use
the variance of the equivalent Cox model score statistic.  As pointed out
by Watson, this estimate is both correct and efficient under $H_0: d=0$,
a null concordance of 1/2, and so is forms a valid test of $H_0$.
However, when the concordance is larger than 1/2 this estimate systematically
overestimates the variance.  
An alternative is the infinitesimal jackknife estimate of $d$
\begin{align*}
  V_{IJ} &= \sum_{i=1}^n w_iU_i^2 \\
  U_i &= \frac{\partial d}{\partial w_i}
\end{align*}
The concordance routine calculates an influence matrix $U$ with one row per
subject and columns that contain the derivitives of the 5 individual counts:
concordant, discordant, tied on x, tied on y, and tied on xy pairs.  From
this it is easy to derive the influence of each subject on $d$, or on any other
of the other possible association measures such as $\tau_a$.
The IJ variance is printed by default but the PH variant is also returned.

One useful property of the IJ estimation is that the variance of the 
difference in concordance between two separated fitted models is also easily
obtained.  If $C_a$ and $C_b$ are the two concordance statistics and $U_{ia}$
and $U_{ib}$ the influence values, the influence vector for $C_a-C_b$ is 
$U_a - U_b$.  It is not necessary that the models be nested.
However, it is necessary that they be computed on the same set of observations.
(Add example)
\end{document}

