\documentclass{article}[11pt]
\usepackage{Sweave}
\usepackage{amsmath}
\addtolength{\textwidth}{1in}
\addtolength{\oddsidemargin}{-.5in}
\setlength{\evensidemargin}{\oddsidemargin}
%\VignetteIndexEntry{SAS comparisons}

\SweaveOpts{keep.source=TRUE, fig=FALSE}
% Ross Ihaka suggestions
\DefineVerbatimEnvironment{Sinput}{Verbatim} {xleftmargin=2em}
\DefineVerbatimEnvironment{Soutput}{Verbatim}{xleftmargin=2em}
\DefineVerbatimEnvironment{Scode}{Verbatim}{xleftmargin=2em}
\fvset{listparameters={\setlength{\topsep}{0pt}}}
\renewenvironment{Schunk}{\vspace{\topsep}}{\vspace{\topsep}}

% I had been putting figures in the figures/ directory, but the standard
%  R build script does not copy it and then R CMD check fails
\SweaveOpts{prefix.string=compete,width=6,height=4}
\newcommand{\myfig}[1]{\includegraphics[height=!, width=\textwidth]
                        {sas-#1.pdf}}
\setkeys{Gin}{width=\textwidth}
<<echo=FALSE>>=
options(continue="  ", width=60)
options(SweaveHooks=list(fig=function() par(mar=c(4.1, 4.1, .3, 1.1))))
pdf.options(pointsize=10) #text in graph about the same as regular text
options(contrasts=c("contr.treatment", "contr.poly")) #ensure default

library(survival)
@ 

\title{SAS comparisons}
\author{Terry Therneau}
\newcommand{\code}[1]{\texttt{#1}}
\begin{document}
\maketitle

\begin{quote}
``I found a bug in your software; it gives a different answer than SAS.''\\
Message from a user.
\end{quote}

One of the perils of writing and maintaining a basic package like
\code{survival} is
the inevitable comparisons to SAS.
This note talks about a few of these. 
It's primary message is that for most issues here is a good reason for the
differences: they are not an oversight.
 
None of the material in this vignette is terribly important, and in fact for
many of cases the change will be numerically small --- of the $n-1$ vs $n$ 
variety  --- and can be ignored for practical purposes.
I'm hoping that it diverts at least some of the comments like the above.
(By the way, in that particular case the real problem was the the
\emph{data sets} used for the SAS and R calls were slightly different.)

\section{Convergence}
It should go without saying that \code{coxph} and SAS \code{phreg} will not
give \emph{exactly} the same answers.
They are both working with finite precision floating point arithmetic, a world
in which $(1.1 + 1.3) - 1.4 \ne 1.1 + (1.3 - 1.4)$.
See the R FAQ, item 7.31 for a longer discussion and explanation of this.

In any case, it is a certainty that even if the two routines are evaluating
the same formulas, and both are using Newton-Raphson iteration
to arrive at a solution, there will be points at which certain arithmetic
operations were done in a different order.
Add onto this slightly different methods for matrix inversion, prescaling
of the data or not, and different thresholds for our
convergence criteria, then yes, the answers will certainly differ in the
final digits.  Neither is wrong in this case. 

\section{Efron approximation}
The mathematics for a Cox model are all worked out for continuous time
values, but in real data there will be ties, i.e., two or more events on
the same day.  
There are several approximations that are used to adapt the method
for tied times: the Breslow and Efron approximations, Prentice's marginal
likelihood, or the
exact partial likelihood option found in Cox's original paper.
SAS defaults to the Breslow approximation and R to the Efron approximation.

There was a lot of interest and activity in this area in the early years
after the introduction of the Cox model, 
but the consensus has now settled down to a few simple points.
\begin{itemize}
  \item The Breslow approximation is both easy to program and computationally
    efficient. The Efron approximation is nearly as fast, and somewhat
    more accurate.
 \item The exact partial likelihood is computationally intensive: if there
   are $d$ events at one time point out of $n$ subjects at risk, the
   likelihood is a sum of all $n \choose d$ possible subsets.
   When $d$ is between 2 and 10 this is manageable, particularly if an efficient
   enumeration due to Gail \cite{Gail81} is used, but beyond that point the
   computation can quickly become untenable.
   Prentice's marginal likelihood requires a numerical integration; it's 
   computation time is intermediate between the Efron and exact methods.
 \item At any time point where there are no ties, all four approximations
   produce identical increments to the partial likelihood.
 \item If the number of
   ties is small to moderate, as it is in most data sets, then the
   \emph{numerical} difference between the methods will be negligible.
\end{itemize}

A primary point that has come to be appreciated is the fourth of these,
and most people have now simply (and sensibly) stopped worrying about it.
Consider for instance the example below using recurrence time for the 
colon cancer data set, and then a second version of the data
where time is rounded to the nearest month.
In the original data most of the event times are unique; the largest
count of ties is 5 events on the same day, which happened twice.
The coefficients under the three approximations hardly differ for this
data set:
standard errors of each coefficient are about 0.1 (not shown)
while the differences in
estimates do not appear until the third decimal point.
That is, there is less than .01 standard error of difference, 
which is essentially identical from a statistical point of view.
 
<<ctest1>>=
rdata <- subset(colon, etype==1)
table(table(colon$time[colon$status==1]))
lfit1 <- coxph(Surv(time, status) ~ rx + adhere + node4, rdata, 
               ties='breslow')
lfit2 <- coxph(Surv(time, status) ~ rx + adhere + node4, rdata, 
               ties='efron')
lfit3 <- coxph(Surv(time, status) ~ rx + adhere + node4, rdata, 
               ties='exact')
rbind(breslow= coef(lfit1), efron=coef(lfit2), exact=coef(lfit3))
@ 

Using the coarsened time scale there are only 67 unique event times
for the 468 events which occur,
leading to some time points with over 20 events.
But even in then coefficients only disagree in the second digit.

<<ctest2>>= 
months <- floor(rdata$time/30.5)
table(table(months[rdata$status==1]))
mfit1 <- coxph(Surv(months, status) ~ rx + adhere + node4, rdata, 
               ties='breslow')
mfit2 <- coxph(Surv(months, status) ~ rx + adhere + node4, rdata, 
               ties='efron')
mfit3 <- coxph(Surv(months, status) ~ rx + adhere + node4, rdata, 
               ties='exact')
rbind(breslow2= coef(mfit1), efron2=coef(mfit2), exact2=coef(mfit3))
@
 
Looking more carefully, the Efron approximation has been affected the least by
the coarsening; it is also (less importantly) closer to the exact 
computation result.
The survival package chose the Efron as the default from a basic
``why not the best'' logic.
After all, if one had two approximations to the cosine function with similar
compute cost, but one of them behaved better in certain edge cases,
any sensible code for fundamental libraries would use the
more stable one.  
However, in a statistical analysis context and given the usual size of
standard errors, a Breslow default will cause no real harm.
The Breslow approximation is the default in many statistical packages for
simple historical reasons: since it is the simplest to program it was
usually the first method to be implemented.

Once having chosen the Efron default it is important to carry
through.  The approximation turns out to have implications for how residuals 
and baseline hazard functions are computed, 
which in turn affects robust variance estimates.
Details of this can be found in the validation vignette. 
This is one area where the phreg procedure is not quite correct, and its
results do not agree with the validation suite.
For all the examples that we have investigated in detail, however, the
practical implications of the inaccuracy have been small.
The largest practical issue vis-a-vis SAS/R is
users who don't read the documentation, and then get worried when
numbers don't exactly agree, since one run used Efron and the other Breslow.

\section{Efficiency of the Cox model}
 \subsection{Counting process data}
The solution to the Cox estimating equations is found using Newton-Raphson
iteration, which requires the computation of first and second derivatives
with respect to to each coefficient.
The basic quantities for these are a weighted mean and variance of $X$,
at each event time, taken over the set of subjects who are at risk at
that event time, using $\exp(X\beta)$ as the weights. 
Let $p$ be the number of covariates, $d$ the number of unique event times
and $n$ the number of observations.
A simple computation of the $d$ variance matrices will be of order
$O(ndp^2)$.  For the rest of this section I will drop the $p$ portion and
refer to the naive method as $O(nd)$.

For ordinary \code{Surv(time, status)} survival data, 
SAS phreg, R coxph, and every
other code that I am aware of reduces this to an $O(n)$ computation 
via a simple strategy:
process the data from largest to smallest survival time.  That way the
sums needed for each of the risk sets occur naturally as we proceed; 
and one pass through the data will suffice to compute them.  
This makes the code quite efficient.
If there are strata in the model, means and variances of $X$ are taken
\emph{within} strata; this adds no extra work since we need only zero the
relevant running sums at the start of each stratum.
The computation starts by sorting the data by reverse time within strata,
but sorting is a basic operation that all packages do efficiently so we
haven't counted it in the total.

When the data is in counting process form, i.e., 
\code{Surv(time1, time2, status)}, the computation is more challenging.
As a way of investigating this we will use a large simulation data set.
It is intended to mimic to some degree the Nurses Health Study, since analysis
of that data set raised a question about the relative speed of coxph and phreg.
The code for the data set is below; most readers can skip 
directly to the printout at the end.

<<nhs1>>=
# Simulate a study with long follow-up
# Base it roughly on the NHS, with follow-up every 2 years
#
set.seed(1960)
n <- 121700    # number of nurses who enrolled
#n <-  12170      # test run, with smaller data
temp <- floor(c(30*365.25, 55*365.25))     
age <- sample(temp[1]:temp[2], n, replace=TRUE) # age in days, at enrollment
iage <- floor(age/365.25)               # birthday age

# a random survival time for each subject.  Minnesota death rates*.8
#  give approx the same number of deaths as the NHS (personal communication).
sy <- survexp.mn[1:110, 'female', "1985"]*365.25 *.8 # yearly hazard rate
dtime <- double(n)
for (i in 30:55) {
    j <- which(iage == i)
    chaz <- rexp(length(j))  # chaz for each subject is exp(1)
    dtime[j] <- approx(cumsum(sy[i:109]), i:109, chaz, rule=2)$y
}
    
# change to days
dtime <- pmax(30, ceiling(dtime*365.25))  # at least 1 month of survival

# up to 16 bi-annual follow-ups for each subject + enrollment visit
#  covariates are rounded to fewer digits to make nicer printout
times <- matrix(sample(700:770, n*16, replace=TRUE), ncol=n)
vage <- apply(rbind(age, times), 2, cumsum)
temp <- data.frame(id=rep(1:n, each=17),
                   vage = c(vage),     # age at visit
                   x1 = round(runif(n* 17, 100, 200)),
                   x2 = round(rgamma(n* 17, 1,2), 2))

dummy <- data.frame(id=1:n, dtime=dtime, agem = floor(age*12/365.25))
nhs <- tmerge(dummy, dummy, id=id,  death=event(dtime),
               options= list(tstartname="age1", tstopname="age2"))
nhs <- tmerge(nhs, temp, id=id, qnum= cumtdc(vage), 
               x1=tdc(vage, x1), x2=tdc(vage, x2))
latedeath <- with(nhs, death==1 & (age2-age1) > 800)
nhs <- subset(nhs, age1>0 & !latedeath)

# Add a lot of binomial variables.  Adding this at the end in this
#  way allows me to write + xb in the model rather than +xb.1 + xb.2 + ...xb.40
#  That is, same model, less typing.
nhs$xb <- matrix(rbinom(nrow(nhs)*40, 1, .4), ncol=40)

# Look at statistics for the first 1/3, 2/3, and 3/3 of the subjects
nsubject <- max(nhs$id)
stats <- matrix(0, 5, 3)

for (i in 1:3) {
    temp <- subset(nhs, id <= (nsubject* i/3))
    stats[1,i] <- nrow(temp)
  
    km <- survfit(Surv(age1, age2, death) ~ 1, temp)
    stats[2,i] <- sum(km$n.event)
    stats[3,i] <- sum(km$n.event>0)
    stats[4,i] <- mean(km$n.risk[km$n.event >0])

    time1 <- system.time(coxph(Surv(age1, age2, death) ~ x1+ x2 + xb,
                         ties="breslow", temp))
    stats[5,i] <- sum(time1[-3])   # ignore elapsed time
}

dimnames(stats) <- list(c("number of observations", "number of events",
                          "unique event times", "mean number at risk",
                          "cpu time"),
                        c("first 1/3", "2/3", "all"))

round(stats)
@ 

The resulting data set has 1.8 million records for 122 thousand subjects,
reflecting the long bi-annual follow up for the NHS.  
Subjects have a new observation at each visit, giving the new values of their
covariates, and their last visit will end in a censoring or death.
Since we are only interested in compute time issues the covariates were
created as simple random numbers without any structure, thus none of the
variables are expected to be statistically important.

The approach used by coxph is to use a running sum for (time1, time2) data,
just as it does for simple survival, but in this case
subjects will be added \emph{and} subtracted from the totals.  
Again working backwards in time, an
observation of (100, 200, 1) would be added to the totals when 
the code's internal time
value crosses 200, and then removed when the time crosses 100.  
This requires some care in the internal routines to avoid catastrophic
cancellation; e.g., in finite precision arithmetic (1e18 + 123) - (1e18) =0 so
a large outlier that is added and then later removed can be deadly.
(Mitigation for this is discussed in the noweb source code.)
The basic algorithm has a run time of $O(2n)$.
Looking at the results above, we see that the execution time is almost
perfectly linear in $n$, the number of observations.
The ideal, but unobtainable algorithm would be linear in the number
of unique death times $d$ = the number of terms in the partial likelihood
sum, with no time at all devoted to bookkeeping; $d$ grows
slower than $n$ due to ties.

Running the same data set using SAS phreg, the execution times were 
85, 303, and 867 seconds for 1/9, 2/9, and 3/9 of the subjects,
a quadratic growth rate.  (We used ties='breslow' in R so as to match the 
phreg default.)
The phreg algorithm appears to use the simpler approach, which is to calculate
the variance anew at each death time.
In this case the dominating term in the compute time will be $O(nd)$,
and $d$ grows with $n$. 

Note that because of licensing restrictions SAS jobs were run on a 
dedicated server, so times will not be exactly comparable.  Based on the
hardware of the R and SAS servers we expect no more than a 2x distortion.
This ``fudge factor'' should be applied to all the results to follow.

\subsection{Partitioned data sets}
 The NHS example started with a complaint that R was 8 fold \emph{slower}
than SAS on the actual NHS data set, which puzzled me.
In what situation could this happen?
One way is to create a version of the data where $n$ grows, but the size
of the average risk set $m$ and the number of events $d$
do not.
This can be done by \emph{dicing} the nhs data set, i.e., cut each observation 
into multiple small pieces.  
(In the same sense as dicing vegetables in the kitchen.)
The very first observation of the simulation data set, for instance, is a
censored time span from age 40 to age 42.06 = 14610 to 15361 days of age.
What if we were to replace this observation by a whole set of rows
with (age1, age2, death) values of (14610, 14611, 0), (14611, 14612, 0),
(14612, 14613, 0), \ldots (15360, 15361, 0).
Since NHS subjects are contacted every two years, this will increase
the data set size by approximately 730 fold,
leading to a consequence 700 fold increase in the compute time for \code{coxph}.

This kind of expansion is sometimes done to create dummy covariates x*log(t)
as a step towards testing proportional hazards. 
The PH test does not require expansion for every single day, however, only for
the set of unique death times,
i.e., we can use the set of death times as the cut points when creating an
expanded data set.
(Any intervals that don't overlap a death time turn out to play no role in
the Cox partial likelihood.)
In the survival package an expanded data set is never needed for this
particular purpose, however, since the \code{cox.zph} function directly computes
the relevant score test in $O(n)$ time, without data expansion.

Another use of such data sets is when a user desires to control the risk
sets exactly. An example is nested case-control sampling, where at each 
death time a subsample of the subjects who were at risk at that time is
selected.  
Say there were 52 events and 10 controls were selected for each.
A special data set is constructed which has 52*11 observations: the first
event + the 10 controls for it, then the second event + its 10 controls, etc.
Add to the data set a \code{group} variable that identifies the
52 groups and a status variable of 1=event/ 0=control.
A standard Cox model program can now be used to fit the model.
A \code{time} value, which used by the \code{coxph} routine to determine risk
sets, is not needed since the risk sets are already set, and one can use
the call
\begin{verbatim}
   fit <- coxph(Surv(dummy, status) ~ strata(group) + x1 + x2 + ...)
\end{verbatim}
where \code{dummy} is a dummy variable set to a constant value. The fit is
independent of what value is chosen, we often use 1.

Since models using \code{Surv(time, status)} are faster than those that
use the \code{Surv(time1, time2, status)} form --- somewhat so for coxph and
very much so for phreg --- this raises the question of whether partitioning
the data set at every event time could actually be faster.
In the \code{nhs} data set over half of the days in the study window contain
a death (\Sexpr{stat[3,3]} unique event times over age 30-89) which would 
dividing each 2 year follow-up into approximately 370 portions, leading to a
data set which is simply unmanagable.
We will scale the example down in two ways:
\begin{itemize}
  \item Coarsen the time scale to use age in months.  
    Per the prior section on the Efron estimate, for a data set with 10+ years
    of follow-up
    the use of months vs. days will hardly change the final Cox model estimates.
  \item Use only the first 1/10 of the subjects.
\end{itemize}
Since nearly every month has a death, each two-year interval between visits
will split into 20 or more segments.
The resulting data set is about 20/10 = 2 fold larger than the \code{nhs}
sample.

<<nhs2, fig=TRUE>>=
temp <-  subset(nhs, id < 12170)    # 121700 / 10
temp$month1 <- floor(temp$age1* 12/365.25)
temp$month2 <- floor(temp$age2* 12/365.25)
temp <- subset(temp, month1 < month2)  # remove any accidental ties
dtime <- sort(unique(temp$month2[temp$death == 1]))
nhs2 <- survSplit(Surv(month1, month2, death) ~., data=temp, cut=dtime)
nrow(nhs2)/nrow(temp)

nrow(nhs2)  # 3.7 million rows

stat2 <- matrix(0, 6, 3)
maxid <- max(nhs2$id)
dummy <- rep(1, nrow(nhs2))
for (i in 1:3) {
    temp <- subset(nhs2, id <= (maxid*i/3))  # 1/3, 2/3, 3/3
    km <- survfit(Surv(month1, month2, death) ~ 1, data=temp)
    time1 <- system.time(coxph(Surv(month1, month2, death) ~ x1 + x2 + xb,
                               ties = "breslow", data=temp))
    time2 <- system.time(coxph(Surv(dummy, death) ~ x1 + x2 + xb+ 
                                   strata(month2), ties='breslow', temp))
    stat2[1, i] <- nrow(temp)
    stat2[2, i] <- sum(temp$death)
    stat2[3, i] <- length(unique(temp$age2[temp$death==1]))
    stat2[4, i] <- mean(km$n.risk[km$n.event > 0])
    stat2[5, i] <- sum(time1[-3])
    stat2[6, i] <- sum(time2[-3])
}

dimnames(stat2) <- list(c("observations", "death", "unique deaths",
                          "mean risk set", 
                          "coxph time 1", "coxph time 2"),
                        c("first 1/3", "2/3", "all"))
round(stat2)
plot(c(stats[1,], stat2[1,], stat2[1,])/1e6, 
     c(stats[5,], stat2[5,], stat2[6,]),
     pch=c(1,1,1,2,2,2,3,3,3),
     xlab="Data set size (millions)", ylab="Coxph run time")
@ 

The coxph compute time is linear in the number of rows, as before, and continues
the series of values found in the first test.
The row count grows faster than the number of subjects, since the number of 
cut points per subject also grows with $n$,
though that will eventually slow (there are only so many possible death times).
The two fits in the loop, one using (age1, age2) and the other stratified on
the age at death, give exactly the same likelihood since they have 
exactly the same risk sets at each event time.
The expected compute time for the second will grow at the rate of $dm$,
where $m$ is the number of subjects per strata;
when constructing risk sets the code only has to search within stratum.
However, the
construction of the data ensures that each row of data is in exactly one
stratum, i.e., that $E(m) = n/d$, so this approach has exactly the same
growth rate as the counting process data.
(The fact that the counting process code + expanded data 
combination is somewhat faster is a bit of a surprise, but it remains linear.)
If we expand the full nhs data set (not shown) the result has 40.3 million
rows = a 23.7 fold increase, and the coxph call took 20.3 times as long
to execute as for the unexpanded data set.

For SAS however, the different constructions differ substantially in 
compute time.  The SAS code is shown below.
The first form, reported above, takes 11 seconds for 1/10 the data set
and is anticipated to take approx 1100 seconds for the full counting 
process data.
The second form took 109 seconds for 1/3 of the expanded data set  = 1/30
of the subjects.
The third approach, like R,
will have $O(md) = O(n)$ behavior since both SAS and R are
using an ``ordinary'' Cox model for the fit.
Using this form phreg took
7.7, 12.3, and 18.4 seconds for 1/3, 2/3, and 3/3 of the 
expanded data, which extrapolates to 184 seconds for an expanded form of
the entire nhs cohort (40 million rows); 
less than 1/5 of the counting process form.

\begin{verbatim}
proc phreg data= nhs;
   where (id <= 12170);
   model (month1 month2) * death(0) =  x1 x2 xb_1 xb_2 xb_3 xb_4 xb_5 xb_6 xb_7
        xb_8 xb_9 xb_10 xb_11 xb_12 xb_13 xb_14 xb_15 xb_16 xb_17 xb_18 xb_19
        xb_20 xb_21 xb_22 xb_23 xb_24 xb_25 xb_26 xb_27
        xb_28 xb_29 xb_30 xb_31 xb_32 xb_33 xb_34 xb_35 xb_36
        xb_37 xb_38 xb_39 xb_40;

proc phreg data=nhs2;
   where (id < 8113);
   model (month1 month2) * death(0) =  x1 x2 xb_1 xb_2 xb_3 xb_4 xb_5 xb_6 xb_7
        xb_8 xb_9 xb_10 xb_11 xb_12 xb_13 xb_14 xb_15 xb_16 xb_17 xb_18 xb_19
        xb_20 xb_21 xb_22 xb_23 xb_24 xb_25 xb_26 xb_27
        xb_28 xb_29 xb_30 xb_31 xb_32 xb_33 xb_34 xb_35 xb_36
        xb_37 xb_38 xb_39 xb_40;

proc phreg data=nhs2;
   model dummy*death(0) = x1 x2 xb_1 xb_2 xb_3 xb_4 xb_5 xb_6 xb_7
        xb_8 xb_9 xb_10 xb_11 xb_12 xb_13 xb_14 xb_15 xb_16 xb_17 xb_18 xb_19
        xb_20 xb_21 xb_22 xb_23 xb_24 xb_25 xb_26 xb_27
        xb_28 xb_29 xb_30 xb_31 xb_32 xb_33 xb_34 xb_35 xb_36
        xb_37 xb_38 xb_39 xb_40;
   strata month2;
\end{verbatim} 

The main take home message for R is ``don't dice your data set!''
Though that step is essential to achieve good performance in SAS,
what makes things faster for one statistical package does not necessarily
work for another.
This is especially true for something like the NHS simulation, where per
subject covariate updates occur at a low frequency.  
The 1.7 million row / 122 thousand subject counting process data set took
7.5 seconds, while the diced version took 24 times
as long.  The increase would be even larger if we 
had cut the data at every death day rather than using months.

\subsection{Risk set sampling}

The average number at risk, at each death, in the full Cox model was over
60 thousand.  
This is statistical overkill: 10-20 conrols per case will work just as well.
We can thin this down by doing \emph{risk set} subsampling, e.g., for each
risk set evaluate only a portion of the subjects.
For the NHS data one simple way to choose a subset is to use all subjects
who started the study as the same age (in months) as the case.
In our (age1, age2) Cox model the risk set for an event that occured at
age 40.5 years, say (484 months) would be all subjects who were alive and
on study at that age; we will argue that the particular month in which someone 
happened to enroll will not differentiate them with respect to future
survival, as compared to others of the same age.
One simple way to accomplish the sub-sampling is to match subjects into 
sets based on
their age at first response, in months; the variable \code{agem} in our
data set above.  This gives risk sets of size 350--450, about 150
fold smaller than before. 
What does it do to the compute time?  
Code wise, this corresponds to simply adding \code{strata(mage)} to the model.

<<strata>>=
system.time(assign("fit1", coxph(Surv(age1, age2, death) ~ x1 + x2 + xb,
                                 ties='breslow', data=nhs)))

system.time(assign("fit2", coxph(Surv(age1, age2, death) ~ x1 + x2 + xb +
                                 strata(agem), ties='breslow', data=nhs)))
@ 

Reducing the average number at risk by 150 fold has reduced the compute time
by about 25\%; the overall size of the data set has not changed and size
dominates the \code{coxph} compute time.
The effect of stratification on phreg, however, is profound: the run
times for 1/3, 2/3, and 3/3 of the data were now 7, 18, and 33 seconds;
still quadratic but 120 fold smaller than before.
The R and SAS run times are now comparable. 

There is an interesting lesson here. 
As statisticians, our training has
taught us that random sampling works, and
our first reaction to larger and larger data sets should not always be to
look for a bigger hammer --- a faster computer, better algorithm, or
clever data manipulation.
(This speedup required none of those.)

\subsection{Combining approaches}
Further conversation with the NHS study personnel clarified that they were 
using an approach that combined all three of subsampling, coarsening and
data expansion.

\subsection{Data set sizes}
The 'xz' compression option of the \code{save} command 
is particularly successful for data sets like
the NHS simulation that contain a lot of whole numbers,
an rda file containing nhs and nhs2 consumed just under 10 MB of disk space,
while the size using default \code{save} options was 38 MB.
The save operation itself takes longer using xz compression, 
but reloading the save data sets requires similar time.
The SAS data set for nhs used 103 MB and that for nhs2 265 MB using default
options, more knowledgeable SAS coders perhaps could to do better.

\subsection{The FAST option}
Our 2019 version of phreg now has a \emph{FAST} option.  
The phreg manual is vague about the actual approach:
``FAST
uses an alternative algorithm to speed up the fitting of the Cox regression for 
a large data set that has the counting process style of input. 
Simonsen (2014) has demonstrated the efficiency of this algorithm when the 
data set contains a large number of observations and many distinct event times. 
The algorithm requires only one pass through the data to compute the Breslow 
or Efron partial log-likelihood function and the corresponding gradient 
and Hessian. 
PROC PHREG ignores the FAST option if you specify a TIES= option value other
 than BRESLOW or EFRON, 
or if you specify programming statements for time-varying covariates. You might 
not see much improvement in the optimization time if your data set has only a 
moderate number of observations.''

The reference in the manual is ``Simonsen, J. (2014). Statens Serum Institut, 
Copenhagen. Unpublished SAS macro'', which is not helpful.
However, someone else pointed us to a web reference which contains a macro
that is the purported prototype for the phreg option;
it can be found using a web search for the article title 
``A method for speeding up PROC PHREG when doing a Cox regression''.

A closer look at the macro brought on a strong case of deja vu.  Early in the
author's computing career an analysis data set would sometimes be too
big to fit into memory, and a common work around for a binary outcome was to
\begin{enumerate}
  \item  Reduce any predictors to categorical variables of 2--4 levels.
  \item  Create a new data set with one row for each unique combination of
    $y$ and the (new) predictors, giving each a case weight equal to the number
    of observations in the original data set that fell into that bin.
  \item  Analyze the new data set, using these case weights.
\end{enumerate}
Reductions in the data set size of 20-50 fold were not uncommon.

The FAST option appears to use essentially the same approach.
Say that there are $k$ unique combinations of the categorized covariates.
For a Cox model, the intermediate data set will have 2k observations at each
unique event time, one set containing the count of events at that time and one
the count of those who were at risk but did not have an event at said time.
Then fit an ordinary Cox model to this data set, stratified by event time and
weighted by the counts.
The compute time will be $O(kd)$, where $k$ is potentially much smaller than
$O(nd)$ for the original data.  Observations with a weight of 0, if any, 
can of course be eliminated from the data set before the fit.
Creating the collapsed data set is itself a task that potentially
requires $O(nd)$ time,
since for each event time the tally needs to check all $n$ observations to
see if their (time1, time2) interval includes the given event.
However, this is essentially an indexing problem and can be done quickly using
hashing methods.

As someone who often preaches against categorization of predictors, the
author does not find this approach attractive.

\section{Type 3 tests}
Another user query has been for ``type 3'' tests of survival models.
Up until the latest release (at our institution) the phreg procedure would
automatically print a quantity and statistic labeled ``type 3'' 
--- you couldn't turn it off in fact.  
They have now been relabeled as 
``joint tests'' but appear to be the same values as before.
The primary problem with these is that they depend on the way in which
covariates are coded.

\begin{verbatim}
data test;
input y x1 $ x2 $;
cards4;
 1 a  A
 2 b  B
10 c  C
50 a  D
 5 b  A
 4 c  B
 8 a  C
40 b  D
60 c  A
20 a  B
21 b  C
22 c  D
 3 a  A
 5 b  B
12 c  C
52 a  D
 7 b  A
 8 c  B
16 a  C
88 a  D
58 a  A
28 a  B
20 a  C
 5 a  B
;;;;

data test2; set test;
   status =1;  * add a dummy status variable;
   if (x2= 'A') then x2a="xA"; else x2a=x2;
   if (x2= 'B') then x2b="xB"; else x2b=x2; * make B the reference;
   if (x2= 'C') then x2c="xC"; else x2c=x2;

proc phreg data = test2;
   class x1 x2;
   model time * status(0) = x1 x2 x1*x2/ type1;

proc phreg data = test2;
   class x1 x2b;
   model time * status(0) = x1 x2b x1*x2b/ type1;

proc phreg data = test2;
   class x1 x2c;
   model time * status(0) = x1 x2c x1*x2c / type1;

proc phreg data = test2;
   class x1 x2d;
   model time * status(0) = x1 x2c x1*x2c / type1;

proc phreg data= test2;
    class x1 x2/ param=effect;
    model time * status(0) = x1 x2 x1*x2 / type1;
\end{verbatim}

The resulting `joint tests' for covariate \code{x1} were 7.9, 5.4,
0.7, 2.4, and 5.3:
coding x2 differently changes the results for x1!
What are we to make of this?
The sequential or ``type 1'' tests are identical for all 5 runs.

\subsection*{Linear Models}
Type III tests and sums of squares can be traced back to a 1934 paper
by F. Yates \cite{Yates34}.
For a linear model that was fit to unbalanced data, 
he proposed using a population estimate for the main effects,
where the ``population'' is an ideal experiment, e.g., a completely balanced
factorial study.  One can create his proposed estimates by a simple 3 step
process.
\begin{enumerate}
   \item Fit a linear model to the data at hand using ordinary least squares.
   \item For each treatment arm, create the set of predicted values for that
     treatment, across all combinations of the other factors.
   \item The average of these values is the estimated overall effect for
     the treatment.
\end{enumerate}

<<echo=FALSE>>=
data1 <- structure(list(y = c(1, 2, 10, 50, 5, 4, 8, 40, 60, 20, 21, 22, 
  3, 5, 12, 52, 7, 8, 16, 148, 58, 28, 20, 5),
 x1 = structure(c(1L, 2L, 3L, 1L, 2L, 3L, 1L, 2L, 3L, 1L, 2L, 3L, 1L, 2L, 3L, 
                  1L, 2L, 
3L, 1L, 1L, 1L, 1L, 1L, 1L), .Label = c("a", "b", "c"), class = "factor"), 
    x2 = structure(c(1L, 2L, 3L, 4L, 1L, 2L, 3L, 4L, 1L, 2L, 
    3L, 4L, 1L, 2L, 3L, 4L, 1L, 2L, 3L, 4L, 1L, 2L, 3L, 2L), .Label = c("A", 
    "B", "C", "D"), class = "factor"), x3 = c(1L, 2L, 3L, 4L, 
    5L, 6L, 7L, 1L, 2L, 3L, 4L, 5L, 6L, 7L, 1L, 2L, 3L, 4L, 5L, 
    6L, 7L, 1L, 2L, 3L)), .Names = c("y", "x1", "x2", "x3"),
                   class="data.frame")
@ 
<<>>=
with(data1, table(x1, x2))
@ 

In the data set used above there are four levels for \code{x2} for instance,
so the effect for \code{x1='a'} will involve the average of 4 predicted
values.
The estimates are a prediction of what the average result
for x1 \emph{would have been}
had the original data been balanced.
SAS calls these averages \emph{least squares means}.
Modern causal modelers would call this a g-estimation method, though they,
in turn, would almost never choose a factorial experiment as the
underlying population.

The next logical step is to compute standard errors for each of these averages,
along with a global test of whether the averages for x1=a, x1=b and x1=c 
are the same.
The simplest method is to start by creating a matrix with all
of the coefficients for the predicted values as rows, and then use matrix
operations.  
This was not feasible in the computing environment of Yates' day,
however, which led to a small industry of shortcut methods
for producing a ``Yates' sum of squares''.
The SAS GLM algorithm is based on a particular one of these, namely that
for a balanced factorial design, the sums-of-squares for
all of the effects are orthagonal.
Therefore, choose contrasts that are orthagonal to the variance matrix we
\emph{would have had} had the data been of this type.
It can be reproduced using the following steps:
\begin{itemize}
  \item Form a $Z'Z$ matrix from a
    \emph{balanced subset} of the observations in the data set,
  \item create a set of contrasts that are orthogonal to $(Z'Z)^{-1}$
  \item apply those contrasts to the original data and fit, to get 
    sums-of-squares for each term.
\end{itemize}

Here is an example of Yates' tests using a subset of the \code{solder} data set,
based on the SAS algorithm.  The \code{ctest} function contains standard
linear models manipulations to compute the SS and test for a contrast.

<<solder>>=
ctest <- function(contr, fit) {
    estimate <- drop(contr %*% coef(fit))
    var.estimate <- contr %*% vcov(fit) %*% t(contr)
    test <-  solve(var.estimate, estimate)%*% estimate
    list(estimate= estimate, variance= var.estimate, chisq = drop(test))
}

test <- subset(solder, Mask != "A6")
table(test$Opening, test$Mask)   # the data is unbalanced
fit <- lm(skips ~ Opening*Mask, data=test, x=TRUE)  # ordinary linear model
z.balance <- unique(fit$x)    # 12 unique combinations of Opening and Mask
contr <- chol(crossprod(z.balance))   # cholesky decomposition of Z'Z
ctest(contr[2:3,], fit)$chisq         # the test for Opening
ctest(contr[5:8,], fit)$chisq         # the test for Mask
ctest(contr[9:12,], fit)$chisq        # the test for Mask:Opening
@ 

If $L'L = Z'Z$ is the Cholesky decomposition of $Z'Z$, then $L(Z'Z)^{-1}L'=I$,
i.e., the Cholesky decomposition was simply a convenient way to create a
set of contrasts that are orthogonal with respect to $(Z'Z)^{-1}$.
Any full rank contrast matrix $C$ such that $T= C(Z'Z)^-C'$ is block 
diagonal will give the same sums of squares, i.e., one where
$T_{ij}=0$ whenever rows $i$ and $j$ correspond to different terms.
The equivalence is a consequence of Cochan's theorem.
Goodnight \cite{Goodnight78} states that any matrix
which satisfies this condition defines the set of type 3 tests.
The SAS technical report given as the usual
 reference for type 3 tests \cite{SAStype3}
is a simply complex algorithm for creating such an orthogonal decomposition;
it unfortunately depends on using an $X$ matrix in exactly the same
internal form as is used by the SAS glm procedure.
(The report is also remarkably opaque about exactly \emph{what} the algorithm
is computing.)
Both the Cholesky approach and the GLM algorithm are invariant to how
categorical factors are represented: first level as reference, last as
reference, Helmert coding, etc. all yield the same values.

If the linear model has both continuous and categorical predictors, then 
the \code{z.balance} line above needs to be modified to first omit any columns
of X corresponding to continuous variables or interactions with continuous
variables (but should retain the intercept):
SAS ``type 3'' tests for continuous variables have no connection to Yates' 
method or population averages.

The bugaboo comes when the resulting $Z'Z$ matrix is singular.
For example, if we use the entire solder data set, there are 0 observations
in the (Mask=A6, Opening=S) cell, the coefficient vector from the \code{lm}
fit will have an NA in that position, and an attempted Cholesky decomposition
of $Z'Z$ will give an error message. 
In this case a Yates' population average for mask A6 is also undefined, since
one of the predicted values in the average depends on the NA coefficient;
SAS GLM for instance will report a missing value for that element of
the least squares means.
The Yates' sum of squares for comparing the 5 mask types is also undefined;
nevertheless SAS GLM reports a type 3 sums of squares for the Mask effect!  
What appears to be happening is that SAS GLM is creating contrasts that are
orthogonal with respect to a generalized inverse.
The problem is that the resulting test statistics depend
on exactly which generalized inverse is used; different g-inverse
matrices lead to different values.
There are an infinite number of generalized inverses for such a problem,
the documentation in
\cite{SAStype3} does not give enough detail to exactly replicate the
GLM algorithm for this case, and exact mimicry is the only way to exactly 
reproduce GLM type 3 values for an incomplete design. 

An alternate ``type 3'' algorithm found outside of SAS is the following.
\begin{enumerate}
  \item Create a design matrix $X$ for the regression in standard order,
    i.e., from left to right are the intercept, columns for main effects, then
    2-way interactions, then 3-way interactions, etc.
  \item Proceeding from left to right, eliminate any columns which can be
    expressed as a linear combination of prior columns.
  \item Define the RSS for any term as the difference between a fit using
    the full $X$ matrix, and a fit eliminating those (remaining)
    columns that correspond to the term.
\end{enumerate}
If the columns of $X$ for the categorical variables in the model are coded 
using the summation constraint, and the Yates' SS is well
defined (no missing cells),
then, rather remarkably, half-baked approach will re-create 
the Yates sum-of-squares for each term.
If any other method is used to generate the 0/1 columns for a categorical
variable the results are meaningless, and in fact will often change 
by an order of magnitude across different codings.
I refer to this as the not-safe type three (NSTT) algorithm. 
The \code{car} package in R uses this approach, for instance,
and the package's documentation clearly states that models
need to be fit with 
R's \code{contr.sum} option in effect.

Many of us have not seen the sum constraint since graduate school.  As
a reminder here is the two way form where $\epsilon$ is the residual error.
\begin{align*}
  y_{ijk} &= \mu + \alpha_i + \beta_j + \gamma_{ij} + \epsilon_{ijk} \\
     0 &= \sum_i \alpha_i
     0 &= \sum_j \beta_j
     0 &= \sum_i \gamma_{ij} &= \sum_j \gamma_{ij}
\end{align*}
Assume a simple 2 factor model y = trt + x2 + interaction, where treatment
has two levels A/B and x2 has $k$ levels.
case one can quickly verify that the the NSTT test for treatment is actually a
comparison of A vs. B in the \emph{reference} level for x2, ignoring all
others, leading to $k$ different results as we rotate the choice of
reference.  This is very different than the type 3 narrative of being a
global test.
When sum constraints are used the `reference' for x2 will be an average and
the NSTT matches the Yates' estimate.
We suspect that the same general pattern extends to more complex models.


The flawed NSTT algorithm is, I believe, a primary source of a common 
critique that
type 3 tests not marginal, i.e., that they are a test for main effects
in the presence of interactions and are thus invalid.  
This has muddied the discussion of population contrasts
 terribly, and is in addition to the harm 
caused by misapplication of the algorithm: sum constraints are not the
default in most packages and I daresay that many or even most computations have
been wrong.  (How many people read directions?)

\subsection*{Type 3 and phreg}
The NSTT is precisely the algorithm used by the SAS phreg procedure, though
without any warning about how to choose the coding for categorical predictors.
This can be verified for the test example above by creating the
appropriate \code{coxph} fits and performing Wald tests.
For example, the first two fits below code reproduce phreg results
that used 'A' and 'D' as the reference levels.
The \code{yates} function in the survival package produces Yates' 
population contrast;
it's result is identical whether fit1 or fit2 is used as the input.
The final phreg fit that used a sum constraint,
referred to as ``effects'' coding in SAS, agreed with the Yates' result.

<<sasr>>=
contr.x1 <- matrix(0, nrow=2, ncol=11)
contr.x1[1,1] <- contr.x1[2,2] <- 1
fit1 <- coxph(Surv(y) ~ x1*x2, data=data1, ties='breslow')
ctest(contr.x1, fit1)$chisq

options(contrasts=c('contr.SAS', 'contr.poly'))
fit2 <- coxph(Surv(y) ~ x1*x2, data=data1, ties='breslow')
ctest(contr.x1, fit2)

yates(fit1, ~x1, population="factorial")
@ 

\bibliographystyle{plain}
\bibliography{refer}
\end{document}



